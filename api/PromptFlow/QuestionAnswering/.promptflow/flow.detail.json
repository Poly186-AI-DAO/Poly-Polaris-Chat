{
  "flow_runs": [
    {
      "run_id": "4d4879be-de55-475e-85ef-cc3de9de4117_0",
      "status": "Completed",
      "error": null,
      "inputs": {
        "chainType": "stuff",
        "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
        "indexType": "cogsearchvs",
        "postBody": {
          "values": [
            {
              "data": {
                "approach": "rtr",
                "overrides": {
                  "chainType": "stuff",
                  "deploymentType": "gpt3516k",
                  "embeddingModelType": "azureopenai",
                  "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
                  "searchType": "hybridrerank",
                  "semantic_captions": false,
                  "semantic_ranker": true,
                  "temperature": 0,
                  "tokenLength": 1000,
                  "top": 3
                },
                "text": ""
              },
              "recordId": 0
            }
          ]
        },
        "question": "What is the advantage of fine-tuning BERT over using feature-based approaches?"
      },
      "output": {
        "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
        "context": [
          "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
          "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
          "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
        ],
        "output": {
          "values": [
            {
              "recordId": 0,
              "data": {
                "data_points": [
                  "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
                  "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                  "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
                ],
                "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
                "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.']<br>        Question: What is the advantage of fine-tuning BERT over using feature-based approaches?<br>        ",
                "sources": "\nBert.pdf",
                "nextQuestions": "<>\n<>\n<>",
                "error": ""
              }
            }
          ]
        }
      },
      "metrics": null,
      "request": null,
      "parent_run_id": "4d4879be-de55-475e-85ef-cc3de9de4117",
      "root_run_id": "4d4879be-de55-475e-85ef-cc3de9de4117",
      "source_run_id": null,
      "flow_id": "default_flow_id",
      "start_time": "2023-10-12T14:01:54.943308Z",
      "end_time": "2023-10-12T14:01:57.850329Z",
      "index": 0,
      "api_calls": [
        {
          "name": "parseBody",
          "type": "Tool",
          "inputs": {
            "postBody": {
              "values": [
                {
                  "data": {
                    "approach": "rtr",
                    "overrides": {
                      "chainType": "stuff",
                      "deploymentType": "gpt3516k",
                      "embeddingModelType": "azureopenai",
                      "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
                      "searchType": "hybridrerank",
                      "semantic_captions": false,
                      "semantic_ranker": true,
                      "temperature": 0,
                      "tokenLength": 1000,
                      "top": 3
                    },
                    "text": ""
                  },
                  "recordId": 0
                }
              ]
            }
          },
          "output": {
            "chainType": "stuff",
            "deploymentType": "gpt3516k",
            "embeddingModelType": "azureopenai",
            "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
            "searchType": "hybridrerank",
            "semantic_captions": false,
            "semantic_ranker": true,
            "temperature": 0,
            "tokenLength": 1000,
            "top": 3
          },
          "start_time": 1697137314.947264,
          "end_time": 1697137314.949265,
          "error": null,
          "children": null,
          "node_name": "parse_postBody"
        },
        {
          "name": "createLlm",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            }
          },
          "output": {
            "model": "gpt-3.5-turbo",
            "request_timeout": null,
            "max_tokens": 1000,
            "stream": false,
            "n": 1,
            "temperature": 0.3,
            "engine": "chat16k",
            "_type": "azure-openai-chat"
          },
          "start_time": 1697137314.953786,
          "end_time": 1697137314.955814,
          "error": null,
          "children": null,
          "node_name": "create_llm"
        },
        {
          "name": "embedQuestion",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "question": "What is the advantage of fine-tuning BERT over using feature-based approaches?"
          },
          "output": [
            -0.016869042068719864,
            -0.0003010840737260878,
            0.022069774568080902,
            -0.01818922907114029,
            -0.0022569845896214247,
            0.013475230894982815,
            -0.0065809269435703754,
            0.004023900255560875,
            -0.032884631305933,
            -0.019216040149331093,
            0.02651040069758892,
            0.030404282733798027,
            -0.024683475494384766,
            0.0007167676230892539,
            -0.018149223178625107,
            -0.015762219205498695,
            0.036565151065588,
            0.028804056346416473,
            0.005427430849522352,
            -0.025256890803575516,
            -0.011601634323596954,
            -0.009081278927624226,
            -0.024976851418614388,
            -0.011588298715651035,
            -0.02108296938240528,
            -0.0024053386878222227,
            0.045446399599313736,
            -0.020642906427383423,
            -0.01844259724020958,
            0.0037738648243248463,
            0.026403717696666718,
            0.02196309342980385,
            -0.016562333330512047,
            -0.02032286301255226,
            -0.011454946361482143,
            0.009701366536319256,
            -0.0029054090846329927,
            -0.01225505955517292,
            0.024843499064445496,
            -0.01254176627844572,
            0.01114156935364008,
            0.032751280814409256,
            0.011975020170211792,
            -0.015215476043522358,
            -0.0032737944275140762,
            0.0264437235891819,
            0.022069774568080902,
            -0.00033192173577845097,
            -0.03133774548768997,
            0.020056158304214478,
            0.002888740273192525,
            0.030564304441213608,
            -0.016975723206996918,
            0.003323801327496767,
            -0.05198065564036369,
            -0.007454383186995983,
            -0.006334225181490183,
            0.005664131138473749,
            -0.0017552472418174148,
            -0.025723623111844063,
            0.002136967610567808,
            0.006624266039580107,
            -0.00904794130474329,
            0.01246175542473793,
            -0.012735126540064812,
            -0.013095177710056305,
            -0.005047377664595842,
            0.022816546261310577,
            0.010688171721994877,
            -0.0024503450840711594,
            0.012088368646800518,
            0.02571028843522072,
            -0.01602892391383648,
            -0.006470911204814911,
            0.027577217668294907,
            -0.014828754588961601,
            -0.0028720712289214134,
            0.021576372906565666,
            0.01041479967534542,
            -0.0031987838447093964,
            -0.0014477039221674204,
            0.006234211381524801,
            -0.01090153492987156,
            0.014268675819039345,
            0.02060290239751339,
            0.014735408127307892,
            0.0026270365342497826,
            0.0010318119311705232,
            0.003170446492731571,
            0.015802225098013878,
            0.005537446588277817,
            0.018749307841062546,
            0.03416481241583824,
            0.02221646159887314,
            0.011074893176555634,
            -0.0029904211405664682,
            -0.005217401310801506,
            0.014562050811946392,
            -0.00028566524269990623,
            0.004353946540504694,
            -0.009027938358485699,
            0.009401324205100536,
            -0.029817532747983932,
            -0.011661642231047153,
            -0.004167253617197275,
            0.009067943319678307,
            0.018629290163517,
            -0.005494107026606798,
            -0.0012393412180244923,
            -0.0036205099895596504,
            -0.03037761151790619,
            0.014482039958238602,
            -0.014588721096515656,
            -0.014175329357385635,
            -0.0054140957072377205,
            0.0022019767202436924,
            0.0005113219958730042,
            -0.010294782929122448,
            -0.01401530671864748,
            0.004560642410069704,
            0.023629995062947273,
            -1.4324934454634786e-05,
            0.018815983086824417,
            -0.017189087346196175,
            0.01698905974626541,
            -0.019322721287608147,
            -0.015028783120214939,
            -0.017975864931941032,
            -0.007247687317430973,
            -0.009167958050966263,
            0.02002948708832264,
            0.021403014659881592,
            0.00963469035923481,
            -0.02092294581234455,
            -0.007207681890577078,
            0.005680799949914217,
            0.01781584322452545,
            -0.016815701499581337,
            -0.014482039958238602,
            -0.019189368933439255,
            0.023830022662878036,
            0.016175610944628716,
            -0.02591031603515148,
            -0.03267126902937889,
            -0.05371423065662384,
            0.02012283354997635,
            -0.000785110576543957,
            0.030564304441213608,
            0.008974596858024597,
            -0.022456495091319084,
            0.012995163910090923,
            -0.029284123331308365,
            -0.012835141271352768,
            0.03224454075098038,
            0.002288655610755086,
            0.0055507817305624485,
            0.0018986007198691368,
            0.006704277358949184,
            -0.014161994680762291,
            -0.0231099221855402,
            0.01802920550107956,
            0.017975864931941032,
            0.023736676201224327,
            -0.006900971755385399,
            0.017149081453680992,
            0.021949758753180504,
            0.0017919189995154738,
            -0.005604122765362263,
            0.017375780269503593,
            -0.001429367926903069,
            0.005760811269283295,
            0.030110906809568405,
            -0.014455368742346764,
            0.007487721275538206,
            -0.0011893342016264796,
            0.01284180860966444,
            0.03205784782767296,
            -0.020056158304214478,
            -0.004427290055900812,
            -0.03920552134513855,
            0.023029910400509834,
            0.010188101790845394,
            0.03371141478419304,
            -0.013248532079160213,
            -0.018642624840140343,
            0.008454523980617523,
            0.013828613795340061,
            -0.017135746777057648,
            0.007401042152196169,
            -0.036805182695388794,
            0.007361036725342274,
            0.004623984452337027,
            -0.0066509367898106575,
            -0.022496500983834267,
            -0.6383832097053528,
            -0.026083674281835556,
            0.011181574314832687,
            -0.022229798138141632,
            0.015642203390598297,
            0.0144286984577775,
            0.010294782929122448,
            0.008834577165544033,
            -0.014375357888638973,
            0.016895713284611702,
            -0.0036705168895423412,
            -0.0242567490786314,
            -0.012355073355138302,
            -0.017922524362802505,
            -0.006840963382273912,
            -0.024243414402008057,
            0.01041479967534542,
            -0.013495233841240406,
            0.005487439688295126,
            0.0028720712289214134,
            -0.015815559774637222,
            0.03184448555111885,
            -0.023856693878769875,
            -0.017122412100434303,
            -0.011888341046869755,
            -0.010554819367825985,
            0.008614546619355679,
            -0.008961262181401253,
            -0.0003694270271807909,
            -0.004160585813224316,
            -0.011228247545659542,
            0.02679044008255005,
            0.026337042450904846,
            -0.008227825164794922,
            0.03952556475996971,
            0.010241442359983921,
            -0.007987791672348976,
            0.011988354846835136,
            -9.897226846078411e-05,
            0.0432327538728714,
            -0.004000563640147448,
            -0.03136441856622696,
            0.01694905385375023,
            -0.006024181842803955,
            0.006390899885445833,
            0.009814715944230556,
            -0.0008313670987263322,
            0.020896276459097862,
            0.00922129862010479,
            -0.009908062405884266,
            -0.0033804760314524174,
            0.011508287861943245,
            -0.0031371084041893482,
            -6.230043800314888e-05,
            0.016562333330512047,
            -0.014135324396193027,
            0.02120298519730568,
            -0.01952274888753891,
            -0.0004071406729053706,
            -0.006010846700519323,
            0.010381462052464485,
            0.003970559220761061,
            -0.004810677375644445,
            -0.03733859211206436,
            -0.005260740872472525,
            0.007574399933218956,
            0.0069809830747544765,
            0.004774005617946386,
            0.007561064790934324,
            -0.04285936802625656,
            0.007201014086604118,
            0.016322297975420952,
            0.0020636238623410463,
            -0.011081560514867306,
            -0.007681081537157297,
            0.026750434190034866,
            0.01434868760406971,
            -0.007314363494515419,
            -0.006454242393374443,
            0.013001831248402596,
            0.010374794714152813,
            -0.01533549278974533,
            0.010788186453282833,
            0.027577217668294907,
            -0.023349955677986145,
            0.009194628335535526,
            -0.004710663575679064,
            -0.011328262276947498,
            -0.00980804767459631,
            0.003322134492918849,
            -0.0023269944358617067,
            0.025150207802653313,
            -0.009768042713403702,
            -0.042939379811286926,
            0.020109498873353004,
            0.004330609925091267,
            0.004727332387119532,
            -0.00904794130474329,
            0.008327839896082878,
            -0.022469831630587578,
            -0.004547307267785072,
            -0.019882800057530403,
            0.027390524744987488,
            0.015042118728160858,
            0.00933464802801609,
            -0.006587594281882048,
            0.022136451676487923,
            0.0007267690380103886,
            -0.00228532194159925,
            0.0009918063879013062,
            -8.381388761335984e-05,
            0.007681081537157297,
            -0.017055734992027283,
            -0.01434868760406971,
            -0.003103770548477769,
            -0.03584504872560501,
            0.012601775117218494,
            0.0008517866372130811,
            -0.008134478703141212,
            -0.009741371497511864,
            0.0019202703842893243,
            0.024656806141138077,
            0.008567873388528824,
            0.004580644890666008,
            0.011261586099863052,
            0.015562191605567932,
            -0.009461332112550735,
            0.012568436563014984,
            -0.00733436644077301,
            0.002718716161325574,
            -0.00025170212029479444,
            0.002108630258589983,
            0.01781584322452545,
            -0.010228106752038002,
            0.02216312102973461,
            0.02409672737121582,
            0.0046206507831811905,
            -0.0006038350402377546,
            -0.020562896504998207,
            0.0012026693439111114,
            -0.011868338100612164,
            -0.012488425709307194,
            0.006294219754636288,
            -0.014415363781154156,
            -0.04717997834086418,
            -0.016722355037927628,
            0.02098962292075157,
            0.015135465189814568,
            0.0012943489709869027,
            -0.01685570739209652,
            0.008741230703890324,
            -0.018389256671071053,
            -0.012808470986783504,
            -0.0009709700825624168,
            -0.001995281083509326,
            -0.010181433521211147,
            -0.004353946540504694,
            -0.05952838435769081,
            -0.0096013518050313,
            -0.04533971846103668,
            0.006727613974362612,
            0.03352472186088562,
            0.0033721416257321835,
            -0.00436394801363349,
            -0.007681081537157297,
            -0.033258017152547836,
            -0.005140724126249552,
            0.03653847798705101,
            -0.015082123689353466,
            -0.027950603514909744,
            0.006354228127747774,
            -0.0450730137526989,
            0.04003230482339859,
            0.01612227037549019,
            -0.01740245148539543,
            -0.0044839647598564625,
            -0.014988777227699757,
            0.004083908628672361,
            -0.031604450196027756,
            -0.015095459297299385,
            0.0010584824485704303,
            0.020109498873353004,
            0.0012585106305778027,
            -0.017189087346196175,
            0.02555026486515999,
            -0.0022236465010792017,
            -0.012055031023919582,
            -0.0006071688258089125,
            -0.029390806332230568,
            0.047473352402448654,
            0.002633704338222742,
            -0.019389396533370018,
            0.009187960997223854,
            0.017509132623672485,
            0.012181716039776802,
            -0.004353946540504694,
            0.02888406813144684,
            -0.0010501479264348745,
            -0.018562614917755127,
            0.0138552850112319,
            -0.0036605154164135456,
            0.019189368933439255,
            -0.003178780898451805,
            -0.03485824167728424,
            -0.013308540917932987,
            -0.008447856642305851,
            0.010388129390776157,
            -0.03920552134513855,
            -0.0034504858776926994,
            0.00650758296251297,
            0.011181574314832687,
            -0.01901601068675518,
            -0.012581772170960903,
            -0.021629713475704193,
            0.01392196025699377,
            0.01990947127342224,
            -0.011021552607417107,
            0.002422007732093334,
            -0.0055174436420202255,
            0.013268535025417805,
            0.012075033970177174,
            -0.014655397273600101,
            0.024270085617899895,
            -0.002778724767267704,
            -0.005184063222259283,
            -0.0003223370586056262,
            0.0032287880312651396,
            0.005104052368551493,
            0.025950321927666664,
            -0.03421815112233162,
            0.0017044066917151213,
            0.03101770207285881,
            0.007821101695299149,
            0.019136028364300728,
            0.0048806872218847275,
            -0.0024486782494932413,
            0.015962248668074608,
            -0.010881532914936543,
            0.02883072756230831,
            -0.011514955200254917,
            0.017122412100434303,
            0.001248509157449007,
            -0.009361318312585354,
            -0.011134901084005833,
            0.02469681203365326,
            0.0333380289375782,
            0.03813870623707771,
            0.03349805250763893,
            -0.017482461407780647,
            -0.006940977647900581,
            -0.012628445401787758,
            -0.013788608834147453,
            -0.004540639463812113,
            0.009768042713403702,
            0.04120580479502678,
            -0.009981405921280384,
            0.015482180751860142,
            0.005974174477159977,
            0.0027420527767390013,
            0.020309526473283768,
            0.007141005713492632,
            0.016735689714550972,
            0.005267408676445484,
            -0.005374090280383825,
            0.032937973737716675,
            0.0012951823882758617,
            0.007094332482665777,
            -0.013895289972424507,
            -0.0007371871615760028,
            0.00447729742154479,
            -0.016749026253819466,
            -0.017282433807849884,
            -0.0014060313114896417,
            -0.025123538449406624,
            0.019056016579270363,
            0.02066957764327526,
            -0.014322017319500446,
            0.03405813127756119,
            0.0005142390727996826,
            0.040112316608428955,
            -0.012008357793092728,
            -0.02021618001163006,
            0.03336469829082489,
            -0.01080818846821785,
            -0.015002112835645676,
            -0.013681926764547825,
            -0.019762782379984856,
            0.014415363781154156,
            -0.005027374718338251,
            -0.0027270507998764515,
            -0.0025253556668758392,
            -0.0008492862689308822,
            -0.023283278569579124,
            0.0034904915373772383,
            0.00037067721132189035,
            0.020816264674067497,
            0.014282011426985264,
            -0.018295910209417343,
            0.004490632563829422,
            0.011054890230298042,
            0.003813870484009385,
            0.012795135378837585,
            -0.029497487470507622,
            -0.016562333330512047,
            0.021763065829873085,
            -0.01116157229989767,
            -0.03952556475996971,
            -0.021883081644773483,
            -0.026110343635082245,
            -0.02713715471327305,
            0.040085647255182266,
            -0.011888341046869755,
            0.022136451676487923,
            -0.005554115399718285,
            0.0242567490786314,
            0.02555026486515999,
            0.009407991543412209,
            -0.0034704888239502907,
            0.027630558237433434,
            0.011928346939384937,
            -0.005354087334126234,
            -0.010068085044622421,
            -0.0007050993153825402,
            0.013168521225452423,
            0.04304606094956398,
            0.03256458789110184,
            -0.00974803976714611,
            0.010461472906172276,
            -0.01704240031540394,
            0.012875146232545376,
            -0.03320467472076416,
            -0.02536357194185257,
            -0.0011718317400664091,
            -0.017895853146910667,
            -0.011321594007313251,
            -0.007901112549006939,
            -0.0026853783056139946,
            -0.005240737926214933,
            -0.016495656222105026,
            -0.007641076110303402,
            -0.018389256671071053,
            -0.0395522378385067,
            0.019856128841638565,
            0.008367844857275486,
            -0.006880969274789095,
            0.021096304059028625,
            0.030617645010352135,
            0.027310512959957123,
            0.014402028173208237,
            0.008981265127658844,
            0.02117631584405899,
            0.02196309342980385,
            -0.005140724126249552,
            -0.019216040149331093,
            0.0032204536255449057,
            -0.016402309760451317,
            0.00904794130474329,
            0.014602056704461575,
            0.00784110464155674,
            0.020549559965729713,
            -0.00772108742967248,
            -0.009901394136250019,
            0.028057284653186798,
            0.008907921612262726,
            0.011394938454031944,
            0.01216171309351921,
            0.007207681890577078,
            -0.014148659072816372,
            0.010581490583717823,
            -0.03504493460059166,
            -0.00036525976611301303,
            0.031791143119335175,
            -0.006360895931720734,
            0.007661079056560993,
            -0.009247968904674053,
            0.01084152702242136,
            -0.027630558237433434,
            -0.006020847707986832,
            0.029630839824676514,
            -0.0015085458289831877,
            -0.018362585455179214,
            -0.0003198367194272578,
            -0.0021619710605591536,
            -0.01740245148539543,
            -0.012695121578872204,
            -0.016375640407204628,
            0.004904023837298155,
            -0.002453678986057639,
            0.002502019051462412,
            -0.0025136873591691256,
            -0.03085767850279808,
            -0.027470534667372704,
            -0.028777386993169785,
            0.019642766565084457,
            -0.001286847866140306,
            -0.024176737293601036,
            -0.049313612282276154,
            -0.014695403166115284,
            0.022069774568080902,
            0.02336329035460949,
            0.017029065638780594,
            -0.017415786162018776,
            -0.02663041651248932,
            -0.00934798363596201,
            0.007987791672348976,
            -0.021736394613981247,
            0.004193923901766539,
            -0.02400338090956211,
            -0.013008498586714268,
            -0.011534958146512508,
            -0.028937408700585365,
            -0.004750669002532959,
            -0.03299131244421005,
            0.013828613795340061,
            -0.009294642135500908,
            0.01660233736038208,
            0.018709301948547363,
            -0.019416067749261856,
            0.017189087346196175,
            0.01101488433778286,
            0.008341174572706223,
            0.011261586099863052,
            0.02142968401312828,
            -0.030937690287828445,
            0.018055876716971397,
            -0.010294782929122448,
            -0.023549983277916908,
            -0.017122412100434303,
            0.006437573116272688,
            0.009481335058808327,
            0.00430060550570488,
            -0.02920411340892315,
            0.011948348954319954,
            -0.014735408127307892,
            0.02057623118162155,
            -0.02467014081776142,
            -7.1364214818459e-05,
            0.005497440695762634,
            0.03685852512717247,
            0.016482321545481682,
            0.016895713284611702,
            0.022803211584687233,
            0.0029120768886059523,
            -0.00465065473690629,
            -0.008261163718998432,
            -0.02492351084947586,
            0.018562614917755127,
            0.0194294024258852,
            -0.010268112644553185,
            0.009821383282542229,
            0.032191202044487,
            -0.005837488919496536,
            -0.035951729863882065,
            0.02536357194185257,
            -0.04083241894841194,
            0.018682630732655525,
            -0.0010476475581526756,
            -0.017642484977841377,
            -0.03285796195268631,
            -0.02396337501704693,
            0.0066842748783528805,
            0.0009509672527201474,
            -0.04283269867300987,
            0.008201154880225658,
            0.0008651218377053738,
            0.0027320515364408493,
            0.0012268394930288196,
            -0.017975864931941032,
            0.023629995062947273,
            -0.02835065871477127,
            -0.018842654302716255,
            0.004237263463437557,
            0.014508710242807865,
            0.015122129581868649,
            -0.01768249087035656,
            0.013508569449186325,
            -0.033258017152547836,
            -0.013895289972424507,
            0.009001268073916435,
            -0.018109217286109924,
            0.004373949486762285,
            0.018055876716971397,
            0.007707752287387848,
            0.01534882839769125,
            0.03581837937235832,
            0.0036538478452712297,
            0.01958942599594593,
            0.021763065829873085,
            -0.004887355025857687,
            0.009214631281793118,
            -0.023856693878769875,
            0.009614687412977219,
            -0.0017619148129597306,
            0.011034887284040451,
            0.00962135475128889,
            -0.01570887863636017,
            -0.01452204491943121,
            -0.0052207354456186295,
            0.019802788272500038,
            0.010608160868287086,
            -0.007567732594907284,
            -0.018042540177702904,
            -0.02867070399224758,
            -0.023403296247124672,
            -0.024776821956038475,
            0.0019369394285604358,
            -0.003133774735033512,
            -0.005497440695762634,
            -0.05158059671521187,
            -0.01812255196273327,
            0.0008013628539629281,
            0.0031587781850248575,
            0.008267831057310104,
            0.0008297002059407532,
            0.020389538258314133,
            0.0036538478452712297,
            -0.007654411252588034,
            0.009061275981366634,
            0.0030070901848375797,
            -0.014468704350292683,
            -0.007314363494515419,
            -0.02752387709915638,
            -0.011081560514867306,
            0.03640512749552727,
            0.021149644628167152,
            -0.00500737177208066,
            0.00585749140009284,
            -0.010301451198756695,
            -0.001103488728404045,
            0.013228530064225197,
            0.0029070761520415545,
            -0.013155185617506504,
            -0.025243554264307022,
            -0.014708737842738628,
            -0.021389678120613098,
            -0.030804337933659554,
            -0.012588439509272575,
            0.02904408983886242,
            0.013721932657063007,
            -0.008114475756883621,
            -0.029124101623892784,
            0.032964643090963364,
            -0.0002389919973211363,
            -0.0033037986140698195,
            0.004023900255560875,
            0.024883504956960678,
            0.032937973737716675,
            -0.001675235922448337,
            0.011381602846086025,
            0.016242288053035736,
            0.0022786541376262903,
            -0.006347560789436102,
            0.0021619710605591536,
            0.004997370298951864,
            -0.03035094030201435,
            -0.000267537689069286,
            -0.009788044728338718,
            -0.0005438266089186072,
            -0.010421467944979668,
            0.016909047961235046,
            0.01042813528329134,
            -0.03117772378027439,
            -0.02053622528910637,
            0.014322017319500446,
            -0.007281025405973196,
            0.023923369124531746,
            -0.002218645764514804,
            -0.019322721287608147,
            -0.001751913339830935,
            0.013868619687855244,
            -0.01653566211462021,
            0.02869737520813942,
            0.002815396524965763,
            0.00013929045235272497,
            0.019096022471785545,
            0.02708381414413452,
            -0.010001408867537975,
            0.022816546261310577,
            -0.01444203406572342,
            0.008687890134751797,
            0.00265037314966321,
            0.006967647932469845,
            -0.020362867042422295,
            -0.011654974892735481,
            0.006797623820602894,
            0.020749589428305626,
            -0.012535098940134048,
            0.017122412100434303,
            0.007974456064403057,
            -0.011614969000220299,
            0.00662760017439723,
            0.009134619496762753,
            -0.0014418697683140635,
            0.03083100914955139,
            -0.02787059172987938,
            -0.005124054849147797,
            -0.0007338533760048449,
            0.004497299902141094,
            0.017375780269503593,
            -0.013788608834147453,
            0.001751913339830935,
            0.0007451049168594182,
            -0.017029065638780594,
            0.01570887863636017,
            0.025563599541783333,
            0.024150067940354347,
            -0.023763347417116165,
            0.0072743576020002365,
            0.005717471707612276,
            -0.0014535380760207772,
            -0.002433676039800048,
            0.006204206962138414,
            0.01993614062666893,
            -0.03555167466402054,
            -0.013788608834147453,
            0.034111469984054565,
            0.014282011426985264,
            0.020149504765868187,
            -0.00895459484308958,
            -0.011481616646051407,
            -0.014455368742346764,
            0.02984420396387577,
            -0.04373949393630028,
            0.007947785779833794,
            0.012395079247653484,
            -0.013681926764547825,
            -0.013208527117967606,
            -0.018269238993525505,
            -0.015575527213513851,
            -0.01293515507131815,
            0.0007967788842506707,
            -0.0015035450924187899,
            -0.022229798138141632,
            -0.0019869464449584484,
            -0.005197398830205202,
            -0.005670798476785421,
            0.007994459010660648,
            -0.022136451676487923,
            -0.020896276459097862,
            0.018936000764369965,
            0.0206962488591671,
            -0.019376061856746674,
            -0.01366859208792448,
            -0.012435084208846092,
            0.0029220781289041042,
            -0.00328212883323431,
            0.01271512359380722,
            -0.027977272868156433,
            -0.01452204491943121,
            0.0016694017685949802,
            0.01758914440870285,
            -0.011928346939384937,
            -0.04101911187171936,
            -0.0011543292785063386,
            -0.006974315736442804,
            0.009461332112550735,
            -0.0018819316755980253,
            -0.015028783120214939,
            0.022083109244704247,
            -0.01878931373357773,
            -0.008787903934717178,
            0.02591031603515148,
            -0.00884124543517828,
            -0.010481475852429867,
            0.003190449206158519,
            -0.00792111549526453,
            0.013341878540813923,
            0.02479015849530697,
            -0.0097147012129426,
            -0.034964922815561295,
            -0.011754988692700863,
            -0.003883880330249667,
            -0.017882518470287323,
            -0.008487861603498459,
            -0.015548856928944588,
            0.051633939146995544,
            0.031444426625967026,
            -0.010228106752038002,
            -0.010674837045371532,
            0.0024153401609510183,
            -0.025830304250121117,
            0.003423815593123436,
            -0.005870827008038759,
            0.0070809973403811455,
            0.0010659834370017052,
            -0.021229656413197517,
            -0.0023453303147107363,
            0.017735831439495087,
            -0.0004996536881662905,
            -0.004964032210409641,
            -0.008261163718998432,
            -0.0018719303188845515,
            -0.021002957597374916,
            -0.01841592602431774,
            0.00025232721236534417,
            -0.012681785970926285,
            -0.03933887183666229,
            0.014708737842738628,
            0.028617363423109055,
            -0.006250880192965269,
            0.0172424279153347,
            0.04387284442782402,
            -0.00871456041932106,
            0.001788585213944316,
            -0.00974803976714611,
            0.016655679792165756,
            0.02186974696815014,
            0.017389114946126938,
            0.011168239638209343,
            -0.026270367205142975,
            0.01154162548482418,
            0.04171254113316536,
            -0.0037538621108978987,
            -0.02050955593585968,
            0.002850401448085904,
            0.0047906748950481415,
            0.00784110464155674,
            -0.012101704254746437,
            -0.021856412291526794,
            -0.0020186176989227533,
            0.0019469409016892314,
            -0.019856128841638565,
            0.005077381618320942,
            0.01781584322452545,
            -0.02888406813144684,
            0.0073076956905424595,
            0.0028604029212146997,
            0.00657092547044158,
            -0.03157778084278107,
            0.017722496762871742,
            -0.010734844952821732,
            -0.0012285063276067376,
            0.01917603425681591,
            -0.0012593440478667617,
            0.0016493989387527108,
            -0.016655679792165756,
            0.005594121292233467,
            0.009928064420819283,
            -0.006174203008413315,
            -0.03301798179745674,
            0.001644398202188313,
            -0.023216603323817253,
            0.008587876334786415,
            0.007734422571957111,
            -0.044966332614421844,
            -0.006710945162922144,
            -0.0020486218854784966,
            -0.019882800057530403,
            -0.017255762591958046,
            -0.004980701487511396,
            0.000838034669868648,
            -0.005024041049182415,
            0.01698905974626541,
            -0.0060508521273732185,
            -0.016709020361304283,
            -0.01949607953429222,
            0.005604122765362263,
            0.02142968401312828,
            0.00415058434009552,
            0.02695046178996563,
            0.228085458278656,
            -0.029284123331308365,
            -0.00036505141179077327,
            0.00515739293769002,
            0.003147109877318144,
            0.019802788272500038,
            -0.0013276869431138039,
            -0.0026303704362362623,
            -0.009774710051715374,
            -0.008867915719747543,
            0.04149917885661125,
            0.006914306897670031,
            -0.01312851533293724,
            -0.0017452457686886191,
            0.018242569640278816,
            -0.02336329035460949,
            -0.014148659072816372,
            -0.03421815112233162,
            -0.012601775117218494,
            0.01242841687053442,
            0.005134056322276592,
            0.01216171309351921,
            0.002530356403440237,
            -0.0008076137164607644,
            0.028937408700585365,
            0.004067239351570606,
            0.0032337887678295374,
            -0.0024486782494932413,
            0.021149644628167152,
            0.004924026783555746,
            -0.006197539623826742,
            -0.005454101599752903,
            0.011701648123562336,
            0.02085627056658268,
            -0.010248109698295593,
            -0.0009134619613178074,
            -0.029977554455399513,
            0.003218786558136344,
            0.018749307841062546,
            -0.017122412100434303,
            0.027657227590680122,
            -0.007541061844676733,
            0.010268112644553185,
            -0.04133915528655052,
            0.008041132241487503,
            -0.0009776377119123936,
            -0.004910691641271114,
            -0.006454242393374443,
            -0.004447293002158403,
            0.02202976867556572,
            -0.008947926573455334,
            0.02545691840350628,
            0.02098962292075157,
            0.005194064695388079,
            0.008361177518963814,
            0.00035546673461794853,
            -0.009854720905423164,
            0.009041273035109043,
            0.027977272868156433,
            0.005440766457468271,
            -0.008961262181401253,
            -0.0001698155829217285,
            -0.0010168098378926516,
            0.02301657572388649,
            -0.01312851533293724,
            0.022109780460596085,
            -0.02612367831170559,
            0.0231099221855402,
            0.0356316864490509,
            0.021509695798158646,
            0.007054326590150595,
            -0.023790016770362854,
            -0.007741090375930071,
            0.014895430766046047,
            -0.04389951750636101,
            -0.03424482420086861,
            0.051313892006874084,
            0.012488425709307194,
            0.039472226053476334,
            0.03003089688718319,
            0.009701366536319256,
            0.001888599363155663,
            -0.018175892531871796,
            -0.015495515428483486,
            -0.0010159764206036925,
            -0.04376616328954697,
            0.029790861532092094,
            -0.023056579753756523,
            0.001105155679397285,
            -0.024736817926168442,
            0.003963891416788101,
            -0.008494529873132706,
            -0.01133492961525917,
            -0.007301028352230787,
            0.008241160772740841,
            0.013481899164617062,
            0.017162416130304337,
            0.012588439509272575,
            -0.010454805567860603,
            -0.009167958050966263,
            -0.02545691840350628,
            0.06902305036783218,
            -0.0025470254477113485,
            0.00812781136482954,
            0.002713715424761176,
            0.01816255785524845,
            -0.013681926764547825,
            0.014055312611162663,
            0.008461191318929195,
            -0.011741654016077518,
            0.016802366822957993,
            -0.03139108791947365,
            0.008001127280294895,
            -0.0035538338124752045,
            0.011814997531473637,
            0.012115039862692356,
            0.029577499255537987,
            0.003673850791528821,
            0.018269238993525505,
            -0.0144286984577775,
            0.01733577437698841,
            -0.022403154522180557,
            0.009834717959165573,
            0.015162135474383831,
            0.005960839334875345,
            -0.004463961813598871,
            -0.018215898424386978,
            0.010241442359983921,
            -0.03221787139773369,
            -0.0032254543621093035,
            -0.014868760481476784,
            -0.02552359364926815,
            0.022109780460596085,
            -0.02835065871477127,
            -0.02536357194185257,
            -0.003680518362671137,
            -0.001108489464968443,
            -0.024963514879345894,
            -0.02416340261697769,
            0.02170972339808941,
            0.0020536226220428944,
            0.008201154880225658,
            0.016842372715473175,
            -7.469802221748978e-05,
            -0.020136170089244843,
            0.002567028161138296,
            -0.008741230703890324,
            -0.0031604450196027756,
            0.00635756179690361,
            -0.02727050706744194,
            -0.008581208065152168,
            -0.017882518470287323,
            -0.011448279023170471,
            -0.0006538420566357672,
            0.02216312102973461,
            0.010534817352890968,
            -0.006407569162547588,
            -0.008154481649398804,
            0.00683429604396224,
            0.005110719706863165,
            -0.02209644578397274,
            -0.021896416321396828,
            0.0017252429388463497,
            -0.0078077660873532295,
            -0.009794712997972965,
            -0.0016910715494304895,
            -0.16845038533210754,
            -0.0010668168542906642,
            0.025443583726882935,
            -0.046779923141002655,
            0.011995022185146809,
            0.0024936844129115343,
            0.02479015849530697,
            -0.030110906809568405,
            -0.05054045096039772,
            0.014602056704461575,
            -0.00010371252574259415,
            0.0020119501277804375,
            0.0016485655214637518,
            -0.009794712997972965,
            -0.03248457610607147,
            0.006497581955045462,
            -0.0242567490786314,
            0.01818922907114029,
            0.014588721096515656,
            -0.0027403859421610832,
            0.0377119779586792,
            -0.03800535202026367,
            0.026297036558389664,
            -0.011694980785250664,
            0.03547166287899017,
            -0.0014026975259184837,
            0.013775273226201534,
            0.019189368933439255,
            -0.01246175542473793,
            -0.04491299018263817,
            -0.0012218387564644217,
            -0.006454242393374443,
            0.03488491475582123,
            -0.022043105214834213,
            -0.009341315366327763,
            0.015148799866437912,
            -0.010774850845336914,
            -0.0057208058424293995,
            -0.00950800534337759,
            0.006914306897670031,
            0.029897544533014297,
            0.0073410337790846825,
            0.0043272762559354305,
            -0.008227825164794922,
            -0.02479015849530697,
            0.0020919612143188715,
            0.029124101623892784,
            -0.012181716039776802,
            0.004500634036958218,
            -0.011054890230298042,
            0.008987932465970516,
            -0.0006059186416678131,
            -0.002218645764514804,
            0.011148236691951752,
            0.0030404282733798027,
            0.003563835285604,
            0.028430670499801636,
            0.018469268456101418,
            -0.011301591992378235,
            -0.017189087346196175,
            -0.009121284820139408,
            -0.007814433425664902,
            0.0042939381673932076,
            0.009814715944230556,
            -0.015922242775559425,
            -0.011654974892735481,
            -0.009501338005065918,
            -0.012868478894233704,
            -0.007901112549006939,
            0.021002957597374916,
            -0.031444426625967026,
            -0.011895008385181427,
            -0.01071484200656414,
            -0.0052774096839129925,
            0.005614123772829771,
            0.017855847254395485,
            -0.017802506685256958,
            -0.022043105214834213,
            0.00307876686565578,
            -0.002470347797498107,
            -0.02237648516893387,
            0.021536367014050484,
            -0.0094279944896698,
            0.0033371367026120424,
            0.005594121292233467,
            0.016775695607066154,
            -0.0004917359328828752,
            0.02492351084947586,
            -0.019816124811768532,
            0.0009793045464903116,
            0.039125509560108185,
            -0.012721791863441467,
            0.0025470254477113485,
            -0.0034138141199946404,
            -0.010041413828730583,
            0.011214912869036198,
            0.0067742872051894665,
            0.0020119501277804375,
            0.004130581859499216,
            -0.018269238993525505,
            0.010754847899079323,
            -0.0027687232941389084,
            -0.015242146328091621,
            0.020136170089244843,
            0.03507160767912865,
            0.016175610944628716,
            -0.015562191605567932,
            0.003032093634828925,
            0.04336610808968544,
            0.00842785369604826,
            -0.01685570739209652,
            -0.004907357972115278,
            0.0009042940218932927,
            -0.0053107477724552155,
            -0.004023900255560875,
            0.03003089688718319,
            -0.0006146698724478483,
            -0.017469126731157303,
            -0.0029120768886059523,
            -0.019322721287608147,
            0.048806872218847275,
            -0.028404001146554947,
            -0.03237789496779442,
            0.029257453978061676,
            -0.011568295769393444,
            -0.0314977690577507,
            -0.11649640649557114,
            -0.025030191987752914,
            -0.011788327246904373,
            0.01285514421761036,
            -0.004233929794281721,
            0.009074611589312553,
            0.015922242775559425,
            0.024203408509492874,
            -0.03248457610607147,
            0.04189923405647278,
            -0.00016929468256421387,
            -0.01910935714840889,
            0.01704240031540394,
            -0.015175470151007175,
            -0.0073076956905424595,
            0.001351857092231512,
            0.012901817448437214,
            0.010488144122064114,
            -0.01071484200656414,
            0.028083955869078636,
            0.012068366631865501,
            -0.031071042641997337,
            0.021483026444911957,
            -0.016895713284611702,
            -0.004440625198185444,
            0.0018135887803509831,
            -0.017882518470287323,
            0.008594543673098087,
            -0.0010759849101305008,
            -0.022496500983834267,
            0.016322297975420952,
            -0.036778513342142105,
            0.0071143354289233685,
            -0.012515095993876457,
            0.01876264251768589,
            -0.01033478882163763,
            0.003907216712832451,
            -0.021483026444911957,
            -0.016802366822957993,
            -0.006744283251464367,
            -0.011421608738601208,
            0.005090716760605574,
            0.0011609968496486545,
            0.0032421231735497713,
            -0.01736244559288025,
            -0.0172424279153347,
            -0.022109780460596085,
            0.027030473574995995,
            0.0031721133273094893,
            -0.03235122188925743,
            -0.028270648792386055,
            -0.023643329739570618,
            -0.05024707689881325,
            -0.0038005353417247534,
            0.0024736816994845867,
            -0.014055312611162663,
            -0.001976944971829653,
            0.0036371788010001183,
            -0.01090153492987156,
            -0.005967507138848305,
            0.003607174614444375,
            -0.009701366536319256,
            -0.011088227853178978,
            0.029790861532092094,
            0.01476207934319973,
            -0.005707470700144768,
            -0.010001408867537975,
            -0.03240456432104111,
            -0.006384232547134161,
            -0.026110343635082245,
            -0.0015960581367835402,
            0.00963469035923481,
            0.0008342841756530106,
            0.007801098749041557,
            -0.03565835580229759,
            -0.0012410080526024103,
            -0.018709301948547363,
            -0.01910935714840889,
            -0.0038038690108805895,
            0.010368126444518566,
            -0.026577075943350792,
            -0.003317133756354451,
            0.008401183411478996,
            -0.024803493171930313,
            -0.013721932657063007,
            -0.0030054233502596617,
            -0.006550922524183989,
            -0.005297412630170584,
            -0.002998755546286702,
            -0.02708381414413452,
            0.0029920879751443863,
            0.02933746576309204,
            -0.011468281969428062,
            0.000514655839651823,
            -0.00992139708250761,
            0.021096304059028625,
            -0.0028037282172590494,
            -0.023816687986254692,
            0.0264437235891819,
            0.02308325096964836,
            -0.017949193716049194,
            -0.005944170523434877,
            -0.02301657572388649,
            0.0164423156529665,
            -0.009501338005065918,
            -0.008721228688955307,
            0.002960416954010725,
            -0.011468281969428062,
            0.04200591519474983,
            0.016709020361304283,
            -0.022563178092241287,
            0.008581208065152168,
            -0.009968070313334465,
            0.021136309951543808,
            0.0014035309432074428,
            -0.027217166498303413,
            -0.014922101981937885,
            -0.022403154522180557,
            0.014735408127307892,
            -0.022936563938856125,
            0.022149786353111267,
            0.01940273307263851,
            -0.005264074541628361,
            -0.014868760481476784,
            0.021029628813266754,
            0.0015785556752234697,
            0.007894445210695267,
            -0.002181974006816745,
            -0.00782776903361082,
            0.021056298166513443,
            -0.010708174668252468,
            -0.022176455706357956,
            0.031071042641997337,
            -0.01892266422510147,
            0.005000704433768988,
            0.014255341142416,
            -0.002030286006629467,
            0.015988918021321297,
            0.001396863372065127,
            -0.004817345179617405,
            0.022523172199726105,
            -0.006697610020637512,
            -0.005140724126249552,
            -0.02260318398475647,
            0.029070761054754257,
            -0.01850927248597145,
            0.013388551771640778,
            -0.007467718329280615,
            -0.022456495091319084,
            -0.0038738788571208715,
            0.01860261894762516,
            -0.018709301948547363,
            0.01704240031540394,
            0.0144286984577775,
            0.01222172100096941,
            -0.031444426625967026,
            -0.0016002253396436572,
            -0.004183922428637743,
            -0.016682349145412445,
            0.0001160580141004175,
            0.01542883925139904,
            0.003500493010506034,
            0.02701713889837265,
            0.004593980498611927,
            -0.009021270088851452,
            -0.010654834099113941,
            0.004697328433394432,
            0.013208527117967606,
            0.003597173374146223,
            0.013828613795340061,
            -0.003433816833421588,
            -0.012121707201004028,
            7.297902629943565e-05,
            0.011188242584466934,
            0.0011493285419419408,
            0.0016310630599036813,
            0.0344848558306694,
            0.0188559889793396,
            0.020522890612483025,
            -0.0037671972531825304,
            -0.00636422960087657,
            0.0476067028939724,
            0.014575386419892311,
            -0.019696107134222984,
            -0.016455650329589844,
            0.009374653920531273,
            0.01374860294163227,
            0.00465732254087925,
            0.006860966328531504,
            -0.02559027075767517,
            -0.011908343993127346,
            0.005557449534535408,
            0.007961121387779713,
            0.008087805472314358,
            -0.01366859208792448,
            0.03723191097378731,
            0.0037638633511960506,
            0.01720242202281952,
            0.0061842044815421104,
            0.030137578025460243,
            0.02304324507713318,
            0.006227543577551842,
            0.00256369449198246,
            -0.003430483164265752,
            -0.00278539233841002,
            0.010514814406633377,
            -0.004057237878441811,
            0.027630558237433434,
            -0.011401605792343616,
            -0.01742912083864212,
            0.019376061856746674,
            0.027230501174926758,
            0.0093146450817585,
            -0.008567873388528824,
            -0.01745579205453396,
            -0.007207681890577078,
            -0.017309105023741722,
            0.01660233736038208,
            -0.007327698636800051,
            -0.024870168417692184,
            -0.02463013492524624,
            0.005114053376019001,
            0.011975020170211792,
            -0.015562191605567932,
            0.049126919358968735,
            -0.013468563556671143,
            0.019829459488391876,
            -0.009454664774239063,
            0.025656946003437042,
            -0.020722918212413788,
            0.013681926764547825,
            0.02987087331712246,
            0.01765581965446472,
            -0.006704277358949184,
            0.010174766182899475,
            0.001715241582132876,
            -0.007134337909519672,
            -0.011434943415224552,
            0.02168305404484272,
            0.02603033185005188,
            -0.019949475303292274,
            0.07227684557437897,
            0.024483447894454002,
            0.007394374813884497,
            -0.0018469267524778843,
            -0.02349664270877838,
            0.022109780460596085,
            0.005240737926214933,
            0.015988918021321297,
            -0.023643329739570618,
            0.0017569140763953328,
            0.004317274782806635,
            0.0042572664096951485,
            0.0002967084583360702,
            -0.02361665852367878,
            0.0012376742670312524,
            -0.0051207211799919605,
            0.013175188563764095,
            0.003943888936191797,
            0.003112104954198003,
            0.00040609887219034135,
            0.03669850155711174,
            -0.009928064420819283,
            0.02244316041469574,
            0.0023703337647020817,
            -0.023643329739570618,
            0.004730666056275368,
            0.009594684466719627,
            0.03152443841099739,
            -0.0029920879751443863,
            -0.03699187561869621,
            0.016869042068719864,
            0.013301873579621315,
            -0.029470816254615784,
            -0.0031887823715806007,
            0.002603699918836355,
            -0.011301591992378235,
            -0.012708456255495548,
            -0.011701648123562336,
            0.0018019204726442695,
            0.012915152125060558,
            0.020749589428305626,
            0.03573836758732796,
            -0.021069634705781937,
            -0.023723341524600983,
            -0.0015610532136633992,
            0.002572028897702694,
            -0.01204836368560791,
            -0.02200309932231903,
            -0.006360895931720734
          ],
          "start_time": 1697137314.954796,
          "end_time": 1697137315.767159,
          "error": null,
          "children": [
            {
              "name": "openai.api_resources.embedding.Embedding.create",
              "type": "LLM",
              "inputs": {
                "input": "What is the advantage of fine-tuning BERT over using feature-based approaches?",
                "engine": "embedding"
              },
              "output": {
                "object": "list",
                "data": [
                  {
                    "object": "embedding",
                    "index": 0,
                    "embedding": [
                      -0.016869042068719864,
                      -0.0003010840737260878,
                      0.022069774568080902,
                      -0.01818922907114029,
                      -0.0022569845896214247,
                      0.013475230894982815,
                      -0.0065809269435703754,
                      0.004023900255560875,
                      -0.032884631305933,
                      -0.019216040149331093,
                      0.02651040069758892,
                      0.030404282733798027,
                      -0.024683475494384766,
                      0.0007167676230892539,
                      -0.018149223178625107,
                      -0.015762219205498695,
                      0.036565151065588,
                      0.028804056346416473,
                      0.005427430849522352,
                      -0.025256890803575516,
                      -0.011601634323596954,
                      -0.009081278927624226,
                      -0.024976851418614388,
                      -0.011588298715651035,
                      -0.02108296938240528,
                      -0.0024053386878222227,
                      0.045446399599313736,
                      -0.020642906427383423,
                      -0.01844259724020958,
                      0.0037738648243248463,
                      0.026403717696666718,
                      0.02196309342980385,
                      -0.016562333330512047,
                      -0.02032286301255226,
                      -0.011454946361482143,
                      0.009701366536319256,
                      -0.0029054090846329927,
                      -0.01225505955517292,
                      0.024843499064445496,
                      -0.01254176627844572,
                      0.01114156935364008,
                      0.032751280814409256,
                      0.011975020170211792,
                      -0.015215476043522358,
                      -0.0032737944275140762,
                      0.0264437235891819,
                      0.022069774568080902,
                      -0.00033192173577845097,
                      -0.03133774548768997,
                      0.020056158304214478,
                      0.002888740273192525,
                      0.030564304441213608,
                      -0.016975723206996918,
                      0.003323801327496767,
                      -0.05198065564036369,
                      -0.007454383186995983,
                      -0.006334225181490183,
                      0.005664131138473749,
                      -0.0017552472418174148,
                      -0.025723623111844063,
                      0.002136967610567808,
                      0.006624266039580107,
                      -0.00904794130474329,
                      0.01246175542473793,
                      -0.012735126540064812,
                      -0.013095177710056305,
                      -0.005047377664595842,
                      0.022816546261310577,
                      0.010688171721994877,
                      -0.0024503450840711594,
                      0.012088368646800518,
                      0.02571028843522072,
                      -0.01602892391383648,
                      -0.006470911204814911,
                      0.027577217668294907,
                      -0.014828754588961601,
                      -0.0028720712289214134,
                      0.021576372906565666,
                      0.01041479967534542,
                      -0.0031987838447093964,
                      -0.0014477039221674204,
                      0.006234211381524801,
                      -0.01090153492987156,
                      0.014268675819039345,
                      0.02060290239751339,
                      0.014735408127307892,
                      0.0026270365342497826,
                      0.0010318119311705232,
                      0.003170446492731571,
                      0.015802225098013878,
                      0.005537446588277817,
                      0.018749307841062546,
                      0.03416481241583824,
                      0.02221646159887314,
                      0.011074893176555634,
                      -0.0029904211405664682,
                      -0.005217401310801506,
                      0.014562050811946392,
                      -0.00028566524269990623,
                      0.004353946540504694,
                      -0.009027938358485699,
                      0.009401324205100536,
                      -0.029817532747983932,
                      -0.011661642231047153,
                      -0.004167253617197275,
                      0.009067943319678307,
                      0.018629290163517,
                      -0.005494107026606798,
                      -0.0012393412180244923,
                      -0.0036205099895596504,
                      -0.03037761151790619,
                      0.014482039958238602,
                      -0.014588721096515656,
                      -0.014175329357385635,
                      -0.0054140957072377205,
                      0.0022019767202436924,
                      0.0005113219958730042,
                      -0.010294782929122448,
                      -0.01401530671864748,
                      0.004560642410069704,
                      0.023629995062947273,
                      -1.4324934454634786e-05,
                      0.018815983086824417,
                      -0.017189087346196175,
                      0.01698905974626541,
                      -0.019322721287608147,
                      -0.015028783120214939,
                      -0.017975864931941032,
                      -0.007247687317430973,
                      -0.009167958050966263,
                      0.02002948708832264,
                      0.021403014659881592,
                      0.00963469035923481,
                      -0.02092294581234455,
                      -0.007207681890577078,
                      0.005680799949914217,
                      0.01781584322452545,
                      -0.016815701499581337,
                      -0.014482039958238602,
                      -0.019189368933439255,
                      0.023830022662878036,
                      0.016175610944628716,
                      -0.02591031603515148,
                      -0.03267126902937889,
                      -0.05371423065662384,
                      0.02012283354997635,
                      -0.000785110576543957,
                      0.030564304441213608,
                      0.008974596858024597,
                      -0.022456495091319084,
                      0.012995163910090923,
                      -0.029284123331308365,
                      -0.012835141271352768,
                      0.03224454075098038,
                      0.002288655610755086,
                      0.0055507817305624485,
                      0.0018986007198691368,
                      0.006704277358949184,
                      -0.014161994680762291,
                      -0.0231099221855402,
                      0.01802920550107956,
                      0.017975864931941032,
                      0.023736676201224327,
                      -0.006900971755385399,
                      0.017149081453680992,
                      0.021949758753180504,
                      0.0017919189995154738,
                      -0.005604122765362263,
                      0.017375780269503593,
                      -0.001429367926903069,
                      0.005760811269283295,
                      0.030110906809568405,
                      -0.014455368742346764,
                      0.007487721275538206,
                      -0.0011893342016264796,
                      0.01284180860966444,
                      0.03205784782767296,
                      -0.020056158304214478,
                      -0.004427290055900812,
                      -0.03920552134513855,
                      0.023029910400509834,
                      0.010188101790845394,
                      0.03371141478419304,
                      -0.013248532079160213,
                      -0.018642624840140343,
                      0.008454523980617523,
                      0.013828613795340061,
                      -0.017135746777057648,
                      0.007401042152196169,
                      -0.036805182695388794,
                      0.007361036725342274,
                      0.004623984452337027,
                      -0.0066509367898106575,
                      -0.022496500983834267,
                      -0.6383832097053528,
                      -0.026083674281835556,
                      0.011181574314832687,
                      -0.022229798138141632,
                      0.015642203390598297,
                      0.0144286984577775,
                      0.010294782929122448,
                      0.008834577165544033,
                      -0.014375357888638973,
                      0.016895713284611702,
                      -0.0036705168895423412,
                      -0.0242567490786314,
                      -0.012355073355138302,
                      -0.017922524362802505,
                      -0.006840963382273912,
                      -0.024243414402008057,
                      0.01041479967534542,
                      -0.013495233841240406,
                      0.005487439688295126,
                      0.0028720712289214134,
                      -0.015815559774637222,
                      0.03184448555111885,
                      -0.023856693878769875,
                      -0.017122412100434303,
                      -0.011888341046869755,
                      -0.010554819367825985,
                      0.008614546619355679,
                      -0.008961262181401253,
                      -0.0003694270271807909,
                      -0.004160585813224316,
                      -0.011228247545659542,
                      0.02679044008255005,
                      0.026337042450904846,
                      -0.008227825164794922,
                      0.03952556475996971,
                      0.010241442359983921,
                      -0.007987791672348976,
                      0.011988354846835136,
                      -9.897226846078411e-05,
                      0.0432327538728714,
                      -0.004000563640147448,
                      -0.03136441856622696,
                      0.01694905385375023,
                      -0.006024181842803955,
                      0.006390899885445833,
                      0.009814715944230556,
                      -0.0008313670987263322,
                      0.020896276459097862,
                      0.00922129862010479,
                      -0.009908062405884266,
                      -0.0033804760314524174,
                      0.011508287861943245,
                      -0.0031371084041893482,
                      -6.230043800314888e-05,
                      0.016562333330512047,
                      -0.014135324396193027,
                      0.02120298519730568,
                      -0.01952274888753891,
                      -0.0004071406729053706,
                      -0.006010846700519323,
                      0.010381462052464485,
                      0.003970559220761061,
                      -0.004810677375644445,
                      -0.03733859211206436,
                      -0.005260740872472525,
                      0.007574399933218956,
                      0.0069809830747544765,
                      0.004774005617946386,
                      0.007561064790934324,
                      -0.04285936802625656,
                      0.007201014086604118,
                      0.016322297975420952,
                      0.0020636238623410463,
                      -0.011081560514867306,
                      -0.007681081537157297,
                      0.026750434190034866,
                      0.01434868760406971,
                      -0.007314363494515419,
                      -0.006454242393374443,
                      0.013001831248402596,
                      0.010374794714152813,
                      -0.01533549278974533,
                      0.010788186453282833,
                      0.027577217668294907,
                      -0.023349955677986145,
                      0.009194628335535526,
                      -0.004710663575679064,
                      -0.011328262276947498,
                      -0.00980804767459631,
                      0.003322134492918849,
                      -0.0023269944358617067,
                      0.025150207802653313,
                      -0.009768042713403702,
                      -0.042939379811286926,
                      0.020109498873353004,
                      0.004330609925091267,
                      0.004727332387119532,
                      -0.00904794130474329,
                      0.008327839896082878,
                      -0.022469831630587578,
                      -0.004547307267785072,
                      -0.019882800057530403,
                      0.027390524744987488,
                      0.015042118728160858,
                      0.00933464802801609,
                      -0.006587594281882048,
                      0.022136451676487923,
                      0.0007267690380103886,
                      -0.00228532194159925,
                      0.0009918063879013062,
                      -8.381388761335984e-05,
                      0.007681081537157297,
                      -0.017055734992027283,
                      -0.01434868760406971,
                      -0.003103770548477769,
                      -0.03584504872560501,
                      0.012601775117218494,
                      0.0008517866372130811,
                      -0.008134478703141212,
                      -0.009741371497511864,
                      0.0019202703842893243,
                      0.024656806141138077,
                      0.008567873388528824,
                      0.004580644890666008,
                      0.011261586099863052,
                      0.015562191605567932,
                      -0.009461332112550735,
                      0.012568436563014984,
                      -0.00733436644077301,
                      0.002718716161325574,
                      -0.00025170212029479444,
                      0.002108630258589983,
                      0.01781584322452545,
                      -0.010228106752038002,
                      0.02216312102973461,
                      0.02409672737121582,
                      0.0046206507831811905,
                      -0.0006038350402377546,
                      -0.020562896504998207,
                      0.0012026693439111114,
                      -0.011868338100612164,
                      -0.012488425709307194,
                      0.006294219754636288,
                      -0.014415363781154156,
                      -0.04717997834086418,
                      -0.016722355037927628,
                      0.02098962292075157,
                      0.015135465189814568,
                      0.0012943489709869027,
                      -0.01685570739209652,
                      0.008741230703890324,
                      -0.018389256671071053,
                      -0.012808470986783504,
                      -0.0009709700825624168,
                      -0.001995281083509326,
                      -0.010181433521211147,
                      -0.004353946540504694,
                      -0.05952838435769081,
                      -0.0096013518050313,
                      -0.04533971846103668,
                      0.006727613974362612,
                      0.03352472186088562,
                      0.0033721416257321835,
                      -0.00436394801363349,
                      -0.007681081537157297,
                      -0.033258017152547836,
                      -0.005140724126249552,
                      0.03653847798705101,
                      -0.015082123689353466,
                      -0.027950603514909744,
                      0.006354228127747774,
                      -0.0450730137526989,
                      0.04003230482339859,
                      0.01612227037549019,
                      -0.01740245148539543,
                      -0.0044839647598564625,
                      -0.014988777227699757,
                      0.004083908628672361,
                      -0.031604450196027756,
                      -0.015095459297299385,
                      0.0010584824485704303,
                      0.020109498873353004,
                      0.0012585106305778027,
                      -0.017189087346196175,
                      0.02555026486515999,
                      -0.0022236465010792017,
                      -0.012055031023919582,
                      -0.0006071688258089125,
                      -0.029390806332230568,
                      0.047473352402448654,
                      0.002633704338222742,
                      -0.019389396533370018,
                      0.009187960997223854,
                      0.017509132623672485,
                      0.012181716039776802,
                      -0.004353946540504694,
                      0.02888406813144684,
                      -0.0010501479264348745,
                      -0.018562614917755127,
                      0.0138552850112319,
                      -0.0036605154164135456,
                      0.019189368933439255,
                      -0.003178780898451805,
                      -0.03485824167728424,
                      -0.013308540917932987,
                      -0.008447856642305851,
                      0.010388129390776157,
                      -0.03920552134513855,
                      -0.0034504858776926994,
                      0.00650758296251297,
                      0.011181574314832687,
                      -0.01901601068675518,
                      -0.012581772170960903,
                      -0.021629713475704193,
                      0.01392196025699377,
                      0.01990947127342224,
                      -0.011021552607417107,
                      0.002422007732093334,
                      -0.0055174436420202255,
                      0.013268535025417805,
                      0.012075033970177174,
                      -0.014655397273600101,
                      0.024270085617899895,
                      -0.002778724767267704,
                      -0.005184063222259283,
                      -0.0003223370586056262,
                      0.0032287880312651396,
                      0.005104052368551493,
                      0.025950321927666664,
                      -0.03421815112233162,
                      0.0017044066917151213,
                      0.03101770207285881,
                      0.007821101695299149,
                      0.019136028364300728,
                      0.0048806872218847275,
                      -0.0024486782494932413,
                      0.015962248668074608,
                      -0.010881532914936543,
                      0.02883072756230831,
                      -0.011514955200254917,
                      0.017122412100434303,
                      0.001248509157449007,
                      -0.009361318312585354,
                      -0.011134901084005833,
                      0.02469681203365326,
                      0.0333380289375782,
                      0.03813870623707771,
                      0.03349805250763893,
                      -0.017482461407780647,
                      -0.006940977647900581,
                      -0.012628445401787758,
                      -0.013788608834147453,
                      -0.004540639463812113,
                      0.009768042713403702,
                      0.04120580479502678,
                      -0.009981405921280384,
                      0.015482180751860142,
                      0.005974174477159977,
                      0.0027420527767390013,
                      0.020309526473283768,
                      0.007141005713492632,
                      0.016735689714550972,
                      0.005267408676445484,
                      -0.005374090280383825,
                      0.032937973737716675,
                      0.0012951823882758617,
                      0.007094332482665777,
                      -0.013895289972424507,
                      -0.0007371871615760028,
                      0.00447729742154479,
                      -0.016749026253819466,
                      -0.017282433807849884,
                      -0.0014060313114896417,
                      -0.025123538449406624,
                      0.019056016579270363,
                      0.02066957764327526,
                      -0.014322017319500446,
                      0.03405813127756119,
                      0.0005142390727996826,
                      0.040112316608428955,
                      -0.012008357793092728,
                      -0.02021618001163006,
                      0.03336469829082489,
                      -0.01080818846821785,
                      -0.015002112835645676,
                      -0.013681926764547825,
                      -0.019762782379984856,
                      0.014415363781154156,
                      -0.005027374718338251,
                      -0.0027270507998764515,
                      -0.0025253556668758392,
                      -0.0008492862689308822,
                      -0.023283278569579124,
                      0.0034904915373772383,
                      0.00037067721132189035,
                      0.020816264674067497,
                      0.014282011426985264,
                      -0.018295910209417343,
                      0.004490632563829422,
                      0.011054890230298042,
                      0.003813870484009385,
                      0.012795135378837585,
                      -0.029497487470507622,
                      -0.016562333330512047,
                      0.021763065829873085,
                      -0.01116157229989767,
                      -0.03952556475996971,
                      -0.021883081644773483,
                      -0.026110343635082245,
                      -0.02713715471327305,
                      0.040085647255182266,
                      -0.011888341046869755,
                      0.022136451676487923,
                      -0.005554115399718285,
                      0.0242567490786314,
                      0.02555026486515999,
                      0.009407991543412209,
                      -0.0034704888239502907,
                      0.027630558237433434,
                      0.011928346939384937,
                      -0.005354087334126234,
                      -0.010068085044622421,
                      -0.0007050993153825402,
                      0.013168521225452423,
                      0.04304606094956398,
                      0.03256458789110184,
                      -0.00974803976714611,
                      0.010461472906172276,
                      -0.01704240031540394,
                      0.012875146232545376,
                      -0.03320467472076416,
                      -0.02536357194185257,
                      -0.0011718317400664091,
                      -0.017895853146910667,
                      -0.011321594007313251,
                      -0.007901112549006939,
                      -0.0026853783056139946,
                      -0.005240737926214933,
                      -0.016495656222105026,
                      -0.007641076110303402,
                      -0.018389256671071053,
                      -0.0395522378385067,
                      0.019856128841638565,
                      0.008367844857275486,
                      -0.006880969274789095,
                      0.021096304059028625,
                      0.030617645010352135,
                      0.027310512959957123,
                      0.014402028173208237,
                      0.008981265127658844,
                      0.02117631584405899,
                      0.02196309342980385,
                      -0.005140724126249552,
                      -0.019216040149331093,
                      0.0032204536255449057,
                      -0.016402309760451317,
                      0.00904794130474329,
                      0.014602056704461575,
                      0.00784110464155674,
                      0.020549559965729713,
                      -0.00772108742967248,
                      -0.009901394136250019,
                      0.028057284653186798,
                      0.008907921612262726,
                      0.011394938454031944,
                      0.01216171309351921,
                      0.007207681890577078,
                      -0.014148659072816372,
                      0.010581490583717823,
                      -0.03504493460059166,
                      -0.00036525976611301303,
                      0.031791143119335175,
                      -0.006360895931720734,
                      0.007661079056560993,
                      -0.009247968904674053,
                      0.01084152702242136,
                      -0.027630558237433434,
                      -0.006020847707986832,
                      0.029630839824676514,
                      -0.0015085458289831877,
                      -0.018362585455179214,
                      -0.0003198367194272578,
                      -0.0021619710605591536,
                      -0.01740245148539543,
                      -0.012695121578872204,
                      -0.016375640407204628,
                      0.004904023837298155,
                      -0.002453678986057639,
                      0.002502019051462412,
                      -0.0025136873591691256,
                      -0.03085767850279808,
                      -0.027470534667372704,
                      -0.028777386993169785,
                      0.019642766565084457,
                      -0.001286847866140306,
                      -0.024176737293601036,
                      -0.049313612282276154,
                      -0.014695403166115284,
                      0.022069774568080902,
                      0.02336329035460949,
                      0.017029065638780594,
                      -0.017415786162018776,
                      -0.02663041651248932,
                      -0.00934798363596201,
                      0.007987791672348976,
                      -0.021736394613981247,
                      0.004193923901766539,
                      -0.02400338090956211,
                      -0.013008498586714268,
                      -0.011534958146512508,
                      -0.028937408700585365,
                      -0.004750669002532959,
                      -0.03299131244421005,
                      0.013828613795340061,
                      -0.009294642135500908,
                      0.01660233736038208,
                      0.018709301948547363,
                      -0.019416067749261856,
                      0.017189087346196175,
                      0.01101488433778286,
                      0.008341174572706223,
                      0.011261586099863052,
                      0.02142968401312828,
                      -0.030937690287828445,
                      0.018055876716971397,
                      -0.010294782929122448,
                      -0.023549983277916908,
                      -0.017122412100434303,
                      0.006437573116272688,
                      0.009481335058808327,
                      0.00430060550570488,
                      -0.02920411340892315,
                      0.011948348954319954,
                      -0.014735408127307892,
                      0.02057623118162155,
                      -0.02467014081776142,
                      -7.1364214818459e-05,
                      0.005497440695762634,
                      0.03685852512717247,
                      0.016482321545481682,
                      0.016895713284611702,
                      0.022803211584687233,
                      0.0029120768886059523,
                      -0.00465065473690629,
                      -0.008261163718998432,
                      -0.02492351084947586,
                      0.018562614917755127,
                      0.0194294024258852,
                      -0.010268112644553185,
                      0.009821383282542229,
                      0.032191202044487,
                      -0.005837488919496536,
                      -0.035951729863882065,
                      0.02536357194185257,
                      -0.04083241894841194,
                      0.018682630732655525,
                      -0.0010476475581526756,
                      -0.017642484977841377,
                      -0.03285796195268631,
                      -0.02396337501704693,
                      0.0066842748783528805,
                      0.0009509672527201474,
                      -0.04283269867300987,
                      0.008201154880225658,
                      0.0008651218377053738,
                      0.0027320515364408493,
                      0.0012268394930288196,
                      -0.017975864931941032,
                      0.023629995062947273,
                      -0.02835065871477127,
                      -0.018842654302716255,
                      0.004237263463437557,
                      0.014508710242807865,
                      0.015122129581868649,
                      -0.01768249087035656,
                      0.013508569449186325,
                      -0.033258017152547836,
                      -0.013895289972424507,
                      0.009001268073916435,
                      -0.018109217286109924,
                      0.004373949486762285,
                      0.018055876716971397,
                      0.007707752287387848,
                      0.01534882839769125,
                      0.03581837937235832,
                      0.0036538478452712297,
                      0.01958942599594593,
                      0.021763065829873085,
                      -0.004887355025857687,
                      0.009214631281793118,
                      -0.023856693878769875,
                      0.009614687412977219,
                      -0.0017619148129597306,
                      0.011034887284040451,
                      0.00962135475128889,
                      -0.01570887863636017,
                      -0.01452204491943121,
                      -0.0052207354456186295,
                      0.019802788272500038,
                      0.010608160868287086,
                      -0.007567732594907284,
                      -0.018042540177702904,
                      -0.02867070399224758,
                      -0.023403296247124672,
                      -0.024776821956038475,
                      0.0019369394285604358,
                      -0.003133774735033512,
                      -0.005497440695762634,
                      -0.05158059671521187,
                      -0.01812255196273327,
                      0.0008013628539629281,
                      0.0031587781850248575,
                      0.008267831057310104,
                      0.0008297002059407532,
                      0.020389538258314133,
                      0.0036538478452712297,
                      -0.007654411252588034,
                      0.009061275981366634,
                      0.0030070901848375797,
                      -0.014468704350292683,
                      -0.007314363494515419,
                      -0.02752387709915638,
                      -0.011081560514867306,
                      0.03640512749552727,
                      0.021149644628167152,
                      -0.00500737177208066,
                      0.00585749140009284,
                      -0.010301451198756695,
                      -0.001103488728404045,
                      0.013228530064225197,
                      0.0029070761520415545,
                      -0.013155185617506504,
                      -0.025243554264307022,
                      -0.014708737842738628,
                      -0.021389678120613098,
                      -0.030804337933659554,
                      -0.012588439509272575,
                      0.02904408983886242,
                      0.013721932657063007,
                      -0.008114475756883621,
                      -0.029124101623892784,
                      0.032964643090963364,
                      -0.0002389919973211363,
                      -0.0033037986140698195,
                      0.004023900255560875,
                      0.024883504956960678,
                      0.032937973737716675,
                      -0.001675235922448337,
                      0.011381602846086025,
                      0.016242288053035736,
                      0.0022786541376262903,
                      -0.006347560789436102,
                      0.0021619710605591536,
                      0.004997370298951864,
                      -0.03035094030201435,
                      -0.000267537689069286,
                      -0.009788044728338718,
                      -0.0005438266089186072,
                      -0.010421467944979668,
                      0.016909047961235046,
                      0.01042813528329134,
                      -0.03117772378027439,
                      -0.02053622528910637,
                      0.014322017319500446,
                      -0.007281025405973196,
                      0.023923369124531746,
                      -0.002218645764514804,
                      -0.019322721287608147,
                      -0.001751913339830935,
                      0.013868619687855244,
                      -0.01653566211462021,
                      0.02869737520813942,
                      0.002815396524965763,
                      0.00013929045235272497,
                      0.019096022471785545,
                      0.02708381414413452,
                      -0.010001408867537975,
                      0.022816546261310577,
                      -0.01444203406572342,
                      0.008687890134751797,
                      0.00265037314966321,
                      0.006967647932469845,
                      -0.020362867042422295,
                      -0.011654974892735481,
                      0.006797623820602894,
                      0.020749589428305626,
                      -0.012535098940134048,
                      0.017122412100434303,
                      0.007974456064403057,
                      -0.011614969000220299,
                      0.00662760017439723,
                      0.009134619496762753,
                      -0.0014418697683140635,
                      0.03083100914955139,
                      -0.02787059172987938,
                      -0.005124054849147797,
                      -0.0007338533760048449,
                      0.004497299902141094,
                      0.017375780269503593,
                      -0.013788608834147453,
                      0.001751913339830935,
                      0.0007451049168594182,
                      -0.017029065638780594,
                      0.01570887863636017,
                      0.025563599541783333,
                      0.024150067940354347,
                      -0.023763347417116165,
                      0.0072743576020002365,
                      0.005717471707612276,
                      -0.0014535380760207772,
                      -0.002433676039800048,
                      0.006204206962138414,
                      0.01993614062666893,
                      -0.03555167466402054,
                      -0.013788608834147453,
                      0.034111469984054565,
                      0.014282011426985264,
                      0.020149504765868187,
                      -0.00895459484308958,
                      -0.011481616646051407,
                      -0.014455368742346764,
                      0.02984420396387577,
                      -0.04373949393630028,
                      0.007947785779833794,
                      0.012395079247653484,
                      -0.013681926764547825,
                      -0.013208527117967606,
                      -0.018269238993525505,
                      -0.015575527213513851,
                      -0.01293515507131815,
                      0.0007967788842506707,
                      -0.0015035450924187899,
                      -0.022229798138141632,
                      -0.0019869464449584484,
                      -0.005197398830205202,
                      -0.005670798476785421,
                      0.007994459010660648,
                      -0.022136451676487923,
                      -0.020896276459097862,
                      0.018936000764369965,
                      0.0206962488591671,
                      -0.019376061856746674,
                      -0.01366859208792448,
                      -0.012435084208846092,
                      0.0029220781289041042,
                      -0.00328212883323431,
                      0.01271512359380722,
                      -0.027977272868156433,
                      -0.01452204491943121,
                      0.0016694017685949802,
                      0.01758914440870285,
                      -0.011928346939384937,
                      -0.04101911187171936,
                      -0.0011543292785063386,
                      -0.006974315736442804,
                      0.009461332112550735,
                      -0.0018819316755980253,
                      -0.015028783120214939,
                      0.022083109244704247,
                      -0.01878931373357773,
                      -0.008787903934717178,
                      0.02591031603515148,
                      -0.00884124543517828,
                      -0.010481475852429867,
                      0.003190449206158519,
                      -0.00792111549526453,
                      0.013341878540813923,
                      0.02479015849530697,
                      -0.0097147012129426,
                      -0.034964922815561295,
                      -0.011754988692700863,
                      -0.003883880330249667,
                      -0.017882518470287323,
                      -0.008487861603498459,
                      -0.015548856928944588,
                      0.051633939146995544,
                      0.031444426625967026,
                      -0.010228106752038002,
                      -0.010674837045371532,
                      0.0024153401609510183,
                      -0.025830304250121117,
                      0.003423815593123436,
                      -0.005870827008038759,
                      0.0070809973403811455,
                      0.0010659834370017052,
                      -0.021229656413197517,
                      -0.0023453303147107363,
                      0.017735831439495087,
                      -0.0004996536881662905,
                      -0.004964032210409641,
                      -0.008261163718998432,
                      -0.0018719303188845515,
                      -0.021002957597374916,
                      -0.01841592602431774,
                      0.00025232721236534417,
                      -0.012681785970926285,
                      -0.03933887183666229,
                      0.014708737842738628,
                      0.028617363423109055,
                      -0.006250880192965269,
                      0.0172424279153347,
                      0.04387284442782402,
                      -0.00871456041932106,
                      0.001788585213944316,
                      -0.00974803976714611,
                      0.016655679792165756,
                      0.02186974696815014,
                      0.017389114946126938,
                      0.011168239638209343,
                      -0.026270367205142975,
                      0.01154162548482418,
                      0.04171254113316536,
                      -0.0037538621108978987,
                      -0.02050955593585968,
                      0.002850401448085904,
                      0.0047906748950481415,
                      0.00784110464155674,
                      -0.012101704254746437,
                      -0.021856412291526794,
                      -0.0020186176989227533,
                      0.0019469409016892314,
                      -0.019856128841638565,
                      0.005077381618320942,
                      0.01781584322452545,
                      -0.02888406813144684,
                      0.0073076956905424595,
                      0.0028604029212146997,
                      0.00657092547044158,
                      -0.03157778084278107,
                      0.017722496762871742,
                      -0.010734844952821732,
                      -0.0012285063276067376,
                      0.01917603425681591,
                      -0.0012593440478667617,
                      0.0016493989387527108,
                      -0.016655679792165756,
                      0.005594121292233467,
                      0.009928064420819283,
                      -0.006174203008413315,
                      -0.03301798179745674,
                      0.001644398202188313,
                      -0.023216603323817253,
                      0.008587876334786415,
                      0.007734422571957111,
                      -0.044966332614421844,
                      -0.006710945162922144,
                      -0.0020486218854784966,
                      -0.019882800057530403,
                      -0.017255762591958046,
                      -0.004980701487511396,
                      0.000838034669868648,
                      -0.005024041049182415,
                      0.01698905974626541,
                      -0.0060508521273732185,
                      -0.016709020361304283,
                      -0.01949607953429222,
                      0.005604122765362263,
                      0.02142968401312828,
                      0.00415058434009552,
                      0.02695046178996563,
                      0.228085458278656,
                      -0.029284123331308365,
                      -0.00036505141179077327,
                      0.00515739293769002,
                      0.003147109877318144,
                      0.019802788272500038,
                      -0.0013276869431138039,
                      -0.0026303704362362623,
                      -0.009774710051715374,
                      -0.008867915719747543,
                      0.04149917885661125,
                      0.006914306897670031,
                      -0.01312851533293724,
                      -0.0017452457686886191,
                      0.018242569640278816,
                      -0.02336329035460949,
                      -0.014148659072816372,
                      -0.03421815112233162,
                      -0.012601775117218494,
                      0.01242841687053442,
                      0.005134056322276592,
                      0.01216171309351921,
                      0.002530356403440237,
                      -0.0008076137164607644,
                      0.028937408700585365,
                      0.004067239351570606,
                      0.0032337887678295374,
                      -0.0024486782494932413,
                      0.021149644628167152,
                      0.004924026783555746,
                      -0.006197539623826742,
                      -0.005454101599752903,
                      0.011701648123562336,
                      0.02085627056658268,
                      -0.010248109698295593,
                      -0.0009134619613178074,
                      -0.029977554455399513,
                      0.003218786558136344,
                      0.018749307841062546,
                      -0.017122412100434303,
                      0.027657227590680122,
                      -0.007541061844676733,
                      0.010268112644553185,
                      -0.04133915528655052,
                      0.008041132241487503,
                      -0.0009776377119123936,
                      -0.004910691641271114,
                      -0.006454242393374443,
                      -0.004447293002158403,
                      0.02202976867556572,
                      -0.008947926573455334,
                      0.02545691840350628,
                      0.02098962292075157,
                      0.005194064695388079,
                      0.008361177518963814,
                      0.00035546673461794853,
                      -0.009854720905423164,
                      0.009041273035109043,
                      0.027977272868156433,
                      0.005440766457468271,
                      -0.008961262181401253,
                      -0.0001698155829217285,
                      -0.0010168098378926516,
                      0.02301657572388649,
                      -0.01312851533293724,
                      0.022109780460596085,
                      -0.02612367831170559,
                      0.0231099221855402,
                      0.0356316864490509,
                      0.021509695798158646,
                      0.007054326590150595,
                      -0.023790016770362854,
                      -0.007741090375930071,
                      0.014895430766046047,
                      -0.04389951750636101,
                      -0.03424482420086861,
                      0.051313892006874084,
                      0.012488425709307194,
                      0.039472226053476334,
                      0.03003089688718319,
                      0.009701366536319256,
                      0.001888599363155663,
                      -0.018175892531871796,
                      -0.015495515428483486,
                      -0.0010159764206036925,
                      -0.04376616328954697,
                      0.029790861532092094,
                      -0.023056579753756523,
                      0.001105155679397285,
                      -0.024736817926168442,
                      0.003963891416788101,
                      -0.008494529873132706,
                      -0.01133492961525917,
                      -0.007301028352230787,
                      0.008241160772740841,
                      0.013481899164617062,
                      0.017162416130304337,
                      0.012588439509272575,
                      -0.010454805567860603,
                      -0.009167958050966263,
                      -0.02545691840350628,
                      0.06902305036783218,
                      -0.0025470254477113485,
                      0.00812781136482954,
                      0.002713715424761176,
                      0.01816255785524845,
                      -0.013681926764547825,
                      0.014055312611162663,
                      0.008461191318929195,
                      -0.011741654016077518,
                      0.016802366822957993,
                      -0.03139108791947365,
                      0.008001127280294895,
                      -0.0035538338124752045,
                      0.011814997531473637,
                      0.012115039862692356,
                      0.029577499255537987,
                      0.003673850791528821,
                      0.018269238993525505,
                      -0.0144286984577775,
                      0.01733577437698841,
                      -0.022403154522180557,
                      0.009834717959165573,
                      0.015162135474383831,
                      0.005960839334875345,
                      -0.004463961813598871,
                      -0.018215898424386978,
                      0.010241442359983921,
                      -0.03221787139773369,
                      -0.0032254543621093035,
                      -0.014868760481476784,
                      -0.02552359364926815,
                      0.022109780460596085,
                      -0.02835065871477127,
                      -0.02536357194185257,
                      -0.003680518362671137,
                      -0.001108489464968443,
                      -0.024963514879345894,
                      -0.02416340261697769,
                      0.02170972339808941,
                      0.0020536226220428944,
                      0.008201154880225658,
                      0.016842372715473175,
                      -7.469802221748978e-05,
                      -0.020136170089244843,
                      0.002567028161138296,
                      -0.008741230703890324,
                      -0.0031604450196027756,
                      0.00635756179690361,
                      -0.02727050706744194,
                      -0.008581208065152168,
                      -0.017882518470287323,
                      -0.011448279023170471,
                      -0.0006538420566357672,
                      0.02216312102973461,
                      0.010534817352890968,
                      -0.006407569162547588,
                      -0.008154481649398804,
                      0.00683429604396224,
                      0.005110719706863165,
                      -0.02209644578397274,
                      -0.021896416321396828,
                      0.0017252429388463497,
                      -0.0078077660873532295,
                      -0.009794712997972965,
                      -0.0016910715494304895,
                      -0.16845038533210754,
                      -0.0010668168542906642,
                      0.025443583726882935,
                      -0.046779923141002655,
                      0.011995022185146809,
                      0.0024936844129115343,
                      0.02479015849530697,
                      -0.030110906809568405,
                      -0.05054045096039772,
                      0.014602056704461575,
                      -0.00010371252574259415,
                      0.0020119501277804375,
                      0.0016485655214637518,
                      -0.009794712997972965,
                      -0.03248457610607147,
                      0.006497581955045462,
                      -0.0242567490786314,
                      0.01818922907114029,
                      0.014588721096515656,
                      -0.0027403859421610832,
                      0.0377119779586792,
                      -0.03800535202026367,
                      0.026297036558389664,
                      -0.011694980785250664,
                      0.03547166287899017,
                      -0.0014026975259184837,
                      0.013775273226201534,
                      0.019189368933439255,
                      -0.01246175542473793,
                      -0.04491299018263817,
                      -0.0012218387564644217,
                      -0.006454242393374443,
                      0.03488491475582123,
                      -0.022043105214834213,
                      -0.009341315366327763,
                      0.015148799866437912,
                      -0.010774850845336914,
                      -0.0057208058424293995,
                      -0.00950800534337759,
                      0.006914306897670031,
                      0.029897544533014297,
                      0.0073410337790846825,
                      0.0043272762559354305,
                      -0.008227825164794922,
                      -0.02479015849530697,
                      0.0020919612143188715,
                      0.029124101623892784,
                      -0.012181716039776802,
                      0.004500634036958218,
                      -0.011054890230298042,
                      0.008987932465970516,
                      -0.0006059186416678131,
                      -0.002218645764514804,
                      0.011148236691951752,
                      0.0030404282733798027,
                      0.003563835285604,
                      0.028430670499801636,
                      0.018469268456101418,
                      -0.011301591992378235,
                      -0.017189087346196175,
                      -0.009121284820139408,
                      -0.007814433425664902,
                      0.0042939381673932076,
                      0.009814715944230556,
                      -0.015922242775559425,
                      -0.011654974892735481,
                      -0.009501338005065918,
                      -0.012868478894233704,
                      -0.007901112549006939,
                      0.021002957597374916,
                      -0.031444426625967026,
                      -0.011895008385181427,
                      -0.01071484200656414,
                      -0.0052774096839129925,
                      0.005614123772829771,
                      0.017855847254395485,
                      -0.017802506685256958,
                      -0.022043105214834213,
                      0.00307876686565578,
                      -0.002470347797498107,
                      -0.02237648516893387,
                      0.021536367014050484,
                      -0.0094279944896698,
                      0.0033371367026120424,
                      0.005594121292233467,
                      0.016775695607066154,
                      -0.0004917359328828752,
                      0.02492351084947586,
                      -0.019816124811768532,
                      0.0009793045464903116,
                      0.039125509560108185,
                      -0.012721791863441467,
                      0.0025470254477113485,
                      -0.0034138141199946404,
                      -0.010041413828730583,
                      0.011214912869036198,
                      0.0067742872051894665,
                      0.0020119501277804375,
                      0.004130581859499216,
                      -0.018269238993525505,
                      0.010754847899079323,
                      -0.0027687232941389084,
                      -0.015242146328091621,
                      0.020136170089244843,
                      0.03507160767912865,
                      0.016175610944628716,
                      -0.015562191605567932,
                      0.003032093634828925,
                      0.04336610808968544,
                      0.00842785369604826,
                      -0.01685570739209652,
                      -0.004907357972115278,
                      0.0009042940218932927,
                      -0.0053107477724552155,
                      -0.004023900255560875,
                      0.03003089688718319,
                      -0.0006146698724478483,
                      -0.017469126731157303,
                      -0.0029120768886059523,
                      -0.019322721287608147,
                      0.048806872218847275,
                      -0.028404001146554947,
                      -0.03237789496779442,
                      0.029257453978061676,
                      -0.011568295769393444,
                      -0.0314977690577507,
                      -0.11649640649557114,
                      -0.025030191987752914,
                      -0.011788327246904373,
                      0.01285514421761036,
                      -0.004233929794281721,
                      0.009074611589312553,
                      0.015922242775559425,
                      0.024203408509492874,
                      -0.03248457610607147,
                      0.04189923405647278,
                      -0.00016929468256421387,
                      -0.01910935714840889,
                      0.01704240031540394,
                      -0.015175470151007175,
                      -0.0073076956905424595,
                      0.001351857092231512,
                      0.012901817448437214,
                      0.010488144122064114,
                      -0.01071484200656414,
                      0.028083955869078636,
                      0.012068366631865501,
                      -0.031071042641997337,
                      0.021483026444911957,
                      -0.016895713284611702,
                      -0.004440625198185444,
                      0.0018135887803509831,
                      -0.017882518470287323,
                      0.008594543673098087,
                      -0.0010759849101305008,
                      -0.022496500983834267,
                      0.016322297975420952,
                      -0.036778513342142105,
                      0.0071143354289233685,
                      -0.012515095993876457,
                      0.01876264251768589,
                      -0.01033478882163763,
                      0.003907216712832451,
                      -0.021483026444911957,
                      -0.016802366822957993,
                      -0.006744283251464367,
                      -0.011421608738601208,
                      0.005090716760605574,
                      0.0011609968496486545,
                      0.0032421231735497713,
                      -0.01736244559288025,
                      -0.0172424279153347,
                      -0.022109780460596085,
                      0.027030473574995995,
                      0.0031721133273094893,
                      -0.03235122188925743,
                      -0.028270648792386055,
                      -0.023643329739570618,
                      -0.05024707689881325,
                      -0.0038005353417247534,
                      0.0024736816994845867,
                      -0.014055312611162663,
                      -0.001976944971829653,
                      0.0036371788010001183,
                      -0.01090153492987156,
                      -0.005967507138848305,
                      0.003607174614444375,
                      -0.009701366536319256,
                      -0.011088227853178978,
                      0.029790861532092094,
                      0.01476207934319973,
                      -0.005707470700144768,
                      -0.010001408867537975,
                      -0.03240456432104111,
                      -0.006384232547134161,
                      -0.026110343635082245,
                      -0.0015960581367835402,
                      0.00963469035923481,
                      0.0008342841756530106,
                      0.007801098749041557,
                      -0.03565835580229759,
                      -0.0012410080526024103,
                      -0.018709301948547363,
                      -0.01910935714840889,
                      -0.0038038690108805895,
                      0.010368126444518566,
                      -0.026577075943350792,
                      -0.003317133756354451,
                      0.008401183411478996,
                      -0.024803493171930313,
                      -0.013721932657063007,
                      -0.0030054233502596617,
                      -0.006550922524183989,
                      -0.005297412630170584,
                      -0.002998755546286702,
                      -0.02708381414413452,
                      0.0029920879751443863,
                      0.02933746576309204,
                      -0.011468281969428062,
                      0.000514655839651823,
                      -0.00992139708250761,
                      0.021096304059028625,
                      -0.0028037282172590494,
                      -0.023816687986254692,
                      0.0264437235891819,
                      0.02308325096964836,
                      -0.017949193716049194,
                      -0.005944170523434877,
                      -0.02301657572388649,
                      0.0164423156529665,
                      -0.009501338005065918,
                      -0.008721228688955307,
                      0.002960416954010725,
                      -0.011468281969428062,
                      0.04200591519474983,
                      0.016709020361304283,
                      -0.022563178092241287,
                      0.008581208065152168,
                      -0.009968070313334465,
                      0.021136309951543808,
                      0.0014035309432074428,
                      -0.027217166498303413,
                      -0.014922101981937885,
                      -0.022403154522180557,
                      0.014735408127307892,
                      -0.022936563938856125,
                      0.022149786353111267,
                      0.01940273307263851,
                      -0.005264074541628361,
                      -0.014868760481476784,
                      0.021029628813266754,
                      0.0015785556752234697,
                      0.007894445210695267,
                      -0.002181974006816745,
                      -0.00782776903361082,
                      0.021056298166513443,
                      -0.010708174668252468,
                      -0.022176455706357956,
                      0.031071042641997337,
                      -0.01892266422510147,
                      0.005000704433768988,
                      0.014255341142416,
                      -0.002030286006629467,
                      0.015988918021321297,
                      0.001396863372065127,
                      -0.004817345179617405,
                      0.022523172199726105,
                      -0.006697610020637512,
                      -0.005140724126249552,
                      -0.02260318398475647,
                      0.029070761054754257,
                      -0.01850927248597145,
                      0.013388551771640778,
                      -0.007467718329280615,
                      -0.022456495091319084,
                      -0.0038738788571208715,
                      0.01860261894762516,
                      -0.018709301948547363,
                      0.01704240031540394,
                      0.0144286984577775,
                      0.01222172100096941,
                      -0.031444426625967026,
                      -0.0016002253396436572,
                      -0.004183922428637743,
                      -0.016682349145412445,
                      0.0001160580141004175,
                      0.01542883925139904,
                      0.003500493010506034,
                      0.02701713889837265,
                      0.004593980498611927,
                      -0.009021270088851452,
                      -0.010654834099113941,
                      0.004697328433394432,
                      0.013208527117967606,
                      0.003597173374146223,
                      0.013828613795340061,
                      -0.003433816833421588,
                      -0.012121707201004028,
                      7.297902629943565e-05,
                      0.011188242584466934,
                      0.0011493285419419408,
                      0.0016310630599036813,
                      0.0344848558306694,
                      0.0188559889793396,
                      0.020522890612483025,
                      -0.0037671972531825304,
                      -0.00636422960087657,
                      0.0476067028939724,
                      0.014575386419892311,
                      -0.019696107134222984,
                      -0.016455650329589844,
                      0.009374653920531273,
                      0.01374860294163227,
                      0.00465732254087925,
                      0.006860966328531504,
                      -0.02559027075767517,
                      -0.011908343993127346,
                      0.005557449534535408,
                      0.007961121387779713,
                      0.008087805472314358,
                      -0.01366859208792448,
                      0.03723191097378731,
                      0.0037638633511960506,
                      0.01720242202281952,
                      0.0061842044815421104,
                      0.030137578025460243,
                      0.02304324507713318,
                      0.006227543577551842,
                      0.00256369449198246,
                      -0.003430483164265752,
                      -0.00278539233841002,
                      0.010514814406633377,
                      -0.004057237878441811,
                      0.027630558237433434,
                      -0.011401605792343616,
                      -0.01742912083864212,
                      0.019376061856746674,
                      0.027230501174926758,
                      0.0093146450817585,
                      -0.008567873388528824,
                      -0.01745579205453396,
                      -0.007207681890577078,
                      -0.017309105023741722,
                      0.01660233736038208,
                      -0.007327698636800051,
                      -0.024870168417692184,
                      -0.02463013492524624,
                      0.005114053376019001,
                      0.011975020170211792,
                      -0.015562191605567932,
                      0.049126919358968735,
                      -0.013468563556671143,
                      0.019829459488391876,
                      -0.009454664774239063,
                      0.025656946003437042,
                      -0.020722918212413788,
                      0.013681926764547825,
                      0.02987087331712246,
                      0.01765581965446472,
                      -0.006704277358949184,
                      0.010174766182899475,
                      0.001715241582132876,
                      -0.007134337909519672,
                      -0.011434943415224552,
                      0.02168305404484272,
                      0.02603033185005188,
                      -0.019949475303292274,
                      0.07227684557437897,
                      0.024483447894454002,
                      0.007394374813884497,
                      -0.0018469267524778843,
                      -0.02349664270877838,
                      0.022109780460596085,
                      0.005240737926214933,
                      0.015988918021321297,
                      -0.023643329739570618,
                      0.0017569140763953328,
                      0.004317274782806635,
                      0.0042572664096951485,
                      0.0002967084583360702,
                      -0.02361665852367878,
                      0.0012376742670312524,
                      -0.0051207211799919605,
                      0.013175188563764095,
                      0.003943888936191797,
                      0.003112104954198003,
                      0.00040609887219034135,
                      0.03669850155711174,
                      -0.009928064420819283,
                      0.02244316041469574,
                      0.0023703337647020817,
                      -0.023643329739570618,
                      0.004730666056275368,
                      0.009594684466719627,
                      0.03152443841099739,
                      -0.0029920879751443863,
                      -0.03699187561869621,
                      0.016869042068719864,
                      0.013301873579621315,
                      -0.029470816254615784,
                      -0.0031887823715806007,
                      0.002603699918836355,
                      -0.011301591992378235,
                      -0.012708456255495548,
                      -0.011701648123562336,
                      0.0018019204726442695,
                      0.012915152125060558,
                      0.020749589428305626,
                      0.03573836758732796,
                      -0.021069634705781937,
                      -0.023723341524600983,
                      -0.0015610532136633992,
                      0.002572028897702694,
                      -0.01204836368560791,
                      -0.02200309932231903,
                      -0.006360895931720734
                    ]
                  }
                ],
                "model": "ada",
                "usage": {
                  "prompt_tokens": 16,
                  "total_tokens": 16
                }
              },
              "start_time": 1697137314.957793,
              "end_time": 1697137315.754634,
              "error": null,
              "children": null,
              "node_name": null
            }
          ],
          "node_name": "embed_the_question"
        },
        {
          "name": "checkCacheAnswer",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "embeddedQuestion": [
              -0.016869042068719864,
              -0.0003010840737260878,
              0.022069774568080902,
              -0.01818922907114029,
              -0.0022569845896214247,
              0.013475230894982815,
              -0.0065809269435703754,
              0.004023900255560875,
              -0.032884631305933,
              -0.019216040149331093,
              0.02651040069758892,
              0.030404282733798027,
              -0.024683475494384766,
              0.0007167676230892539,
              -0.018149223178625107,
              -0.015762219205498695,
              0.036565151065588,
              0.028804056346416473,
              0.005427430849522352,
              -0.025256890803575516,
              -0.011601634323596954,
              -0.009081278927624226,
              -0.024976851418614388,
              -0.011588298715651035,
              -0.02108296938240528,
              -0.0024053386878222227,
              0.045446399599313736,
              -0.020642906427383423,
              -0.01844259724020958,
              0.0037738648243248463,
              0.026403717696666718,
              0.02196309342980385,
              -0.016562333330512047,
              -0.02032286301255226,
              -0.011454946361482143,
              0.009701366536319256,
              -0.0029054090846329927,
              -0.01225505955517292,
              0.024843499064445496,
              -0.01254176627844572,
              0.01114156935364008,
              0.032751280814409256,
              0.011975020170211792,
              -0.015215476043522358,
              -0.0032737944275140762,
              0.0264437235891819,
              0.022069774568080902,
              -0.00033192173577845097,
              -0.03133774548768997,
              0.020056158304214478,
              0.002888740273192525,
              0.030564304441213608,
              -0.016975723206996918,
              0.003323801327496767,
              -0.05198065564036369,
              -0.007454383186995983,
              -0.006334225181490183,
              0.005664131138473749,
              -0.0017552472418174148,
              -0.025723623111844063,
              0.002136967610567808,
              0.006624266039580107,
              -0.00904794130474329,
              0.01246175542473793,
              -0.012735126540064812,
              -0.013095177710056305,
              -0.005047377664595842,
              0.022816546261310577,
              0.010688171721994877,
              -0.0024503450840711594,
              0.012088368646800518,
              0.02571028843522072,
              -0.01602892391383648,
              -0.006470911204814911,
              0.027577217668294907,
              -0.014828754588961601,
              -0.0028720712289214134,
              0.021576372906565666,
              0.01041479967534542,
              -0.0031987838447093964,
              -0.0014477039221674204,
              0.006234211381524801,
              -0.01090153492987156,
              0.014268675819039345,
              0.02060290239751339,
              0.014735408127307892,
              0.0026270365342497826,
              0.0010318119311705232,
              0.003170446492731571,
              0.015802225098013878,
              0.005537446588277817,
              0.018749307841062546,
              0.03416481241583824,
              0.02221646159887314,
              0.011074893176555634,
              -0.0029904211405664682,
              -0.005217401310801506,
              0.014562050811946392,
              -0.00028566524269990623,
              0.004353946540504694,
              -0.009027938358485699,
              0.009401324205100536,
              -0.029817532747983932,
              -0.011661642231047153,
              -0.004167253617197275,
              0.009067943319678307,
              0.018629290163517,
              -0.005494107026606798,
              -0.0012393412180244923,
              -0.0036205099895596504,
              -0.03037761151790619,
              0.014482039958238602,
              -0.014588721096515656,
              -0.014175329357385635,
              -0.0054140957072377205,
              0.0022019767202436924,
              0.0005113219958730042,
              -0.010294782929122448,
              -0.01401530671864748,
              0.004560642410069704,
              0.023629995062947273,
              -1.4324934454634786e-05,
              0.018815983086824417,
              -0.017189087346196175,
              0.01698905974626541,
              -0.019322721287608147,
              -0.015028783120214939,
              -0.017975864931941032,
              -0.007247687317430973,
              -0.009167958050966263,
              0.02002948708832264,
              0.021403014659881592,
              0.00963469035923481,
              -0.02092294581234455,
              -0.007207681890577078,
              0.005680799949914217,
              0.01781584322452545,
              -0.016815701499581337,
              -0.014482039958238602,
              -0.019189368933439255,
              0.023830022662878036,
              0.016175610944628716,
              -0.02591031603515148,
              -0.03267126902937889,
              -0.05371423065662384,
              0.02012283354997635,
              -0.000785110576543957,
              0.030564304441213608,
              0.008974596858024597,
              -0.022456495091319084,
              0.012995163910090923,
              -0.029284123331308365,
              -0.012835141271352768,
              0.03224454075098038,
              0.002288655610755086,
              0.0055507817305624485,
              0.0018986007198691368,
              0.006704277358949184,
              -0.014161994680762291,
              -0.0231099221855402,
              0.01802920550107956,
              0.017975864931941032,
              0.023736676201224327,
              -0.006900971755385399,
              0.017149081453680992,
              0.021949758753180504,
              0.0017919189995154738,
              -0.005604122765362263,
              0.017375780269503593,
              -0.001429367926903069,
              0.005760811269283295,
              0.030110906809568405,
              -0.014455368742346764,
              0.007487721275538206,
              -0.0011893342016264796,
              0.01284180860966444,
              0.03205784782767296,
              -0.020056158304214478,
              -0.004427290055900812,
              -0.03920552134513855,
              0.023029910400509834,
              0.010188101790845394,
              0.03371141478419304,
              -0.013248532079160213,
              -0.018642624840140343,
              0.008454523980617523,
              0.013828613795340061,
              -0.017135746777057648,
              0.007401042152196169,
              -0.036805182695388794,
              0.007361036725342274,
              0.004623984452337027,
              -0.0066509367898106575,
              -0.022496500983834267,
              -0.6383832097053528,
              -0.026083674281835556,
              0.011181574314832687,
              -0.022229798138141632,
              0.015642203390598297,
              0.0144286984577775,
              0.010294782929122448,
              0.008834577165544033,
              -0.014375357888638973,
              0.016895713284611702,
              -0.0036705168895423412,
              -0.0242567490786314,
              -0.012355073355138302,
              -0.017922524362802505,
              -0.006840963382273912,
              -0.024243414402008057,
              0.01041479967534542,
              -0.013495233841240406,
              0.005487439688295126,
              0.0028720712289214134,
              -0.015815559774637222,
              0.03184448555111885,
              -0.023856693878769875,
              -0.017122412100434303,
              -0.011888341046869755,
              -0.010554819367825985,
              0.008614546619355679,
              -0.008961262181401253,
              -0.0003694270271807909,
              -0.004160585813224316,
              -0.011228247545659542,
              0.02679044008255005,
              0.026337042450904846,
              -0.008227825164794922,
              0.03952556475996971,
              0.010241442359983921,
              -0.007987791672348976,
              0.011988354846835136,
              -9.897226846078411e-05,
              0.0432327538728714,
              -0.004000563640147448,
              -0.03136441856622696,
              0.01694905385375023,
              -0.006024181842803955,
              0.006390899885445833,
              0.009814715944230556,
              -0.0008313670987263322,
              0.020896276459097862,
              0.00922129862010479,
              -0.009908062405884266,
              -0.0033804760314524174,
              0.011508287861943245,
              -0.0031371084041893482,
              -6.230043800314888e-05,
              0.016562333330512047,
              -0.014135324396193027,
              0.02120298519730568,
              -0.01952274888753891,
              -0.0004071406729053706,
              -0.006010846700519323,
              0.010381462052464485,
              0.003970559220761061,
              -0.004810677375644445,
              -0.03733859211206436,
              -0.005260740872472525,
              0.007574399933218956,
              0.0069809830747544765,
              0.004774005617946386,
              0.007561064790934324,
              -0.04285936802625656,
              0.007201014086604118,
              0.016322297975420952,
              0.0020636238623410463,
              -0.011081560514867306,
              -0.007681081537157297,
              0.026750434190034866,
              0.01434868760406971,
              -0.007314363494515419,
              -0.006454242393374443,
              0.013001831248402596,
              0.010374794714152813,
              -0.01533549278974533,
              0.010788186453282833,
              0.027577217668294907,
              -0.023349955677986145,
              0.009194628335535526,
              -0.004710663575679064,
              -0.011328262276947498,
              -0.00980804767459631,
              0.003322134492918849,
              -0.0023269944358617067,
              0.025150207802653313,
              -0.009768042713403702,
              -0.042939379811286926,
              0.020109498873353004,
              0.004330609925091267,
              0.004727332387119532,
              -0.00904794130474329,
              0.008327839896082878,
              -0.022469831630587578,
              -0.004547307267785072,
              -0.019882800057530403,
              0.027390524744987488,
              0.015042118728160858,
              0.00933464802801609,
              -0.006587594281882048,
              0.022136451676487923,
              0.0007267690380103886,
              -0.00228532194159925,
              0.0009918063879013062,
              -8.381388761335984e-05,
              0.007681081537157297,
              -0.017055734992027283,
              -0.01434868760406971,
              -0.003103770548477769,
              -0.03584504872560501,
              0.012601775117218494,
              0.0008517866372130811,
              -0.008134478703141212,
              -0.009741371497511864,
              0.0019202703842893243,
              0.024656806141138077,
              0.008567873388528824,
              0.004580644890666008,
              0.011261586099863052,
              0.015562191605567932,
              -0.009461332112550735,
              0.012568436563014984,
              -0.00733436644077301,
              0.002718716161325574,
              -0.00025170212029479444,
              0.002108630258589983,
              0.01781584322452545,
              -0.010228106752038002,
              0.02216312102973461,
              0.02409672737121582,
              0.0046206507831811905,
              -0.0006038350402377546,
              -0.020562896504998207,
              0.0012026693439111114,
              -0.011868338100612164,
              -0.012488425709307194,
              0.006294219754636288,
              -0.014415363781154156,
              -0.04717997834086418,
              -0.016722355037927628,
              0.02098962292075157,
              0.015135465189814568,
              0.0012943489709869027,
              -0.01685570739209652,
              0.008741230703890324,
              -0.018389256671071053,
              -0.012808470986783504,
              -0.0009709700825624168,
              -0.001995281083509326,
              -0.010181433521211147,
              -0.004353946540504694,
              -0.05952838435769081,
              -0.0096013518050313,
              -0.04533971846103668,
              0.006727613974362612,
              0.03352472186088562,
              0.0033721416257321835,
              -0.00436394801363349,
              -0.007681081537157297,
              -0.033258017152547836,
              -0.005140724126249552,
              0.03653847798705101,
              -0.015082123689353466,
              -0.027950603514909744,
              0.006354228127747774,
              -0.0450730137526989,
              0.04003230482339859,
              0.01612227037549019,
              -0.01740245148539543,
              -0.0044839647598564625,
              -0.014988777227699757,
              0.004083908628672361,
              -0.031604450196027756,
              -0.015095459297299385,
              0.0010584824485704303,
              0.020109498873353004,
              0.0012585106305778027,
              -0.017189087346196175,
              0.02555026486515999,
              -0.0022236465010792017,
              -0.012055031023919582,
              -0.0006071688258089125,
              -0.029390806332230568,
              0.047473352402448654,
              0.002633704338222742,
              -0.019389396533370018,
              0.009187960997223854,
              0.017509132623672485,
              0.012181716039776802,
              -0.004353946540504694,
              0.02888406813144684,
              -0.0010501479264348745,
              -0.018562614917755127,
              0.0138552850112319,
              -0.0036605154164135456,
              0.019189368933439255,
              -0.003178780898451805,
              -0.03485824167728424,
              -0.013308540917932987,
              -0.008447856642305851,
              0.010388129390776157,
              -0.03920552134513855,
              -0.0034504858776926994,
              0.00650758296251297,
              0.011181574314832687,
              -0.01901601068675518,
              -0.012581772170960903,
              -0.021629713475704193,
              0.01392196025699377,
              0.01990947127342224,
              -0.011021552607417107,
              0.002422007732093334,
              -0.0055174436420202255,
              0.013268535025417805,
              0.012075033970177174,
              -0.014655397273600101,
              0.024270085617899895,
              -0.002778724767267704,
              -0.005184063222259283,
              -0.0003223370586056262,
              0.0032287880312651396,
              0.005104052368551493,
              0.025950321927666664,
              -0.03421815112233162,
              0.0017044066917151213,
              0.03101770207285881,
              0.007821101695299149,
              0.019136028364300728,
              0.0048806872218847275,
              -0.0024486782494932413,
              0.015962248668074608,
              -0.010881532914936543,
              0.02883072756230831,
              -0.011514955200254917,
              0.017122412100434303,
              0.001248509157449007,
              -0.009361318312585354,
              -0.011134901084005833,
              0.02469681203365326,
              0.0333380289375782,
              0.03813870623707771,
              0.03349805250763893,
              -0.017482461407780647,
              -0.006940977647900581,
              -0.012628445401787758,
              -0.013788608834147453,
              -0.004540639463812113,
              0.009768042713403702,
              0.04120580479502678,
              -0.009981405921280384,
              0.015482180751860142,
              0.005974174477159977,
              0.0027420527767390013,
              0.020309526473283768,
              0.007141005713492632,
              0.016735689714550972,
              0.005267408676445484,
              -0.005374090280383825,
              0.032937973737716675,
              0.0012951823882758617,
              0.007094332482665777,
              -0.013895289972424507,
              -0.0007371871615760028,
              0.00447729742154479,
              -0.016749026253819466,
              -0.017282433807849884,
              -0.0014060313114896417,
              -0.025123538449406624,
              0.019056016579270363,
              0.02066957764327526,
              -0.014322017319500446,
              0.03405813127756119,
              0.0005142390727996826,
              0.040112316608428955,
              -0.012008357793092728,
              -0.02021618001163006,
              0.03336469829082489,
              -0.01080818846821785,
              -0.015002112835645676,
              -0.013681926764547825,
              -0.019762782379984856,
              0.014415363781154156,
              -0.005027374718338251,
              -0.0027270507998764515,
              -0.0025253556668758392,
              -0.0008492862689308822,
              -0.023283278569579124,
              0.0034904915373772383,
              0.00037067721132189035,
              0.020816264674067497,
              0.014282011426985264,
              -0.018295910209417343,
              0.004490632563829422,
              0.011054890230298042,
              0.003813870484009385,
              0.012795135378837585,
              -0.029497487470507622,
              -0.016562333330512047,
              0.021763065829873085,
              -0.01116157229989767,
              -0.03952556475996971,
              -0.021883081644773483,
              -0.026110343635082245,
              -0.02713715471327305,
              0.040085647255182266,
              -0.011888341046869755,
              0.022136451676487923,
              -0.005554115399718285,
              0.0242567490786314,
              0.02555026486515999,
              0.009407991543412209,
              -0.0034704888239502907,
              0.027630558237433434,
              0.011928346939384937,
              -0.005354087334126234,
              -0.010068085044622421,
              -0.0007050993153825402,
              0.013168521225452423,
              0.04304606094956398,
              0.03256458789110184,
              -0.00974803976714611,
              0.010461472906172276,
              -0.01704240031540394,
              0.012875146232545376,
              -0.03320467472076416,
              -0.02536357194185257,
              -0.0011718317400664091,
              -0.017895853146910667,
              -0.011321594007313251,
              -0.007901112549006939,
              -0.0026853783056139946,
              -0.005240737926214933,
              -0.016495656222105026,
              -0.007641076110303402,
              -0.018389256671071053,
              -0.0395522378385067,
              0.019856128841638565,
              0.008367844857275486,
              -0.006880969274789095,
              0.021096304059028625,
              0.030617645010352135,
              0.027310512959957123,
              0.014402028173208237,
              0.008981265127658844,
              0.02117631584405899,
              0.02196309342980385,
              -0.005140724126249552,
              -0.019216040149331093,
              0.0032204536255449057,
              -0.016402309760451317,
              0.00904794130474329,
              0.014602056704461575,
              0.00784110464155674,
              0.020549559965729713,
              -0.00772108742967248,
              -0.009901394136250019,
              0.028057284653186798,
              0.008907921612262726,
              0.011394938454031944,
              0.01216171309351921,
              0.007207681890577078,
              -0.014148659072816372,
              0.010581490583717823,
              -0.03504493460059166,
              -0.00036525976611301303,
              0.031791143119335175,
              -0.006360895931720734,
              0.007661079056560993,
              -0.009247968904674053,
              0.01084152702242136,
              -0.027630558237433434,
              -0.006020847707986832,
              0.029630839824676514,
              -0.0015085458289831877,
              -0.018362585455179214,
              -0.0003198367194272578,
              -0.0021619710605591536,
              -0.01740245148539543,
              -0.012695121578872204,
              -0.016375640407204628,
              0.004904023837298155,
              -0.002453678986057639,
              0.002502019051462412,
              -0.0025136873591691256,
              -0.03085767850279808,
              -0.027470534667372704,
              -0.028777386993169785,
              0.019642766565084457,
              -0.001286847866140306,
              -0.024176737293601036,
              -0.049313612282276154,
              -0.014695403166115284,
              0.022069774568080902,
              0.02336329035460949,
              0.017029065638780594,
              -0.017415786162018776,
              -0.02663041651248932,
              -0.00934798363596201,
              0.007987791672348976,
              -0.021736394613981247,
              0.004193923901766539,
              -0.02400338090956211,
              -0.013008498586714268,
              -0.011534958146512508,
              -0.028937408700585365,
              -0.004750669002532959,
              -0.03299131244421005,
              0.013828613795340061,
              -0.009294642135500908,
              0.01660233736038208,
              0.018709301948547363,
              -0.019416067749261856,
              0.017189087346196175,
              0.01101488433778286,
              0.008341174572706223,
              0.011261586099863052,
              0.02142968401312828,
              -0.030937690287828445,
              0.018055876716971397,
              -0.010294782929122448,
              -0.023549983277916908,
              -0.017122412100434303,
              0.006437573116272688,
              0.009481335058808327,
              0.00430060550570488,
              -0.02920411340892315,
              0.011948348954319954,
              -0.014735408127307892,
              0.02057623118162155,
              -0.02467014081776142,
              -7.1364214818459e-05,
              0.005497440695762634,
              0.03685852512717247,
              0.016482321545481682,
              0.016895713284611702,
              0.022803211584687233,
              0.0029120768886059523,
              -0.00465065473690629,
              -0.008261163718998432,
              -0.02492351084947586,
              0.018562614917755127,
              0.0194294024258852,
              -0.010268112644553185,
              0.009821383282542229,
              0.032191202044487,
              -0.005837488919496536,
              -0.035951729863882065,
              0.02536357194185257,
              -0.04083241894841194,
              0.018682630732655525,
              -0.0010476475581526756,
              -0.017642484977841377,
              -0.03285796195268631,
              -0.02396337501704693,
              0.0066842748783528805,
              0.0009509672527201474,
              -0.04283269867300987,
              0.008201154880225658,
              0.0008651218377053738,
              0.0027320515364408493,
              0.0012268394930288196,
              -0.017975864931941032,
              0.023629995062947273,
              -0.02835065871477127,
              -0.018842654302716255,
              0.004237263463437557,
              0.014508710242807865,
              0.015122129581868649,
              -0.01768249087035656,
              0.013508569449186325,
              -0.033258017152547836,
              -0.013895289972424507,
              0.009001268073916435,
              -0.018109217286109924,
              0.004373949486762285,
              0.018055876716971397,
              0.007707752287387848,
              0.01534882839769125,
              0.03581837937235832,
              0.0036538478452712297,
              0.01958942599594593,
              0.021763065829873085,
              -0.004887355025857687,
              0.009214631281793118,
              -0.023856693878769875,
              0.009614687412977219,
              -0.0017619148129597306,
              0.011034887284040451,
              0.00962135475128889,
              -0.01570887863636017,
              -0.01452204491943121,
              -0.0052207354456186295,
              0.019802788272500038,
              0.010608160868287086,
              -0.007567732594907284,
              -0.018042540177702904,
              -0.02867070399224758,
              -0.023403296247124672,
              -0.024776821956038475,
              0.0019369394285604358,
              -0.003133774735033512,
              -0.005497440695762634,
              -0.05158059671521187,
              -0.01812255196273327,
              0.0008013628539629281,
              0.0031587781850248575,
              0.008267831057310104,
              0.0008297002059407532,
              0.020389538258314133,
              0.0036538478452712297,
              -0.007654411252588034,
              0.009061275981366634,
              0.0030070901848375797,
              -0.014468704350292683,
              -0.007314363494515419,
              -0.02752387709915638,
              -0.011081560514867306,
              0.03640512749552727,
              0.021149644628167152,
              -0.00500737177208066,
              0.00585749140009284,
              -0.010301451198756695,
              -0.001103488728404045,
              0.013228530064225197,
              0.0029070761520415545,
              -0.013155185617506504,
              -0.025243554264307022,
              -0.014708737842738628,
              -0.021389678120613098,
              -0.030804337933659554,
              -0.012588439509272575,
              0.02904408983886242,
              0.013721932657063007,
              -0.008114475756883621,
              -0.029124101623892784,
              0.032964643090963364,
              -0.0002389919973211363,
              -0.0033037986140698195,
              0.004023900255560875,
              0.024883504956960678,
              0.032937973737716675,
              -0.001675235922448337,
              0.011381602846086025,
              0.016242288053035736,
              0.0022786541376262903,
              -0.006347560789436102,
              0.0021619710605591536,
              0.004997370298951864,
              -0.03035094030201435,
              -0.000267537689069286,
              -0.009788044728338718,
              -0.0005438266089186072,
              -0.010421467944979668,
              0.016909047961235046,
              0.01042813528329134,
              -0.03117772378027439,
              -0.02053622528910637,
              0.014322017319500446,
              -0.007281025405973196,
              0.023923369124531746,
              -0.002218645764514804,
              -0.019322721287608147,
              -0.001751913339830935,
              0.013868619687855244,
              -0.01653566211462021,
              0.02869737520813942,
              0.002815396524965763,
              0.00013929045235272497,
              0.019096022471785545,
              0.02708381414413452,
              -0.010001408867537975,
              0.022816546261310577,
              -0.01444203406572342,
              0.008687890134751797,
              0.00265037314966321,
              0.006967647932469845,
              -0.020362867042422295,
              -0.011654974892735481,
              0.006797623820602894,
              0.020749589428305626,
              -0.012535098940134048,
              0.017122412100434303,
              0.007974456064403057,
              -0.011614969000220299,
              0.00662760017439723,
              0.009134619496762753,
              -0.0014418697683140635,
              0.03083100914955139,
              -0.02787059172987938,
              -0.005124054849147797,
              -0.0007338533760048449,
              0.004497299902141094,
              0.017375780269503593,
              -0.013788608834147453,
              0.001751913339830935,
              0.0007451049168594182,
              -0.017029065638780594,
              0.01570887863636017,
              0.025563599541783333,
              0.024150067940354347,
              -0.023763347417116165,
              0.0072743576020002365,
              0.005717471707612276,
              -0.0014535380760207772,
              -0.002433676039800048,
              0.006204206962138414,
              0.01993614062666893,
              -0.03555167466402054,
              -0.013788608834147453,
              0.034111469984054565,
              0.014282011426985264,
              0.020149504765868187,
              -0.00895459484308958,
              -0.011481616646051407,
              -0.014455368742346764,
              0.02984420396387577,
              -0.04373949393630028,
              0.007947785779833794,
              0.012395079247653484,
              -0.013681926764547825,
              -0.013208527117967606,
              -0.018269238993525505,
              -0.015575527213513851,
              -0.01293515507131815,
              0.0007967788842506707,
              -0.0015035450924187899,
              -0.022229798138141632,
              -0.0019869464449584484,
              -0.005197398830205202,
              -0.005670798476785421,
              0.007994459010660648,
              -0.022136451676487923,
              -0.020896276459097862,
              0.018936000764369965,
              0.0206962488591671,
              -0.019376061856746674,
              -0.01366859208792448,
              -0.012435084208846092,
              0.0029220781289041042,
              -0.00328212883323431,
              0.01271512359380722,
              -0.027977272868156433,
              -0.01452204491943121,
              0.0016694017685949802,
              0.01758914440870285,
              -0.011928346939384937,
              -0.04101911187171936,
              -0.0011543292785063386,
              -0.006974315736442804,
              0.009461332112550735,
              -0.0018819316755980253,
              -0.015028783120214939,
              0.022083109244704247,
              -0.01878931373357773,
              -0.008787903934717178,
              0.02591031603515148,
              -0.00884124543517828,
              -0.010481475852429867,
              0.003190449206158519,
              -0.00792111549526453,
              0.013341878540813923,
              0.02479015849530697,
              -0.0097147012129426,
              -0.034964922815561295,
              -0.011754988692700863,
              -0.003883880330249667,
              -0.017882518470287323,
              -0.008487861603498459,
              -0.015548856928944588,
              0.051633939146995544,
              0.031444426625967026,
              -0.010228106752038002,
              -0.010674837045371532,
              0.0024153401609510183,
              -0.025830304250121117,
              0.003423815593123436,
              -0.005870827008038759,
              0.0070809973403811455,
              0.0010659834370017052,
              -0.021229656413197517,
              -0.0023453303147107363,
              0.017735831439495087,
              -0.0004996536881662905,
              -0.004964032210409641,
              -0.008261163718998432,
              -0.0018719303188845515,
              -0.021002957597374916,
              -0.01841592602431774,
              0.00025232721236534417,
              -0.012681785970926285,
              -0.03933887183666229,
              0.014708737842738628,
              0.028617363423109055,
              -0.006250880192965269,
              0.0172424279153347,
              0.04387284442782402,
              -0.00871456041932106,
              0.001788585213944316,
              -0.00974803976714611,
              0.016655679792165756,
              0.02186974696815014,
              0.017389114946126938,
              0.011168239638209343,
              -0.026270367205142975,
              0.01154162548482418,
              0.04171254113316536,
              -0.0037538621108978987,
              -0.02050955593585968,
              0.002850401448085904,
              0.0047906748950481415,
              0.00784110464155674,
              -0.012101704254746437,
              -0.021856412291526794,
              -0.0020186176989227533,
              0.0019469409016892314,
              -0.019856128841638565,
              0.005077381618320942,
              0.01781584322452545,
              -0.02888406813144684,
              0.0073076956905424595,
              0.0028604029212146997,
              0.00657092547044158,
              -0.03157778084278107,
              0.017722496762871742,
              -0.010734844952821732,
              -0.0012285063276067376,
              0.01917603425681591,
              -0.0012593440478667617,
              0.0016493989387527108,
              -0.016655679792165756,
              0.005594121292233467,
              0.009928064420819283,
              -0.006174203008413315,
              -0.03301798179745674,
              0.001644398202188313,
              -0.023216603323817253,
              0.008587876334786415,
              0.007734422571957111,
              -0.044966332614421844,
              -0.006710945162922144,
              -0.0020486218854784966,
              -0.019882800057530403,
              -0.017255762591958046,
              -0.004980701487511396,
              0.000838034669868648,
              -0.005024041049182415,
              0.01698905974626541,
              -0.0060508521273732185,
              -0.016709020361304283,
              -0.01949607953429222,
              0.005604122765362263,
              0.02142968401312828,
              0.00415058434009552,
              0.02695046178996563,
              0.228085458278656,
              -0.029284123331308365,
              -0.00036505141179077327,
              0.00515739293769002,
              0.003147109877318144,
              0.019802788272500038,
              -0.0013276869431138039,
              -0.0026303704362362623,
              -0.009774710051715374,
              -0.008867915719747543,
              0.04149917885661125,
              0.006914306897670031,
              -0.01312851533293724,
              -0.0017452457686886191,
              0.018242569640278816,
              -0.02336329035460949,
              -0.014148659072816372,
              -0.03421815112233162,
              -0.012601775117218494,
              0.01242841687053442,
              0.005134056322276592,
              0.01216171309351921,
              0.002530356403440237,
              -0.0008076137164607644,
              0.028937408700585365,
              0.004067239351570606,
              0.0032337887678295374,
              -0.0024486782494932413,
              0.021149644628167152,
              0.004924026783555746,
              -0.006197539623826742,
              -0.005454101599752903,
              0.011701648123562336,
              0.02085627056658268,
              -0.010248109698295593,
              -0.0009134619613178074,
              -0.029977554455399513,
              0.003218786558136344,
              0.018749307841062546,
              -0.017122412100434303,
              0.027657227590680122,
              -0.007541061844676733,
              0.010268112644553185,
              -0.04133915528655052,
              0.008041132241487503,
              -0.0009776377119123936,
              -0.004910691641271114,
              -0.006454242393374443,
              -0.004447293002158403,
              0.02202976867556572,
              -0.008947926573455334,
              0.02545691840350628,
              0.02098962292075157,
              0.005194064695388079,
              0.008361177518963814,
              0.00035546673461794853,
              -0.009854720905423164,
              0.009041273035109043,
              0.027977272868156433,
              0.005440766457468271,
              -0.008961262181401253,
              -0.0001698155829217285,
              -0.0010168098378926516,
              0.02301657572388649,
              -0.01312851533293724,
              0.022109780460596085,
              -0.02612367831170559,
              0.0231099221855402,
              0.0356316864490509,
              0.021509695798158646,
              0.007054326590150595,
              -0.023790016770362854,
              -0.007741090375930071,
              0.014895430766046047,
              -0.04389951750636101,
              -0.03424482420086861,
              0.051313892006874084,
              0.012488425709307194,
              0.039472226053476334,
              0.03003089688718319,
              0.009701366536319256,
              0.001888599363155663,
              -0.018175892531871796,
              -0.015495515428483486,
              -0.0010159764206036925,
              -0.04376616328954697,
              0.029790861532092094,
              -0.023056579753756523,
              0.001105155679397285,
              -0.024736817926168442,
              0.003963891416788101,
              -0.008494529873132706,
              -0.01133492961525917,
              -0.007301028352230787,
              0.008241160772740841,
              0.013481899164617062,
              0.017162416130304337,
              0.012588439509272575,
              -0.010454805567860603,
              -0.009167958050966263,
              -0.02545691840350628,
              0.06902305036783218,
              -0.0025470254477113485,
              0.00812781136482954,
              0.002713715424761176,
              0.01816255785524845,
              -0.013681926764547825,
              0.014055312611162663,
              0.008461191318929195,
              -0.011741654016077518,
              0.016802366822957993,
              -0.03139108791947365,
              0.008001127280294895,
              -0.0035538338124752045,
              0.011814997531473637,
              0.012115039862692356,
              0.029577499255537987,
              0.003673850791528821,
              0.018269238993525505,
              -0.0144286984577775,
              0.01733577437698841,
              -0.022403154522180557,
              0.009834717959165573,
              0.015162135474383831,
              0.005960839334875345,
              -0.004463961813598871,
              -0.018215898424386978,
              0.010241442359983921,
              -0.03221787139773369,
              -0.0032254543621093035,
              -0.014868760481476784,
              -0.02552359364926815,
              0.022109780460596085,
              -0.02835065871477127,
              -0.02536357194185257,
              -0.003680518362671137,
              -0.001108489464968443,
              -0.024963514879345894,
              -0.02416340261697769,
              0.02170972339808941,
              0.0020536226220428944,
              0.008201154880225658,
              0.016842372715473175,
              -7.469802221748978e-05,
              -0.020136170089244843,
              0.002567028161138296,
              -0.008741230703890324,
              -0.0031604450196027756,
              0.00635756179690361,
              -0.02727050706744194,
              -0.008581208065152168,
              -0.017882518470287323,
              -0.011448279023170471,
              -0.0006538420566357672,
              0.02216312102973461,
              0.010534817352890968,
              -0.006407569162547588,
              -0.008154481649398804,
              0.00683429604396224,
              0.005110719706863165,
              -0.02209644578397274,
              -0.021896416321396828,
              0.0017252429388463497,
              -0.0078077660873532295,
              -0.009794712997972965,
              -0.0016910715494304895,
              -0.16845038533210754,
              -0.0010668168542906642,
              0.025443583726882935,
              -0.046779923141002655,
              0.011995022185146809,
              0.0024936844129115343,
              0.02479015849530697,
              -0.030110906809568405,
              -0.05054045096039772,
              0.014602056704461575,
              -0.00010371252574259415,
              0.0020119501277804375,
              0.0016485655214637518,
              -0.009794712997972965,
              -0.03248457610607147,
              0.006497581955045462,
              -0.0242567490786314,
              0.01818922907114029,
              0.014588721096515656,
              -0.0027403859421610832,
              0.0377119779586792,
              -0.03800535202026367,
              0.026297036558389664,
              -0.011694980785250664,
              0.03547166287899017,
              -0.0014026975259184837,
              0.013775273226201534,
              0.019189368933439255,
              -0.01246175542473793,
              -0.04491299018263817,
              -0.0012218387564644217,
              -0.006454242393374443,
              0.03488491475582123,
              -0.022043105214834213,
              -0.009341315366327763,
              0.015148799866437912,
              -0.010774850845336914,
              -0.0057208058424293995,
              -0.00950800534337759,
              0.006914306897670031,
              0.029897544533014297,
              0.0073410337790846825,
              0.0043272762559354305,
              -0.008227825164794922,
              -0.02479015849530697,
              0.0020919612143188715,
              0.029124101623892784,
              -0.012181716039776802,
              0.004500634036958218,
              -0.011054890230298042,
              0.008987932465970516,
              -0.0006059186416678131,
              -0.002218645764514804,
              0.011148236691951752,
              0.0030404282733798027,
              0.003563835285604,
              0.028430670499801636,
              0.018469268456101418,
              -0.011301591992378235,
              -0.017189087346196175,
              -0.009121284820139408,
              -0.007814433425664902,
              0.0042939381673932076,
              0.009814715944230556,
              -0.015922242775559425,
              -0.011654974892735481,
              -0.009501338005065918,
              -0.012868478894233704,
              -0.007901112549006939,
              0.021002957597374916,
              -0.031444426625967026,
              -0.011895008385181427,
              -0.01071484200656414,
              -0.0052774096839129925,
              0.005614123772829771,
              0.017855847254395485,
              -0.017802506685256958,
              -0.022043105214834213,
              0.00307876686565578,
              -0.002470347797498107,
              -0.02237648516893387,
              0.021536367014050484,
              -0.0094279944896698,
              0.0033371367026120424,
              0.005594121292233467,
              0.016775695607066154,
              -0.0004917359328828752,
              0.02492351084947586,
              -0.019816124811768532,
              0.0009793045464903116,
              0.039125509560108185,
              -0.012721791863441467,
              0.0025470254477113485,
              -0.0034138141199946404,
              -0.010041413828730583,
              0.011214912869036198,
              0.0067742872051894665,
              0.0020119501277804375,
              0.004130581859499216,
              -0.018269238993525505,
              0.010754847899079323,
              -0.0027687232941389084,
              -0.015242146328091621,
              0.020136170089244843,
              0.03507160767912865,
              0.016175610944628716,
              -0.015562191605567932,
              0.003032093634828925,
              0.04336610808968544,
              0.00842785369604826,
              -0.01685570739209652,
              -0.004907357972115278,
              0.0009042940218932927,
              -0.0053107477724552155,
              -0.004023900255560875,
              0.03003089688718319,
              -0.0006146698724478483,
              -0.017469126731157303,
              -0.0029120768886059523,
              -0.019322721287608147,
              0.048806872218847275,
              -0.028404001146554947,
              -0.03237789496779442,
              0.029257453978061676,
              -0.011568295769393444,
              -0.0314977690577507,
              -0.11649640649557114,
              -0.025030191987752914,
              -0.011788327246904373,
              0.01285514421761036,
              -0.004233929794281721,
              0.009074611589312553,
              0.015922242775559425,
              0.024203408509492874,
              -0.03248457610607147,
              0.04189923405647278,
              -0.00016929468256421387,
              -0.01910935714840889,
              0.01704240031540394,
              -0.015175470151007175,
              -0.0073076956905424595,
              0.001351857092231512,
              0.012901817448437214,
              0.010488144122064114,
              -0.01071484200656414,
              0.028083955869078636,
              0.012068366631865501,
              -0.031071042641997337,
              0.021483026444911957,
              -0.016895713284611702,
              -0.004440625198185444,
              0.0018135887803509831,
              -0.017882518470287323,
              0.008594543673098087,
              -0.0010759849101305008,
              -0.022496500983834267,
              0.016322297975420952,
              -0.036778513342142105,
              0.0071143354289233685,
              -0.012515095993876457,
              0.01876264251768589,
              -0.01033478882163763,
              0.003907216712832451,
              -0.021483026444911957,
              -0.016802366822957993,
              -0.006744283251464367,
              -0.011421608738601208,
              0.005090716760605574,
              0.0011609968496486545,
              0.0032421231735497713,
              -0.01736244559288025,
              -0.0172424279153347,
              -0.022109780460596085,
              0.027030473574995995,
              0.0031721133273094893,
              -0.03235122188925743,
              -0.028270648792386055,
              -0.023643329739570618,
              -0.05024707689881325,
              -0.0038005353417247534,
              0.0024736816994845867,
              -0.014055312611162663,
              -0.001976944971829653,
              0.0036371788010001183,
              -0.01090153492987156,
              -0.005967507138848305,
              0.003607174614444375,
              -0.009701366536319256,
              -0.011088227853178978,
              0.029790861532092094,
              0.01476207934319973,
              -0.005707470700144768,
              -0.010001408867537975,
              -0.03240456432104111,
              -0.006384232547134161,
              -0.026110343635082245,
              -0.0015960581367835402,
              0.00963469035923481,
              0.0008342841756530106,
              0.007801098749041557,
              -0.03565835580229759,
              -0.0012410080526024103,
              -0.018709301948547363,
              -0.01910935714840889,
              -0.0038038690108805895,
              0.010368126444518566,
              -0.026577075943350792,
              -0.003317133756354451,
              0.008401183411478996,
              -0.024803493171930313,
              -0.013721932657063007,
              -0.0030054233502596617,
              -0.006550922524183989,
              -0.005297412630170584,
              -0.002998755546286702,
              -0.02708381414413452,
              0.0029920879751443863,
              0.02933746576309204,
              -0.011468281969428062,
              0.000514655839651823,
              -0.00992139708250761,
              0.021096304059028625,
              -0.0028037282172590494,
              -0.023816687986254692,
              0.0264437235891819,
              0.02308325096964836,
              -0.017949193716049194,
              -0.005944170523434877,
              -0.02301657572388649,
              0.0164423156529665,
              -0.009501338005065918,
              -0.008721228688955307,
              0.002960416954010725,
              -0.011468281969428062,
              0.04200591519474983,
              0.016709020361304283,
              -0.022563178092241287,
              0.008581208065152168,
              -0.009968070313334465,
              0.021136309951543808,
              0.0014035309432074428,
              -0.027217166498303413,
              -0.014922101981937885,
              -0.022403154522180557,
              0.014735408127307892,
              -0.022936563938856125,
              0.022149786353111267,
              0.01940273307263851,
              -0.005264074541628361,
              -0.014868760481476784,
              0.021029628813266754,
              0.0015785556752234697,
              0.007894445210695267,
              -0.002181974006816745,
              -0.00782776903361082,
              0.021056298166513443,
              -0.010708174668252468,
              -0.022176455706357956,
              0.031071042641997337,
              -0.01892266422510147,
              0.005000704433768988,
              0.014255341142416,
              -0.002030286006629467,
              0.015988918021321297,
              0.001396863372065127,
              -0.004817345179617405,
              0.022523172199726105,
              -0.006697610020637512,
              -0.005140724126249552,
              -0.02260318398475647,
              0.029070761054754257,
              -0.01850927248597145,
              0.013388551771640778,
              -0.007467718329280615,
              -0.022456495091319084,
              -0.0038738788571208715,
              0.01860261894762516,
              -0.018709301948547363,
              0.01704240031540394,
              0.0144286984577775,
              0.01222172100096941,
              -0.031444426625967026,
              -0.0016002253396436572,
              -0.004183922428637743,
              -0.016682349145412445,
              0.0001160580141004175,
              0.01542883925139904,
              0.003500493010506034,
              0.02701713889837265,
              0.004593980498611927,
              -0.009021270088851452,
              -0.010654834099113941,
              0.004697328433394432,
              0.013208527117967606,
              0.003597173374146223,
              0.013828613795340061,
              -0.003433816833421588,
              -0.012121707201004028,
              7.297902629943565e-05,
              0.011188242584466934,
              0.0011493285419419408,
              0.0016310630599036813,
              0.0344848558306694,
              0.0188559889793396,
              0.020522890612483025,
              -0.0037671972531825304,
              -0.00636422960087657,
              0.0476067028939724,
              0.014575386419892311,
              -0.019696107134222984,
              -0.016455650329589844,
              0.009374653920531273,
              0.01374860294163227,
              0.00465732254087925,
              0.006860966328531504,
              -0.02559027075767517,
              -0.011908343993127346,
              0.005557449534535408,
              0.007961121387779713,
              0.008087805472314358,
              -0.01366859208792448,
              0.03723191097378731,
              0.0037638633511960506,
              0.01720242202281952,
              0.0061842044815421104,
              0.030137578025460243,
              0.02304324507713318,
              0.006227543577551842,
              0.00256369449198246,
              -0.003430483164265752,
              -0.00278539233841002,
              0.010514814406633377,
              -0.004057237878441811,
              0.027630558237433434,
              -0.011401605792343616,
              -0.01742912083864212,
              0.019376061856746674,
              0.027230501174926758,
              0.0093146450817585,
              -0.008567873388528824,
              -0.01745579205453396,
              -0.007207681890577078,
              -0.017309105023741722,
              0.01660233736038208,
              -0.007327698636800051,
              -0.024870168417692184,
              -0.02463013492524624,
              0.005114053376019001,
              0.011975020170211792,
              -0.015562191605567932,
              0.049126919358968735,
              -0.013468563556671143,
              0.019829459488391876,
              -0.009454664774239063,
              0.025656946003437042,
              -0.020722918212413788,
              0.013681926764547825,
              0.02987087331712246,
              0.01765581965446472,
              -0.006704277358949184,
              0.010174766182899475,
              0.001715241582132876,
              -0.007134337909519672,
              -0.011434943415224552,
              0.02168305404484272,
              0.02603033185005188,
              -0.019949475303292274,
              0.07227684557437897,
              0.024483447894454002,
              0.007394374813884497,
              -0.0018469267524778843,
              -0.02349664270877838,
              0.022109780460596085,
              0.005240737926214933,
              0.015988918021321297,
              -0.023643329739570618,
              0.0017569140763953328,
              0.004317274782806635,
              0.0042572664096951485,
              0.0002967084583360702,
              -0.02361665852367878,
              0.0012376742670312524,
              -0.0051207211799919605,
              0.013175188563764095,
              0.003943888936191797,
              0.003112104954198003,
              0.00040609887219034135,
              0.03669850155711174,
              -0.009928064420819283,
              0.02244316041469574,
              0.0023703337647020817,
              -0.023643329739570618,
              0.004730666056275368,
              0.009594684466719627,
              0.03152443841099739,
              -0.0029920879751443863,
              -0.03699187561869621,
              0.016869042068719864,
              0.013301873579621315,
              -0.029470816254615784,
              -0.0031887823715806007,
              0.002603699918836355,
              -0.011301591992378235,
              -0.012708456255495548,
              -0.011701648123562336,
              0.0018019204726442695,
              0.012915152125060558,
              0.020749589428305626,
              0.03573836758732796,
              -0.021069634705781937,
              -0.023723341524600983,
              -0.0015610532136633992,
              0.002572028897702694,
              -0.01204836368560791,
              -0.02200309932231903,
              -0.006360895931720734
            ],
            "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
            "indexType": "cogsearchvs",
            "question": "What is the advantage of fine-tuning BERT over using feature-based approaches?"
          },
          "output": {
            "jsonAnswer": {
              "data_points": [
                "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
                "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
              ],
              "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
              "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.']<br>        Question: What is the advantage of fine-tuning BERT over using feature-based approaches?<br>        ",
              "sources": "\nBert.pdf",
              "nextQuestions": "<>\n<>\n<>",
              "error": ""
            },
            "existingAnswer": true
          },
          "start_time": 1697137315.800065,
          "end_time": 1697137317.81253,
          "error": null,
          "children": null,
          "node_name": "check_cache_answer"
        },
        {
          "name": "generateFollowupQuestions",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "embeddedQuestion": [
              -0.016869042068719864,
              -0.0003010840737260878,
              0.022069774568080902,
              -0.01818922907114029,
              -0.0022569845896214247,
              0.013475230894982815,
              -0.0065809269435703754,
              0.004023900255560875,
              -0.032884631305933,
              -0.019216040149331093,
              0.02651040069758892,
              0.030404282733798027,
              -0.024683475494384766,
              0.0007167676230892539,
              -0.018149223178625107,
              -0.015762219205498695,
              0.036565151065588,
              0.028804056346416473,
              0.005427430849522352,
              -0.025256890803575516,
              -0.011601634323596954,
              -0.009081278927624226,
              -0.024976851418614388,
              -0.011588298715651035,
              -0.02108296938240528,
              -0.0024053386878222227,
              0.045446399599313736,
              -0.020642906427383423,
              -0.01844259724020958,
              0.0037738648243248463,
              0.026403717696666718,
              0.02196309342980385,
              -0.016562333330512047,
              -0.02032286301255226,
              -0.011454946361482143,
              0.009701366536319256,
              -0.0029054090846329927,
              -0.01225505955517292,
              0.024843499064445496,
              -0.01254176627844572,
              0.01114156935364008,
              0.032751280814409256,
              0.011975020170211792,
              -0.015215476043522358,
              -0.0032737944275140762,
              0.0264437235891819,
              0.022069774568080902,
              -0.00033192173577845097,
              -0.03133774548768997,
              0.020056158304214478,
              0.002888740273192525,
              0.030564304441213608,
              -0.016975723206996918,
              0.003323801327496767,
              -0.05198065564036369,
              -0.007454383186995983,
              -0.006334225181490183,
              0.005664131138473749,
              -0.0017552472418174148,
              -0.025723623111844063,
              0.002136967610567808,
              0.006624266039580107,
              -0.00904794130474329,
              0.01246175542473793,
              -0.012735126540064812,
              -0.013095177710056305,
              -0.005047377664595842,
              0.022816546261310577,
              0.010688171721994877,
              -0.0024503450840711594,
              0.012088368646800518,
              0.02571028843522072,
              -0.01602892391383648,
              -0.006470911204814911,
              0.027577217668294907,
              -0.014828754588961601,
              -0.0028720712289214134,
              0.021576372906565666,
              0.01041479967534542,
              -0.0031987838447093964,
              -0.0014477039221674204,
              0.006234211381524801,
              -0.01090153492987156,
              0.014268675819039345,
              0.02060290239751339,
              0.014735408127307892,
              0.0026270365342497826,
              0.0010318119311705232,
              0.003170446492731571,
              0.015802225098013878,
              0.005537446588277817,
              0.018749307841062546,
              0.03416481241583824,
              0.02221646159887314,
              0.011074893176555634,
              -0.0029904211405664682,
              -0.005217401310801506,
              0.014562050811946392,
              -0.00028566524269990623,
              0.004353946540504694,
              -0.009027938358485699,
              0.009401324205100536,
              -0.029817532747983932,
              -0.011661642231047153,
              -0.004167253617197275,
              0.009067943319678307,
              0.018629290163517,
              -0.005494107026606798,
              -0.0012393412180244923,
              -0.0036205099895596504,
              -0.03037761151790619,
              0.014482039958238602,
              -0.014588721096515656,
              -0.014175329357385635,
              -0.0054140957072377205,
              0.0022019767202436924,
              0.0005113219958730042,
              -0.010294782929122448,
              -0.01401530671864748,
              0.004560642410069704,
              0.023629995062947273,
              -1.4324934454634786e-05,
              0.018815983086824417,
              -0.017189087346196175,
              0.01698905974626541,
              -0.019322721287608147,
              -0.015028783120214939,
              -0.017975864931941032,
              -0.007247687317430973,
              -0.009167958050966263,
              0.02002948708832264,
              0.021403014659881592,
              0.00963469035923481,
              -0.02092294581234455,
              -0.007207681890577078,
              0.005680799949914217,
              0.01781584322452545,
              -0.016815701499581337,
              -0.014482039958238602,
              -0.019189368933439255,
              0.023830022662878036,
              0.016175610944628716,
              -0.02591031603515148,
              -0.03267126902937889,
              -0.05371423065662384,
              0.02012283354997635,
              -0.000785110576543957,
              0.030564304441213608,
              0.008974596858024597,
              -0.022456495091319084,
              0.012995163910090923,
              -0.029284123331308365,
              -0.012835141271352768,
              0.03224454075098038,
              0.002288655610755086,
              0.0055507817305624485,
              0.0018986007198691368,
              0.006704277358949184,
              -0.014161994680762291,
              -0.0231099221855402,
              0.01802920550107956,
              0.017975864931941032,
              0.023736676201224327,
              -0.006900971755385399,
              0.017149081453680992,
              0.021949758753180504,
              0.0017919189995154738,
              -0.005604122765362263,
              0.017375780269503593,
              -0.001429367926903069,
              0.005760811269283295,
              0.030110906809568405,
              -0.014455368742346764,
              0.007487721275538206,
              -0.0011893342016264796,
              0.01284180860966444,
              0.03205784782767296,
              -0.020056158304214478,
              -0.004427290055900812,
              -0.03920552134513855,
              0.023029910400509834,
              0.010188101790845394,
              0.03371141478419304,
              -0.013248532079160213,
              -0.018642624840140343,
              0.008454523980617523,
              0.013828613795340061,
              -0.017135746777057648,
              0.007401042152196169,
              -0.036805182695388794,
              0.007361036725342274,
              0.004623984452337027,
              -0.0066509367898106575,
              -0.022496500983834267,
              -0.6383832097053528,
              -0.026083674281835556,
              0.011181574314832687,
              -0.022229798138141632,
              0.015642203390598297,
              0.0144286984577775,
              0.010294782929122448,
              0.008834577165544033,
              -0.014375357888638973,
              0.016895713284611702,
              -0.0036705168895423412,
              -0.0242567490786314,
              -0.012355073355138302,
              -0.017922524362802505,
              -0.006840963382273912,
              -0.024243414402008057,
              0.01041479967534542,
              -0.013495233841240406,
              0.005487439688295126,
              0.0028720712289214134,
              -0.015815559774637222,
              0.03184448555111885,
              -0.023856693878769875,
              -0.017122412100434303,
              -0.011888341046869755,
              -0.010554819367825985,
              0.008614546619355679,
              -0.008961262181401253,
              -0.0003694270271807909,
              -0.004160585813224316,
              -0.011228247545659542,
              0.02679044008255005,
              0.026337042450904846,
              -0.008227825164794922,
              0.03952556475996971,
              0.010241442359983921,
              -0.007987791672348976,
              0.011988354846835136,
              -9.897226846078411e-05,
              0.0432327538728714,
              -0.004000563640147448,
              -0.03136441856622696,
              0.01694905385375023,
              -0.006024181842803955,
              0.006390899885445833,
              0.009814715944230556,
              -0.0008313670987263322,
              0.020896276459097862,
              0.00922129862010479,
              -0.009908062405884266,
              -0.0033804760314524174,
              0.011508287861943245,
              -0.0031371084041893482,
              -6.230043800314888e-05,
              0.016562333330512047,
              -0.014135324396193027,
              0.02120298519730568,
              -0.01952274888753891,
              -0.0004071406729053706,
              -0.006010846700519323,
              0.010381462052464485,
              0.003970559220761061,
              -0.004810677375644445,
              -0.03733859211206436,
              -0.005260740872472525,
              0.007574399933218956,
              0.0069809830747544765,
              0.004774005617946386,
              0.007561064790934324,
              -0.04285936802625656,
              0.007201014086604118,
              0.016322297975420952,
              0.0020636238623410463,
              -0.011081560514867306,
              -0.007681081537157297,
              0.026750434190034866,
              0.01434868760406971,
              -0.007314363494515419,
              -0.006454242393374443,
              0.013001831248402596,
              0.010374794714152813,
              -0.01533549278974533,
              0.010788186453282833,
              0.027577217668294907,
              -0.023349955677986145,
              0.009194628335535526,
              -0.004710663575679064,
              -0.011328262276947498,
              -0.00980804767459631,
              0.003322134492918849,
              -0.0023269944358617067,
              0.025150207802653313,
              -0.009768042713403702,
              -0.042939379811286926,
              0.020109498873353004,
              0.004330609925091267,
              0.004727332387119532,
              -0.00904794130474329,
              0.008327839896082878,
              -0.022469831630587578,
              -0.004547307267785072,
              -0.019882800057530403,
              0.027390524744987488,
              0.015042118728160858,
              0.00933464802801609,
              -0.006587594281882048,
              0.022136451676487923,
              0.0007267690380103886,
              -0.00228532194159925,
              0.0009918063879013062,
              -8.381388761335984e-05,
              0.007681081537157297,
              -0.017055734992027283,
              -0.01434868760406971,
              -0.003103770548477769,
              -0.03584504872560501,
              0.012601775117218494,
              0.0008517866372130811,
              -0.008134478703141212,
              -0.009741371497511864,
              0.0019202703842893243,
              0.024656806141138077,
              0.008567873388528824,
              0.004580644890666008,
              0.011261586099863052,
              0.015562191605567932,
              -0.009461332112550735,
              0.012568436563014984,
              -0.00733436644077301,
              0.002718716161325574,
              -0.00025170212029479444,
              0.002108630258589983,
              0.01781584322452545,
              -0.010228106752038002,
              0.02216312102973461,
              0.02409672737121582,
              0.0046206507831811905,
              -0.0006038350402377546,
              -0.020562896504998207,
              0.0012026693439111114,
              -0.011868338100612164,
              -0.012488425709307194,
              0.006294219754636288,
              -0.014415363781154156,
              -0.04717997834086418,
              -0.016722355037927628,
              0.02098962292075157,
              0.015135465189814568,
              0.0012943489709869027,
              -0.01685570739209652,
              0.008741230703890324,
              -0.018389256671071053,
              -0.012808470986783504,
              -0.0009709700825624168,
              -0.001995281083509326,
              -0.010181433521211147,
              -0.004353946540504694,
              -0.05952838435769081,
              -0.0096013518050313,
              -0.04533971846103668,
              0.006727613974362612,
              0.03352472186088562,
              0.0033721416257321835,
              -0.00436394801363349,
              -0.007681081537157297,
              -0.033258017152547836,
              -0.005140724126249552,
              0.03653847798705101,
              -0.015082123689353466,
              -0.027950603514909744,
              0.006354228127747774,
              -0.0450730137526989,
              0.04003230482339859,
              0.01612227037549019,
              -0.01740245148539543,
              -0.0044839647598564625,
              -0.014988777227699757,
              0.004083908628672361,
              -0.031604450196027756,
              -0.015095459297299385,
              0.0010584824485704303,
              0.020109498873353004,
              0.0012585106305778027,
              -0.017189087346196175,
              0.02555026486515999,
              -0.0022236465010792017,
              -0.012055031023919582,
              -0.0006071688258089125,
              -0.029390806332230568,
              0.047473352402448654,
              0.002633704338222742,
              -0.019389396533370018,
              0.009187960997223854,
              0.017509132623672485,
              0.012181716039776802,
              -0.004353946540504694,
              0.02888406813144684,
              -0.0010501479264348745,
              -0.018562614917755127,
              0.0138552850112319,
              -0.0036605154164135456,
              0.019189368933439255,
              -0.003178780898451805,
              -0.03485824167728424,
              -0.013308540917932987,
              -0.008447856642305851,
              0.010388129390776157,
              -0.03920552134513855,
              -0.0034504858776926994,
              0.00650758296251297,
              0.011181574314832687,
              -0.01901601068675518,
              -0.012581772170960903,
              -0.021629713475704193,
              0.01392196025699377,
              0.01990947127342224,
              -0.011021552607417107,
              0.002422007732093334,
              -0.0055174436420202255,
              0.013268535025417805,
              0.012075033970177174,
              -0.014655397273600101,
              0.024270085617899895,
              -0.002778724767267704,
              -0.005184063222259283,
              -0.0003223370586056262,
              0.0032287880312651396,
              0.005104052368551493,
              0.025950321927666664,
              -0.03421815112233162,
              0.0017044066917151213,
              0.03101770207285881,
              0.007821101695299149,
              0.019136028364300728,
              0.0048806872218847275,
              -0.0024486782494932413,
              0.015962248668074608,
              -0.010881532914936543,
              0.02883072756230831,
              -0.011514955200254917,
              0.017122412100434303,
              0.001248509157449007,
              -0.009361318312585354,
              -0.011134901084005833,
              0.02469681203365326,
              0.0333380289375782,
              0.03813870623707771,
              0.03349805250763893,
              -0.017482461407780647,
              -0.006940977647900581,
              -0.012628445401787758,
              -0.013788608834147453,
              -0.004540639463812113,
              0.009768042713403702,
              0.04120580479502678,
              -0.009981405921280384,
              0.015482180751860142,
              0.005974174477159977,
              0.0027420527767390013,
              0.020309526473283768,
              0.007141005713492632,
              0.016735689714550972,
              0.005267408676445484,
              -0.005374090280383825,
              0.032937973737716675,
              0.0012951823882758617,
              0.007094332482665777,
              -0.013895289972424507,
              -0.0007371871615760028,
              0.00447729742154479,
              -0.016749026253819466,
              -0.017282433807849884,
              -0.0014060313114896417,
              -0.025123538449406624,
              0.019056016579270363,
              0.02066957764327526,
              -0.014322017319500446,
              0.03405813127756119,
              0.0005142390727996826,
              0.040112316608428955,
              -0.012008357793092728,
              -0.02021618001163006,
              0.03336469829082489,
              -0.01080818846821785,
              -0.015002112835645676,
              -0.013681926764547825,
              -0.019762782379984856,
              0.014415363781154156,
              -0.005027374718338251,
              -0.0027270507998764515,
              -0.0025253556668758392,
              -0.0008492862689308822,
              -0.023283278569579124,
              0.0034904915373772383,
              0.00037067721132189035,
              0.020816264674067497,
              0.014282011426985264,
              -0.018295910209417343,
              0.004490632563829422,
              0.011054890230298042,
              0.003813870484009385,
              0.012795135378837585,
              -0.029497487470507622,
              -0.016562333330512047,
              0.021763065829873085,
              -0.01116157229989767,
              -0.03952556475996971,
              -0.021883081644773483,
              -0.026110343635082245,
              -0.02713715471327305,
              0.040085647255182266,
              -0.011888341046869755,
              0.022136451676487923,
              -0.005554115399718285,
              0.0242567490786314,
              0.02555026486515999,
              0.009407991543412209,
              -0.0034704888239502907,
              0.027630558237433434,
              0.011928346939384937,
              -0.005354087334126234,
              -0.010068085044622421,
              -0.0007050993153825402,
              0.013168521225452423,
              0.04304606094956398,
              0.03256458789110184,
              -0.00974803976714611,
              0.010461472906172276,
              -0.01704240031540394,
              0.012875146232545376,
              -0.03320467472076416,
              -0.02536357194185257,
              -0.0011718317400664091,
              -0.017895853146910667,
              -0.011321594007313251,
              -0.007901112549006939,
              -0.0026853783056139946,
              -0.005240737926214933,
              -0.016495656222105026,
              -0.007641076110303402,
              -0.018389256671071053,
              -0.0395522378385067,
              0.019856128841638565,
              0.008367844857275486,
              -0.006880969274789095,
              0.021096304059028625,
              0.030617645010352135,
              0.027310512959957123,
              0.014402028173208237,
              0.008981265127658844,
              0.02117631584405899,
              0.02196309342980385,
              -0.005140724126249552,
              -0.019216040149331093,
              0.0032204536255449057,
              -0.016402309760451317,
              0.00904794130474329,
              0.014602056704461575,
              0.00784110464155674,
              0.020549559965729713,
              -0.00772108742967248,
              -0.009901394136250019,
              0.028057284653186798,
              0.008907921612262726,
              0.011394938454031944,
              0.01216171309351921,
              0.007207681890577078,
              -0.014148659072816372,
              0.010581490583717823,
              -0.03504493460059166,
              -0.00036525976611301303,
              0.031791143119335175,
              -0.006360895931720734,
              0.007661079056560993,
              -0.009247968904674053,
              0.01084152702242136,
              -0.027630558237433434,
              -0.006020847707986832,
              0.029630839824676514,
              -0.0015085458289831877,
              -0.018362585455179214,
              -0.0003198367194272578,
              -0.0021619710605591536,
              -0.01740245148539543,
              -0.012695121578872204,
              -0.016375640407204628,
              0.004904023837298155,
              -0.002453678986057639,
              0.002502019051462412,
              -0.0025136873591691256,
              -0.03085767850279808,
              -0.027470534667372704,
              -0.028777386993169785,
              0.019642766565084457,
              -0.001286847866140306,
              -0.024176737293601036,
              -0.049313612282276154,
              -0.014695403166115284,
              0.022069774568080902,
              0.02336329035460949,
              0.017029065638780594,
              -0.017415786162018776,
              -0.02663041651248932,
              -0.00934798363596201,
              0.007987791672348976,
              -0.021736394613981247,
              0.004193923901766539,
              -0.02400338090956211,
              -0.013008498586714268,
              -0.011534958146512508,
              -0.028937408700585365,
              -0.004750669002532959,
              -0.03299131244421005,
              0.013828613795340061,
              -0.009294642135500908,
              0.01660233736038208,
              0.018709301948547363,
              -0.019416067749261856,
              0.017189087346196175,
              0.01101488433778286,
              0.008341174572706223,
              0.011261586099863052,
              0.02142968401312828,
              -0.030937690287828445,
              0.018055876716971397,
              -0.010294782929122448,
              -0.023549983277916908,
              -0.017122412100434303,
              0.006437573116272688,
              0.009481335058808327,
              0.00430060550570488,
              -0.02920411340892315,
              0.011948348954319954,
              -0.014735408127307892,
              0.02057623118162155,
              -0.02467014081776142,
              -7.1364214818459e-05,
              0.005497440695762634,
              0.03685852512717247,
              0.016482321545481682,
              0.016895713284611702,
              0.022803211584687233,
              0.0029120768886059523,
              -0.00465065473690629,
              -0.008261163718998432,
              -0.02492351084947586,
              0.018562614917755127,
              0.0194294024258852,
              -0.010268112644553185,
              0.009821383282542229,
              0.032191202044487,
              -0.005837488919496536,
              -0.035951729863882065,
              0.02536357194185257,
              -0.04083241894841194,
              0.018682630732655525,
              -0.0010476475581526756,
              -0.017642484977841377,
              -0.03285796195268631,
              -0.02396337501704693,
              0.0066842748783528805,
              0.0009509672527201474,
              -0.04283269867300987,
              0.008201154880225658,
              0.0008651218377053738,
              0.0027320515364408493,
              0.0012268394930288196,
              -0.017975864931941032,
              0.023629995062947273,
              -0.02835065871477127,
              -0.018842654302716255,
              0.004237263463437557,
              0.014508710242807865,
              0.015122129581868649,
              -0.01768249087035656,
              0.013508569449186325,
              -0.033258017152547836,
              -0.013895289972424507,
              0.009001268073916435,
              -0.018109217286109924,
              0.004373949486762285,
              0.018055876716971397,
              0.007707752287387848,
              0.01534882839769125,
              0.03581837937235832,
              0.0036538478452712297,
              0.01958942599594593,
              0.021763065829873085,
              -0.004887355025857687,
              0.009214631281793118,
              -0.023856693878769875,
              0.009614687412977219,
              -0.0017619148129597306,
              0.011034887284040451,
              0.00962135475128889,
              -0.01570887863636017,
              -0.01452204491943121,
              -0.0052207354456186295,
              0.019802788272500038,
              0.010608160868287086,
              -0.007567732594907284,
              -0.018042540177702904,
              -0.02867070399224758,
              -0.023403296247124672,
              -0.024776821956038475,
              0.0019369394285604358,
              -0.003133774735033512,
              -0.005497440695762634,
              -0.05158059671521187,
              -0.01812255196273327,
              0.0008013628539629281,
              0.0031587781850248575,
              0.008267831057310104,
              0.0008297002059407532,
              0.020389538258314133,
              0.0036538478452712297,
              -0.007654411252588034,
              0.009061275981366634,
              0.0030070901848375797,
              -0.014468704350292683,
              -0.007314363494515419,
              -0.02752387709915638,
              -0.011081560514867306,
              0.03640512749552727,
              0.021149644628167152,
              -0.00500737177208066,
              0.00585749140009284,
              -0.010301451198756695,
              -0.001103488728404045,
              0.013228530064225197,
              0.0029070761520415545,
              -0.013155185617506504,
              -0.025243554264307022,
              -0.014708737842738628,
              -0.021389678120613098,
              -0.030804337933659554,
              -0.012588439509272575,
              0.02904408983886242,
              0.013721932657063007,
              -0.008114475756883621,
              -0.029124101623892784,
              0.032964643090963364,
              -0.0002389919973211363,
              -0.0033037986140698195,
              0.004023900255560875,
              0.024883504956960678,
              0.032937973737716675,
              -0.001675235922448337,
              0.011381602846086025,
              0.016242288053035736,
              0.0022786541376262903,
              -0.006347560789436102,
              0.0021619710605591536,
              0.004997370298951864,
              -0.03035094030201435,
              -0.000267537689069286,
              -0.009788044728338718,
              -0.0005438266089186072,
              -0.010421467944979668,
              0.016909047961235046,
              0.01042813528329134,
              -0.03117772378027439,
              -0.02053622528910637,
              0.014322017319500446,
              -0.007281025405973196,
              0.023923369124531746,
              -0.002218645764514804,
              -0.019322721287608147,
              -0.001751913339830935,
              0.013868619687855244,
              -0.01653566211462021,
              0.02869737520813942,
              0.002815396524965763,
              0.00013929045235272497,
              0.019096022471785545,
              0.02708381414413452,
              -0.010001408867537975,
              0.022816546261310577,
              -0.01444203406572342,
              0.008687890134751797,
              0.00265037314966321,
              0.006967647932469845,
              -0.020362867042422295,
              -0.011654974892735481,
              0.006797623820602894,
              0.020749589428305626,
              -0.012535098940134048,
              0.017122412100434303,
              0.007974456064403057,
              -0.011614969000220299,
              0.00662760017439723,
              0.009134619496762753,
              -0.0014418697683140635,
              0.03083100914955139,
              -0.02787059172987938,
              -0.005124054849147797,
              -0.0007338533760048449,
              0.004497299902141094,
              0.017375780269503593,
              -0.013788608834147453,
              0.001751913339830935,
              0.0007451049168594182,
              -0.017029065638780594,
              0.01570887863636017,
              0.025563599541783333,
              0.024150067940354347,
              -0.023763347417116165,
              0.0072743576020002365,
              0.005717471707612276,
              -0.0014535380760207772,
              -0.002433676039800048,
              0.006204206962138414,
              0.01993614062666893,
              -0.03555167466402054,
              -0.013788608834147453,
              0.034111469984054565,
              0.014282011426985264,
              0.020149504765868187,
              -0.00895459484308958,
              -0.011481616646051407,
              -0.014455368742346764,
              0.02984420396387577,
              -0.04373949393630028,
              0.007947785779833794,
              0.012395079247653484,
              -0.013681926764547825,
              -0.013208527117967606,
              -0.018269238993525505,
              -0.015575527213513851,
              -0.01293515507131815,
              0.0007967788842506707,
              -0.0015035450924187899,
              -0.022229798138141632,
              -0.0019869464449584484,
              -0.005197398830205202,
              -0.005670798476785421,
              0.007994459010660648,
              -0.022136451676487923,
              -0.020896276459097862,
              0.018936000764369965,
              0.0206962488591671,
              -0.019376061856746674,
              -0.01366859208792448,
              -0.012435084208846092,
              0.0029220781289041042,
              -0.00328212883323431,
              0.01271512359380722,
              -0.027977272868156433,
              -0.01452204491943121,
              0.0016694017685949802,
              0.01758914440870285,
              -0.011928346939384937,
              -0.04101911187171936,
              -0.0011543292785063386,
              -0.006974315736442804,
              0.009461332112550735,
              -0.0018819316755980253,
              -0.015028783120214939,
              0.022083109244704247,
              -0.01878931373357773,
              -0.008787903934717178,
              0.02591031603515148,
              -0.00884124543517828,
              -0.010481475852429867,
              0.003190449206158519,
              -0.00792111549526453,
              0.013341878540813923,
              0.02479015849530697,
              -0.0097147012129426,
              -0.034964922815561295,
              -0.011754988692700863,
              -0.003883880330249667,
              -0.017882518470287323,
              -0.008487861603498459,
              -0.015548856928944588,
              0.051633939146995544,
              0.031444426625967026,
              -0.010228106752038002,
              -0.010674837045371532,
              0.0024153401609510183,
              -0.025830304250121117,
              0.003423815593123436,
              -0.005870827008038759,
              0.0070809973403811455,
              0.0010659834370017052,
              -0.021229656413197517,
              -0.0023453303147107363,
              0.017735831439495087,
              -0.0004996536881662905,
              -0.004964032210409641,
              -0.008261163718998432,
              -0.0018719303188845515,
              -0.021002957597374916,
              -0.01841592602431774,
              0.00025232721236534417,
              -0.012681785970926285,
              -0.03933887183666229,
              0.014708737842738628,
              0.028617363423109055,
              -0.006250880192965269,
              0.0172424279153347,
              0.04387284442782402,
              -0.00871456041932106,
              0.001788585213944316,
              -0.00974803976714611,
              0.016655679792165756,
              0.02186974696815014,
              0.017389114946126938,
              0.011168239638209343,
              -0.026270367205142975,
              0.01154162548482418,
              0.04171254113316536,
              -0.0037538621108978987,
              -0.02050955593585968,
              0.002850401448085904,
              0.0047906748950481415,
              0.00784110464155674,
              -0.012101704254746437,
              -0.021856412291526794,
              -0.0020186176989227533,
              0.0019469409016892314,
              -0.019856128841638565,
              0.005077381618320942,
              0.01781584322452545,
              -0.02888406813144684,
              0.0073076956905424595,
              0.0028604029212146997,
              0.00657092547044158,
              -0.03157778084278107,
              0.017722496762871742,
              -0.010734844952821732,
              -0.0012285063276067376,
              0.01917603425681591,
              -0.0012593440478667617,
              0.0016493989387527108,
              -0.016655679792165756,
              0.005594121292233467,
              0.009928064420819283,
              -0.006174203008413315,
              -0.03301798179745674,
              0.001644398202188313,
              -0.023216603323817253,
              0.008587876334786415,
              0.007734422571957111,
              -0.044966332614421844,
              -0.006710945162922144,
              -0.0020486218854784966,
              -0.019882800057530403,
              -0.017255762591958046,
              -0.004980701487511396,
              0.000838034669868648,
              -0.005024041049182415,
              0.01698905974626541,
              -0.0060508521273732185,
              -0.016709020361304283,
              -0.01949607953429222,
              0.005604122765362263,
              0.02142968401312828,
              0.00415058434009552,
              0.02695046178996563,
              0.228085458278656,
              -0.029284123331308365,
              -0.00036505141179077327,
              0.00515739293769002,
              0.003147109877318144,
              0.019802788272500038,
              -0.0013276869431138039,
              -0.0026303704362362623,
              -0.009774710051715374,
              -0.008867915719747543,
              0.04149917885661125,
              0.006914306897670031,
              -0.01312851533293724,
              -0.0017452457686886191,
              0.018242569640278816,
              -0.02336329035460949,
              -0.014148659072816372,
              -0.03421815112233162,
              -0.012601775117218494,
              0.01242841687053442,
              0.005134056322276592,
              0.01216171309351921,
              0.002530356403440237,
              -0.0008076137164607644,
              0.028937408700585365,
              0.004067239351570606,
              0.0032337887678295374,
              -0.0024486782494932413,
              0.021149644628167152,
              0.004924026783555746,
              -0.006197539623826742,
              -0.005454101599752903,
              0.011701648123562336,
              0.02085627056658268,
              -0.010248109698295593,
              -0.0009134619613178074,
              -0.029977554455399513,
              0.003218786558136344,
              0.018749307841062546,
              -0.017122412100434303,
              0.027657227590680122,
              -0.007541061844676733,
              0.010268112644553185,
              -0.04133915528655052,
              0.008041132241487503,
              -0.0009776377119123936,
              -0.004910691641271114,
              -0.006454242393374443,
              -0.004447293002158403,
              0.02202976867556572,
              -0.008947926573455334,
              0.02545691840350628,
              0.02098962292075157,
              0.005194064695388079,
              0.008361177518963814,
              0.00035546673461794853,
              -0.009854720905423164,
              0.009041273035109043,
              0.027977272868156433,
              0.005440766457468271,
              -0.008961262181401253,
              -0.0001698155829217285,
              -0.0010168098378926516,
              0.02301657572388649,
              -0.01312851533293724,
              0.022109780460596085,
              -0.02612367831170559,
              0.0231099221855402,
              0.0356316864490509,
              0.021509695798158646,
              0.007054326590150595,
              -0.023790016770362854,
              -0.007741090375930071,
              0.014895430766046047,
              -0.04389951750636101,
              -0.03424482420086861,
              0.051313892006874084,
              0.012488425709307194,
              0.039472226053476334,
              0.03003089688718319,
              0.009701366536319256,
              0.001888599363155663,
              -0.018175892531871796,
              -0.015495515428483486,
              -0.0010159764206036925,
              -0.04376616328954697,
              0.029790861532092094,
              -0.023056579753756523,
              0.001105155679397285,
              -0.024736817926168442,
              0.003963891416788101,
              -0.008494529873132706,
              -0.01133492961525917,
              -0.007301028352230787,
              0.008241160772740841,
              0.013481899164617062,
              0.017162416130304337,
              0.012588439509272575,
              -0.010454805567860603,
              -0.009167958050966263,
              -0.02545691840350628,
              0.06902305036783218,
              -0.0025470254477113485,
              0.00812781136482954,
              0.002713715424761176,
              0.01816255785524845,
              -0.013681926764547825,
              0.014055312611162663,
              0.008461191318929195,
              -0.011741654016077518,
              0.016802366822957993,
              -0.03139108791947365,
              0.008001127280294895,
              -0.0035538338124752045,
              0.011814997531473637,
              0.012115039862692356,
              0.029577499255537987,
              0.003673850791528821,
              0.018269238993525505,
              -0.0144286984577775,
              0.01733577437698841,
              -0.022403154522180557,
              0.009834717959165573,
              0.015162135474383831,
              0.005960839334875345,
              -0.004463961813598871,
              -0.018215898424386978,
              0.010241442359983921,
              -0.03221787139773369,
              -0.0032254543621093035,
              -0.014868760481476784,
              -0.02552359364926815,
              0.022109780460596085,
              -0.02835065871477127,
              -0.02536357194185257,
              -0.003680518362671137,
              -0.001108489464968443,
              -0.024963514879345894,
              -0.02416340261697769,
              0.02170972339808941,
              0.0020536226220428944,
              0.008201154880225658,
              0.016842372715473175,
              -7.469802221748978e-05,
              -0.020136170089244843,
              0.002567028161138296,
              -0.008741230703890324,
              -0.0031604450196027756,
              0.00635756179690361,
              -0.02727050706744194,
              -0.008581208065152168,
              -0.017882518470287323,
              -0.011448279023170471,
              -0.0006538420566357672,
              0.02216312102973461,
              0.010534817352890968,
              -0.006407569162547588,
              -0.008154481649398804,
              0.00683429604396224,
              0.005110719706863165,
              -0.02209644578397274,
              -0.021896416321396828,
              0.0017252429388463497,
              -0.0078077660873532295,
              -0.009794712997972965,
              -0.0016910715494304895,
              -0.16845038533210754,
              -0.0010668168542906642,
              0.025443583726882935,
              -0.046779923141002655,
              0.011995022185146809,
              0.0024936844129115343,
              0.02479015849530697,
              -0.030110906809568405,
              -0.05054045096039772,
              0.014602056704461575,
              -0.00010371252574259415,
              0.0020119501277804375,
              0.0016485655214637518,
              -0.009794712997972965,
              -0.03248457610607147,
              0.006497581955045462,
              -0.0242567490786314,
              0.01818922907114029,
              0.014588721096515656,
              -0.0027403859421610832,
              0.0377119779586792,
              -0.03800535202026367,
              0.026297036558389664,
              -0.011694980785250664,
              0.03547166287899017,
              -0.0014026975259184837,
              0.013775273226201534,
              0.019189368933439255,
              -0.01246175542473793,
              -0.04491299018263817,
              -0.0012218387564644217,
              -0.006454242393374443,
              0.03488491475582123,
              -0.022043105214834213,
              -0.009341315366327763,
              0.015148799866437912,
              -0.010774850845336914,
              -0.0057208058424293995,
              -0.00950800534337759,
              0.006914306897670031,
              0.029897544533014297,
              0.0073410337790846825,
              0.0043272762559354305,
              -0.008227825164794922,
              -0.02479015849530697,
              0.0020919612143188715,
              0.029124101623892784,
              -0.012181716039776802,
              0.004500634036958218,
              -0.011054890230298042,
              0.008987932465970516,
              -0.0006059186416678131,
              -0.002218645764514804,
              0.011148236691951752,
              0.0030404282733798027,
              0.003563835285604,
              0.028430670499801636,
              0.018469268456101418,
              -0.011301591992378235,
              -0.017189087346196175,
              -0.009121284820139408,
              -0.007814433425664902,
              0.0042939381673932076,
              0.009814715944230556,
              -0.015922242775559425,
              -0.011654974892735481,
              -0.009501338005065918,
              -0.012868478894233704,
              -0.007901112549006939,
              0.021002957597374916,
              -0.031444426625967026,
              -0.011895008385181427,
              -0.01071484200656414,
              -0.0052774096839129925,
              0.005614123772829771,
              0.017855847254395485,
              -0.017802506685256958,
              -0.022043105214834213,
              0.00307876686565578,
              -0.002470347797498107,
              -0.02237648516893387,
              0.021536367014050484,
              -0.0094279944896698,
              0.0033371367026120424,
              0.005594121292233467,
              0.016775695607066154,
              -0.0004917359328828752,
              0.02492351084947586,
              -0.019816124811768532,
              0.0009793045464903116,
              0.039125509560108185,
              -0.012721791863441467,
              0.0025470254477113485,
              -0.0034138141199946404,
              -0.010041413828730583,
              0.011214912869036198,
              0.0067742872051894665,
              0.0020119501277804375,
              0.004130581859499216,
              -0.018269238993525505,
              0.010754847899079323,
              -0.0027687232941389084,
              -0.015242146328091621,
              0.020136170089244843,
              0.03507160767912865,
              0.016175610944628716,
              -0.015562191605567932,
              0.003032093634828925,
              0.04336610808968544,
              0.00842785369604826,
              -0.01685570739209652,
              -0.004907357972115278,
              0.0009042940218932927,
              -0.0053107477724552155,
              -0.004023900255560875,
              0.03003089688718319,
              -0.0006146698724478483,
              -0.017469126731157303,
              -0.0029120768886059523,
              -0.019322721287608147,
              0.048806872218847275,
              -0.028404001146554947,
              -0.03237789496779442,
              0.029257453978061676,
              -0.011568295769393444,
              -0.0314977690577507,
              -0.11649640649557114,
              -0.025030191987752914,
              -0.011788327246904373,
              0.01285514421761036,
              -0.004233929794281721,
              0.009074611589312553,
              0.015922242775559425,
              0.024203408509492874,
              -0.03248457610607147,
              0.04189923405647278,
              -0.00016929468256421387,
              -0.01910935714840889,
              0.01704240031540394,
              -0.015175470151007175,
              -0.0073076956905424595,
              0.001351857092231512,
              0.012901817448437214,
              0.010488144122064114,
              -0.01071484200656414,
              0.028083955869078636,
              0.012068366631865501,
              -0.031071042641997337,
              0.021483026444911957,
              -0.016895713284611702,
              -0.004440625198185444,
              0.0018135887803509831,
              -0.017882518470287323,
              0.008594543673098087,
              -0.0010759849101305008,
              -0.022496500983834267,
              0.016322297975420952,
              -0.036778513342142105,
              0.0071143354289233685,
              -0.012515095993876457,
              0.01876264251768589,
              -0.01033478882163763,
              0.003907216712832451,
              -0.021483026444911957,
              -0.016802366822957993,
              -0.006744283251464367,
              -0.011421608738601208,
              0.005090716760605574,
              0.0011609968496486545,
              0.0032421231735497713,
              -0.01736244559288025,
              -0.0172424279153347,
              -0.022109780460596085,
              0.027030473574995995,
              0.0031721133273094893,
              -0.03235122188925743,
              -0.028270648792386055,
              -0.023643329739570618,
              -0.05024707689881325,
              -0.0038005353417247534,
              0.0024736816994845867,
              -0.014055312611162663,
              -0.001976944971829653,
              0.0036371788010001183,
              -0.01090153492987156,
              -0.005967507138848305,
              0.003607174614444375,
              -0.009701366536319256,
              -0.011088227853178978,
              0.029790861532092094,
              0.01476207934319973,
              -0.005707470700144768,
              -0.010001408867537975,
              -0.03240456432104111,
              -0.006384232547134161,
              -0.026110343635082245,
              -0.0015960581367835402,
              0.00963469035923481,
              0.0008342841756530106,
              0.007801098749041557,
              -0.03565835580229759,
              -0.0012410080526024103,
              -0.018709301948547363,
              -0.01910935714840889,
              -0.0038038690108805895,
              0.010368126444518566,
              -0.026577075943350792,
              -0.003317133756354451,
              0.008401183411478996,
              -0.024803493171930313,
              -0.013721932657063007,
              -0.0030054233502596617,
              -0.006550922524183989,
              -0.005297412630170584,
              -0.002998755546286702,
              -0.02708381414413452,
              0.0029920879751443863,
              0.02933746576309204,
              -0.011468281969428062,
              0.000514655839651823,
              -0.00992139708250761,
              0.021096304059028625,
              -0.0028037282172590494,
              -0.023816687986254692,
              0.0264437235891819,
              0.02308325096964836,
              -0.017949193716049194,
              -0.005944170523434877,
              -0.02301657572388649,
              0.0164423156529665,
              -0.009501338005065918,
              -0.008721228688955307,
              0.002960416954010725,
              -0.011468281969428062,
              0.04200591519474983,
              0.016709020361304283,
              -0.022563178092241287,
              0.008581208065152168,
              -0.009968070313334465,
              0.021136309951543808,
              0.0014035309432074428,
              -0.027217166498303413,
              -0.014922101981937885,
              -0.022403154522180557,
              0.014735408127307892,
              -0.022936563938856125,
              0.022149786353111267,
              0.01940273307263851,
              -0.005264074541628361,
              -0.014868760481476784,
              0.021029628813266754,
              0.0015785556752234697,
              0.007894445210695267,
              -0.002181974006816745,
              -0.00782776903361082,
              0.021056298166513443,
              -0.010708174668252468,
              -0.022176455706357956,
              0.031071042641997337,
              -0.01892266422510147,
              0.005000704433768988,
              0.014255341142416,
              -0.002030286006629467,
              0.015988918021321297,
              0.001396863372065127,
              -0.004817345179617405,
              0.022523172199726105,
              -0.006697610020637512,
              -0.005140724126249552,
              -0.02260318398475647,
              0.029070761054754257,
              -0.01850927248597145,
              0.013388551771640778,
              -0.007467718329280615,
              -0.022456495091319084,
              -0.0038738788571208715,
              0.01860261894762516,
              -0.018709301948547363,
              0.01704240031540394,
              0.0144286984577775,
              0.01222172100096941,
              -0.031444426625967026,
              -0.0016002253396436572,
              -0.004183922428637743,
              -0.016682349145412445,
              0.0001160580141004175,
              0.01542883925139904,
              0.003500493010506034,
              0.02701713889837265,
              0.004593980498611927,
              -0.009021270088851452,
              -0.010654834099113941,
              0.004697328433394432,
              0.013208527117967606,
              0.003597173374146223,
              0.013828613795340061,
              -0.003433816833421588,
              -0.012121707201004028,
              7.297902629943565e-05,
              0.011188242584466934,
              0.0011493285419419408,
              0.0016310630599036813,
              0.0344848558306694,
              0.0188559889793396,
              0.020522890612483025,
              -0.0037671972531825304,
              -0.00636422960087657,
              0.0476067028939724,
              0.014575386419892311,
              -0.019696107134222984,
              -0.016455650329589844,
              0.009374653920531273,
              0.01374860294163227,
              0.00465732254087925,
              0.006860966328531504,
              -0.02559027075767517,
              -0.011908343993127346,
              0.005557449534535408,
              0.007961121387779713,
              0.008087805472314358,
              -0.01366859208792448,
              0.03723191097378731,
              0.0037638633511960506,
              0.01720242202281952,
              0.0061842044815421104,
              0.030137578025460243,
              0.02304324507713318,
              0.006227543577551842,
              0.00256369449198246,
              -0.003430483164265752,
              -0.00278539233841002,
              0.010514814406633377,
              -0.004057237878441811,
              0.027630558237433434,
              -0.011401605792343616,
              -0.01742912083864212,
              0.019376061856746674,
              0.027230501174926758,
              0.0093146450817585,
              -0.008567873388528824,
              -0.01745579205453396,
              -0.007207681890577078,
              -0.017309105023741722,
              0.01660233736038208,
              -0.007327698636800051,
              -0.024870168417692184,
              -0.02463013492524624,
              0.005114053376019001,
              0.011975020170211792,
              -0.015562191605567932,
              0.049126919358968735,
              -0.013468563556671143,
              0.019829459488391876,
              -0.009454664774239063,
              0.025656946003437042,
              -0.020722918212413788,
              0.013681926764547825,
              0.02987087331712246,
              0.01765581965446472,
              -0.006704277358949184,
              0.010174766182899475,
              0.001715241582132876,
              -0.007134337909519672,
              -0.011434943415224552,
              0.02168305404484272,
              0.02603033185005188,
              -0.019949475303292274,
              0.07227684557437897,
              0.024483447894454002,
              0.007394374813884497,
              -0.0018469267524778843,
              -0.02349664270877838,
              0.022109780460596085,
              0.005240737926214933,
              0.015988918021321297,
              -0.023643329739570618,
              0.0017569140763953328,
              0.004317274782806635,
              0.0042572664096951485,
              0.0002967084583360702,
              -0.02361665852367878,
              0.0012376742670312524,
              -0.0051207211799919605,
              0.013175188563764095,
              0.003943888936191797,
              0.003112104954198003,
              0.00040609887219034135,
              0.03669850155711174,
              -0.009928064420819283,
              0.02244316041469574,
              0.0023703337647020817,
              -0.023643329739570618,
              0.004730666056275368,
              0.009594684466719627,
              0.03152443841099739,
              -0.0029920879751443863,
              -0.03699187561869621,
              0.016869042068719864,
              0.013301873579621315,
              -0.029470816254615784,
              -0.0031887823715806007,
              0.002603699918836355,
              -0.011301591992378235,
              -0.012708456255495548,
              -0.011701648123562336,
              0.0018019204726442695,
              0.012915152125060558,
              0.020749589428305626,
              0.03573836758732796,
              -0.021069634705781937,
              -0.023723341524600983,
              -0.0015610532136633992,
              0.002572028897702694,
              -0.01204836368560791,
              -0.02200309932231903,
              -0.006360895931720734
            ],
            "existingAnswer": true,
            "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
            "indexType": "cogsearchvs",
            "jsonAnswer": {
              "data_points": [
                "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
                "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
              ],
              "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
              "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.']<br>        Question: What is the advantage of fine-tuning BERT over using feature-based approaches?<br>        ",
              "sources": "\nBert.pdf",
              "nextQuestions": "<>\n<>\n<>",
              "error": ""
            },
            "llm": {
              "model": "gpt-3.5-turbo",
              "request_timeout": null,
              "max_tokens": 1000,
              "stream": false,
              "n": 1,
              "temperature": 0.3,
              "engine": "chat16k",
              "_type": "azure-openai-chat"
            },
            "modifiedAnswer": "",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "promptTemplate": "",
            "question": "What is the advantage of fine-tuning BERT over using feature-based approaches?",
            "retrievedDocs": ""
          },
          "output": {
            "values": [
              {
                "recordId": 0,
                "data": {
                  "data_points": [
                    "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
                    "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                    "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
                  ],
                  "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
                  "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.']<br>        Question: What is the advantage of fine-tuning BERT over using feature-based approaches?<br>        ",
                  "sources": "\nBert.pdf",
                  "nextQuestions": "<>\n<>\n<>",
                  "error": ""
                }
              }
            ]
          },
          "start_time": 1697137317.824167,
          "end_time": 1697137317.831733,
          "error": null,
          "children": null,
          "node_name": "followup_questions"
        }
      ],
      "variant_id": "",
      "name": "",
      "description": "",
      "tags": null,
      "system_metrics": {
        "duration": 2.907021,
        "total_tokens": 16
      },
      "result": {
        "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
        "context": [
          "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
          "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
          "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
        ],
        "output": {
          "values": [
            {
              "recordId": 0,
              "data": {
                "data_points": [
                  "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
                  "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                  "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
                ],
                "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
                "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.']<br>        Question: What is the advantage of fine-tuning BERT over using feature-based approaches?<br>        ",
                "sources": "\nBert.pdf",
                "nextQuestions": "<>\n<>\n<>",
                "error": ""
              }
            }
          ]
        }
      },
      "upload_metrics": false
    }
  ],
  "node_runs": [
    {
      "node": "parse_postBody",
      "flow_run_id": "4d4879be-de55-475e-85ef-cc3de9de4117",
      "run_id": "4d4879be-de55-475e-85ef-cc3de9de4117_parse_postBody_0",
      "status": "Completed",
      "inputs": {
        "postBody": {
          "values": [
            {
              "data": {
                "approach": "rtr",
                "overrides": {
                  "chainType": "stuff",
                  "deploymentType": "gpt3516k",
                  "embeddingModelType": "azureopenai",
                  "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
                  "searchType": "hybridrerank",
                  "semantic_captions": false,
                  "semantic_ranker": true,
                  "temperature": 0,
                  "tokenLength": 1000,
                  "top": 3
                },
                "text": ""
              },
              "recordId": 0
            }
          ]
        }
      },
      "output": {
        "chainType": "stuff",
        "deploymentType": "gpt3516k",
        "embeddingModelType": "azureopenai",
        "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
        "searchType": "hybridrerank",
        "semantic_captions": false,
        "semantic_ranker": true,
        "temperature": 0,
        "tokenLength": 1000,
        "top": 3
      },
      "metrics": null,
      "error": null,
      "parent_run_id": "4d4879be-de55-475e-85ef-cc3de9de4117_0",
      "start_time": "2023-10-12T14:01:54.947264Z",
      "end_time": "2023-10-12T14:01:54.950264Z",
      "index": 0,
      "api_calls": [
        {
          "name": "parseBody",
          "type": "Tool",
          "inputs": {
            "postBody": {
              "values": [
                {
                  "data": {
                    "approach": "rtr",
                    "overrides": {
                      "chainType": "stuff",
                      "deploymentType": "gpt3516k",
                      "embeddingModelType": "azureopenai",
                      "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
                      "searchType": "hybridrerank",
                      "semantic_captions": false,
                      "semantic_ranker": true,
                      "temperature": 0,
                      "tokenLength": 1000,
                      "top": 3
                    },
                    "text": ""
                  },
                  "recordId": 0
                }
              ]
            }
          },
          "output": {
            "chainType": "stuff",
            "deploymentType": "gpt3516k",
            "embeddingModelType": "azureopenai",
            "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
            "searchType": "hybridrerank",
            "semantic_captions": false,
            "semantic_ranker": true,
            "temperature": 0,
            "tokenLength": 1000,
            "top": 3
          },
          "start_time": 1697137314.947264,
          "end_time": 1697137314.949265,
          "error": null,
          "children": null,
          "node_name": "parse_postBody"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "",
        "stderr": ""
      },
      "system_metrics": {
        "duration": 0.003
      },
      "result": {
        "chainType": "stuff",
        "deploymentType": "gpt3516k",
        "embeddingModelType": "azureopenai",
        "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
        "searchType": "hybridrerank",
        "semantic_captions": false,
        "semantic_ranker": true,
        "temperature": 0,
        "tokenLength": 1000,
        "top": 3
      }
    },
    {
      "node": "create_llm",
      "flow_run_id": "4d4879be-de55-475e-85ef-cc3de9de4117",
      "run_id": "4d4879be-de55-475e-85ef-cc3de9de4117_create_llm_0",
      "status": "Completed",
      "inputs": {
        "conn": "entaoai",
        "overrides": {
          "chainType": "stuff",
          "deploymentType": "gpt3516k",
          "embeddingModelType": "azureopenai",
          "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
          "searchType": "hybridrerank",
          "semantic_captions": false,
          "semantic_ranker": true,
          "temperature": 0,
          "tokenLength": 1000,
          "top": 3
        }
      },
      "output": "AzureChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.3, model_kwargs={}, openai_api_key='1361532858cb428c8da412a73105de78', openai_api_base='https://dataaiapim.azure-api.net', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=1000, tiktoken_model_name=None, deployment_name='chat16k', model_version='', openai_api_type='azure', openai_api_version='2023-07-01-preview')",
      "metrics": null,
      "error": null,
      "parent_run_id": "4d4879be-de55-475e-85ef-cc3de9de4117_0",
      "start_time": "2023-10-12T14:01:54.953786Z",
      "end_time": "2023-10-12T14:01:54.957793Z",
      "index": 0,
      "api_calls": [
        {
          "name": "createLlm",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            }
          },
          "output": {
            "model": "gpt-3.5-turbo",
            "request_timeout": null,
            "max_tokens": 1000,
            "stream": false,
            "n": 1,
            "temperature": 0.3,
            "engine": "chat16k",
            "_type": "azure-openai-chat"
          },
          "start_time": 1697137314.953786,
          "end_time": 1697137314.955814,
          "error": null,
          "children": null,
          "node_name": "create_llm"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "",
        "stderr": ""
      },
      "system_metrics": {
        "duration": 0.004007
      },
      "result": "AzureChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.3, model_kwargs={}, openai_api_key='1361532858cb428c8da412a73105de78', openai_api_base='https://dataaiapim.azure-api.net', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=1000, tiktoken_model_name=None, deployment_name='chat16k', model_version='', openai_api_type='azure', openai_api_version='2023-07-01-preview')"
    },
    {
      "node": "embed_the_question",
      "flow_run_id": "4d4879be-de55-475e-85ef-cc3de9de4117",
      "run_id": "4d4879be-de55-475e-85ef-cc3de9de4117_embed_the_question_0",
      "status": "Completed",
      "inputs": {
        "conn": "entaoai",
        "overrides": {
          "chainType": "stuff",
          "deploymentType": "gpt3516k",
          "embeddingModelType": "azureopenai",
          "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
          "searchType": "hybridrerank",
          "semantic_captions": false,
          "semantic_ranker": true,
          "temperature": 0,
          "tokenLength": 1000,
          "top": 3
        },
        "question": "What is the advantage of fine-tuning BERT over using feature-based approaches?"
      },
      "output": [
        -0.016869042068719864,
        -0.0003010840737260878,
        0.022069774568080902,
        -0.01818922907114029,
        -0.0022569845896214247,
        0.013475230894982815,
        -0.0065809269435703754,
        0.004023900255560875,
        -0.032884631305933,
        -0.019216040149331093,
        0.02651040069758892,
        0.030404282733798027,
        -0.024683475494384766,
        0.0007167676230892539,
        -0.018149223178625107,
        -0.015762219205498695,
        0.036565151065588,
        0.028804056346416473,
        0.005427430849522352,
        -0.025256890803575516,
        -0.011601634323596954,
        -0.009081278927624226,
        -0.024976851418614388,
        -0.011588298715651035,
        -0.02108296938240528,
        -0.0024053386878222227,
        0.045446399599313736,
        -0.020642906427383423,
        -0.01844259724020958,
        0.0037738648243248463,
        0.026403717696666718,
        0.02196309342980385,
        -0.016562333330512047,
        -0.02032286301255226,
        -0.011454946361482143,
        0.009701366536319256,
        -0.0029054090846329927,
        -0.01225505955517292,
        0.024843499064445496,
        -0.01254176627844572,
        0.01114156935364008,
        0.032751280814409256,
        0.011975020170211792,
        -0.015215476043522358,
        -0.0032737944275140762,
        0.0264437235891819,
        0.022069774568080902,
        -0.00033192173577845097,
        -0.03133774548768997,
        0.020056158304214478,
        0.002888740273192525,
        0.030564304441213608,
        -0.016975723206996918,
        0.003323801327496767,
        -0.05198065564036369,
        -0.007454383186995983,
        -0.006334225181490183,
        0.005664131138473749,
        -0.0017552472418174148,
        -0.025723623111844063,
        0.002136967610567808,
        0.006624266039580107,
        -0.00904794130474329,
        0.01246175542473793,
        -0.012735126540064812,
        -0.013095177710056305,
        -0.005047377664595842,
        0.022816546261310577,
        0.010688171721994877,
        -0.0024503450840711594,
        0.012088368646800518,
        0.02571028843522072,
        -0.01602892391383648,
        -0.006470911204814911,
        0.027577217668294907,
        -0.014828754588961601,
        -0.0028720712289214134,
        0.021576372906565666,
        0.01041479967534542,
        -0.0031987838447093964,
        -0.0014477039221674204,
        0.006234211381524801,
        -0.01090153492987156,
        0.014268675819039345,
        0.02060290239751339,
        0.014735408127307892,
        0.0026270365342497826,
        0.0010318119311705232,
        0.003170446492731571,
        0.015802225098013878,
        0.005537446588277817,
        0.018749307841062546,
        0.03416481241583824,
        0.02221646159887314,
        0.011074893176555634,
        -0.0029904211405664682,
        -0.005217401310801506,
        0.014562050811946392,
        -0.00028566524269990623,
        0.004353946540504694,
        -0.009027938358485699,
        0.009401324205100536,
        -0.029817532747983932,
        -0.011661642231047153,
        -0.004167253617197275,
        0.009067943319678307,
        0.018629290163517,
        -0.005494107026606798,
        -0.0012393412180244923,
        -0.0036205099895596504,
        -0.03037761151790619,
        0.014482039958238602,
        -0.014588721096515656,
        -0.014175329357385635,
        -0.0054140957072377205,
        0.0022019767202436924,
        0.0005113219958730042,
        -0.010294782929122448,
        -0.01401530671864748,
        0.004560642410069704,
        0.023629995062947273,
        -1.4324934454634786e-05,
        0.018815983086824417,
        -0.017189087346196175,
        0.01698905974626541,
        -0.019322721287608147,
        -0.015028783120214939,
        -0.017975864931941032,
        -0.007247687317430973,
        -0.009167958050966263,
        0.02002948708832264,
        0.021403014659881592,
        0.00963469035923481,
        -0.02092294581234455,
        -0.007207681890577078,
        0.005680799949914217,
        0.01781584322452545,
        -0.016815701499581337,
        -0.014482039958238602,
        -0.019189368933439255,
        0.023830022662878036,
        0.016175610944628716,
        -0.02591031603515148,
        -0.03267126902937889,
        -0.05371423065662384,
        0.02012283354997635,
        -0.000785110576543957,
        0.030564304441213608,
        0.008974596858024597,
        -0.022456495091319084,
        0.012995163910090923,
        -0.029284123331308365,
        -0.012835141271352768,
        0.03224454075098038,
        0.002288655610755086,
        0.0055507817305624485,
        0.0018986007198691368,
        0.006704277358949184,
        -0.014161994680762291,
        -0.0231099221855402,
        0.01802920550107956,
        0.017975864931941032,
        0.023736676201224327,
        -0.006900971755385399,
        0.017149081453680992,
        0.021949758753180504,
        0.0017919189995154738,
        -0.005604122765362263,
        0.017375780269503593,
        -0.001429367926903069,
        0.005760811269283295,
        0.030110906809568405,
        -0.014455368742346764,
        0.007487721275538206,
        -0.0011893342016264796,
        0.01284180860966444,
        0.03205784782767296,
        -0.020056158304214478,
        -0.004427290055900812,
        -0.03920552134513855,
        0.023029910400509834,
        0.010188101790845394,
        0.03371141478419304,
        -0.013248532079160213,
        -0.018642624840140343,
        0.008454523980617523,
        0.013828613795340061,
        -0.017135746777057648,
        0.007401042152196169,
        -0.036805182695388794,
        0.007361036725342274,
        0.004623984452337027,
        -0.0066509367898106575,
        -0.022496500983834267,
        -0.6383832097053528,
        -0.026083674281835556,
        0.011181574314832687,
        -0.022229798138141632,
        0.015642203390598297,
        0.0144286984577775,
        0.010294782929122448,
        0.008834577165544033,
        -0.014375357888638973,
        0.016895713284611702,
        -0.0036705168895423412,
        -0.0242567490786314,
        -0.012355073355138302,
        -0.017922524362802505,
        -0.006840963382273912,
        -0.024243414402008057,
        0.01041479967534542,
        -0.013495233841240406,
        0.005487439688295126,
        0.0028720712289214134,
        -0.015815559774637222,
        0.03184448555111885,
        -0.023856693878769875,
        -0.017122412100434303,
        -0.011888341046869755,
        -0.010554819367825985,
        0.008614546619355679,
        -0.008961262181401253,
        -0.0003694270271807909,
        -0.004160585813224316,
        -0.011228247545659542,
        0.02679044008255005,
        0.026337042450904846,
        -0.008227825164794922,
        0.03952556475996971,
        0.010241442359983921,
        -0.007987791672348976,
        0.011988354846835136,
        -9.897226846078411e-05,
        0.0432327538728714,
        -0.004000563640147448,
        -0.03136441856622696,
        0.01694905385375023,
        -0.006024181842803955,
        0.006390899885445833,
        0.009814715944230556,
        -0.0008313670987263322,
        0.020896276459097862,
        0.00922129862010479,
        -0.009908062405884266,
        -0.0033804760314524174,
        0.011508287861943245,
        -0.0031371084041893482,
        -6.230043800314888e-05,
        0.016562333330512047,
        -0.014135324396193027,
        0.02120298519730568,
        -0.01952274888753891,
        -0.0004071406729053706,
        -0.006010846700519323,
        0.010381462052464485,
        0.003970559220761061,
        -0.004810677375644445,
        -0.03733859211206436,
        -0.005260740872472525,
        0.007574399933218956,
        0.0069809830747544765,
        0.004774005617946386,
        0.007561064790934324,
        -0.04285936802625656,
        0.007201014086604118,
        0.016322297975420952,
        0.0020636238623410463,
        -0.011081560514867306,
        -0.007681081537157297,
        0.026750434190034866,
        0.01434868760406971,
        -0.007314363494515419,
        -0.006454242393374443,
        0.013001831248402596,
        0.010374794714152813,
        -0.01533549278974533,
        0.010788186453282833,
        0.027577217668294907,
        -0.023349955677986145,
        0.009194628335535526,
        -0.004710663575679064,
        -0.011328262276947498,
        -0.00980804767459631,
        0.003322134492918849,
        -0.0023269944358617067,
        0.025150207802653313,
        -0.009768042713403702,
        -0.042939379811286926,
        0.020109498873353004,
        0.004330609925091267,
        0.004727332387119532,
        -0.00904794130474329,
        0.008327839896082878,
        -0.022469831630587578,
        -0.004547307267785072,
        -0.019882800057530403,
        0.027390524744987488,
        0.015042118728160858,
        0.00933464802801609,
        -0.006587594281882048,
        0.022136451676487923,
        0.0007267690380103886,
        -0.00228532194159925,
        0.0009918063879013062,
        -8.381388761335984e-05,
        0.007681081537157297,
        -0.017055734992027283,
        -0.01434868760406971,
        -0.003103770548477769,
        -0.03584504872560501,
        0.012601775117218494,
        0.0008517866372130811,
        -0.008134478703141212,
        -0.009741371497511864,
        0.0019202703842893243,
        0.024656806141138077,
        0.008567873388528824,
        0.004580644890666008,
        0.011261586099863052,
        0.015562191605567932,
        -0.009461332112550735,
        0.012568436563014984,
        -0.00733436644077301,
        0.002718716161325574,
        -0.00025170212029479444,
        0.002108630258589983,
        0.01781584322452545,
        -0.010228106752038002,
        0.02216312102973461,
        0.02409672737121582,
        0.0046206507831811905,
        -0.0006038350402377546,
        -0.020562896504998207,
        0.0012026693439111114,
        -0.011868338100612164,
        -0.012488425709307194,
        0.006294219754636288,
        -0.014415363781154156,
        -0.04717997834086418,
        -0.016722355037927628,
        0.02098962292075157,
        0.015135465189814568,
        0.0012943489709869027,
        -0.01685570739209652,
        0.008741230703890324,
        -0.018389256671071053,
        -0.012808470986783504,
        -0.0009709700825624168,
        -0.001995281083509326,
        -0.010181433521211147,
        -0.004353946540504694,
        -0.05952838435769081,
        -0.0096013518050313,
        -0.04533971846103668,
        0.006727613974362612,
        0.03352472186088562,
        0.0033721416257321835,
        -0.00436394801363349,
        -0.007681081537157297,
        -0.033258017152547836,
        -0.005140724126249552,
        0.03653847798705101,
        -0.015082123689353466,
        -0.027950603514909744,
        0.006354228127747774,
        -0.0450730137526989,
        0.04003230482339859,
        0.01612227037549019,
        -0.01740245148539543,
        -0.0044839647598564625,
        -0.014988777227699757,
        0.004083908628672361,
        -0.031604450196027756,
        -0.015095459297299385,
        0.0010584824485704303,
        0.020109498873353004,
        0.0012585106305778027,
        -0.017189087346196175,
        0.02555026486515999,
        -0.0022236465010792017,
        -0.012055031023919582,
        -0.0006071688258089125,
        -0.029390806332230568,
        0.047473352402448654,
        0.002633704338222742,
        -0.019389396533370018,
        0.009187960997223854,
        0.017509132623672485,
        0.012181716039776802,
        -0.004353946540504694,
        0.02888406813144684,
        -0.0010501479264348745,
        -0.018562614917755127,
        0.0138552850112319,
        -0.0036605154164135456,
        0.019189368933439255,
        -0.003178780898451805,
        -0.03485824167728424,
        -0.013308540917932987,
        -0.008447856642305851,
        0.010388129390776157,
        -0.03920552134513855,
        -0.0034504858776926994,
        0.00650758296251297,
        0.011181574314832687,
        -0.01901601068675518,
        -0.012581772170960903,
        -0.021629713475704193,
        0.01392196025699377,
        0.01990947127342224,
        -0.011021552607417107,
        0.002422007732093334,
        -0.0055174436420202255,
        0.013268535025417805,
        0.012075033970177174,
        -0.014655397273600101,
        0.024270085617899895,
        -0.002778724767267704,
        -0.005184063222259283,
        -0.0003223370586056262,
        0.0032287880312651396,
        0.005104052368551493,
        0.025950321927666664,
        -0.03421815112233162,
        0.0017044066917151213,
        0.03101770207285881,
        0.007821101695299149,
        0.019136028364300728,
        0.0048806872218847275,
        -0.0024486782494932413,
        0.015962248668074608,
        -0.010881532914936543,
        0.02883072756230831,
        -0.011514955200254917,
        0.017122412100434303,
        0.001248509157449007,
        -0.009361318312585354,
        -0.011134901084005833,
        0.02469681203365326,
        0.0333380289375782,
        0.03813870623707771,
        0.03349805250763893,
        -0.017482461407780647,
        -0.006940977647900581,
        -0.012628445401787758,
        -0.013788608834147453,
        -0.004540639463812113,
        0.009768042713403702,
        0.04120580479502678,
        -0.009981405921280384,
        0.015482180751860142,
        0.005974174477159977,
        0.0027420527767390013,
        0.020309526473283768,
        0.007141005713492632,
        0.016735689714550972,
        0.005267408676445484,
        -0.005374090280383825,
        0.032937973737716675,
        0.0012951823882758617,
        0.007094332482665777,
        -0.013895289972424507,
        -0.0007371871615760028,
        0.00447729742154479,
        -0.016749026253819466,
        -0.017282433807849884,
        -0.0014060313114896417,
        -0.025123538449406624,
        0.019056016579270363,
        0.02066957764327526,
        -0.014322017319500446,
        0.03405813127756119,
        0.0005142390727996826,
        0.040112316608428955,
        -0.012008357793092728,
        -0.02021618001163006,
        0.03336469829082489,
        -0.01080818846821785,
        -0.015002112835645676,
        -0.013681926764547825,
        -0.019762782379984856,
        0.014415363781154156,
        -0.005027374718338251,
        -0.0027270507998764515,
        -0.0025253556668758392,
        -0.0008492862689308822,
        -0.023283278569579124,
        0.0034904915373772383,
        0.00037067721132189035,
        0.020816264674067497,
        0.014282011426985264,
        -0.018295910209417343,
        0.004490632563829422,
        0.011054890230298042,
        0.003813870484009385,
        0.012795135378837585,
        -0.029497487470507622,
        -0.016562333330512047,
        0.021763065829873085,
        -0.01116157229989767,
        -0.03952556475996971,
        -0.021883081644773483,
        -0.026110343635082245,
        -0.02713715471327305,
        0.040085647255182266,
        -0.011888341046869755,
        0.022136451676487923,
        -0.005554115399718285,
        0.0242567490786314,
        0.02555026486515999,
        0.009407991543412209,
        -0.0034704888239502907,
        0.027630558237433434,
        0.011928346939384937,
        -0.005354087334126234,
        -0.010068085044622421,
        -0.0007050993153825402,
        0.013168521225452423,
        0.04304606094956398,
        0.03256458789110184,
        -0.00974803976714611,
        0.010461472906172276,
        -0.01704240031540394,
        0.012875146232545376,
        -0.03320467472076416,
        -0.02536357194185257,
        -0.0011718317400664091,
        -0.017895853146910667,
        -0.011321594007313251,
        -0.007901112549006939,
        -0.0026853783056139946,
        -0.005240737926214933,
        -0.016495656222105026,
        -0.007641076110303402,
        -0.018389256671071053,
        -0.0395522378385067,
        0.019856128841638565,
        0.008367844857275486,
        -0.006880969274789095,
        0.021096304059028625,
        0.030617645010352135,
        0.027310512959957123,
        0.014402028173208237,
        0.008981265127658844,
        0.02117631584405899,
        0.02196309342980385,
        -0.005140724126249552,
        -0.019216040149331093,
        0.0032204536255449057,
        -0.016402309760451317,
        0.00904794130474329,
        0.014602056704461575,
        0.00784110464155674,
        0.020549559965729713,
        -0.00772108742967248,
        -0.009901394136250019,
        0.028057284653186798,
        0.008907921612262726,
        0.011394938454031944,
        0.01216171309351921,
        0.007207681890577078,
        -0.014148659072816372,
        0.010581490583717823,
        -0.03504493460059166,
        -0.00036525976611301303,
        0.031791143119335175,
        -0.006360895931720734,
        0.007661079056560993,
        -0.009247968904674053,
        0.01084152702242136,
        -0.027630558237433434,
        -0.006020847707986832,
        0.029630839824676514,
        -0.0015085458289831877,
        -0.018362585455179214,
        -0.0003198367194272578,
        -0.0021619710605591536,
        -0.01740245148539543,
        -0.012695121578872204,
        -0.016375640407204628,
        0.004904023837298155,
        -0.002453678986057639,
        0.002502019051462412,
        -0.0025136873591691256,
        -0.03085767850279808,
        -0.027470534667372704,
        -0.028777386993169785,
        0.019642766565084457,
        -0.001286847866140306,
        -0.024176737293601036,
        -0.049313612282276154,
        -0.014695403166115284,
        0.022069774568080902,
        0.02336329035460949,
        0.017029065638780594,
        -0.017415786162018776,
        -0.02663041651248932,
        -0.00934798363596201,
        0.007987791672348976,
        -0.021736394613981247,
        0.004193923901766539,
        -0.02400338090956211,
        -0.013008498586714268,
        -0.011534958146512508,
        -0.028937408700585365,
        -0.004750669002532959,
        -0.03299131244421005,
        0.013828613795340061,
        -0.009294642135500908,
        0.01660233736038208,
        0.018709301948547363,
        -0.019416067749261856,
        0.017189087346196175,
        0.01101488433778286,
        0.008341174572706223,
        0.011261586099863052,
        0.02142968401312828,
        -0.030937690287828445,
        0.018055876716971397,
        -0.010294782929122448,
        -0.023549983277916908,
        -0.017122412100434303,
        0.006437573116272688,
        0.009481335058808327,
        0.00430060550570488,
        -0.02920411340892315,
        0.011948348954319954,
        -0.014735408127307892,
        0.02057623118162155,
        -0.02467014081776142,
        -7.1364214818459e-05,
        0.005497440695762634,
        0.03685852512717247,
        0.016482321545481682,
        0.016895713284611702,
        0.022803211584687233,
        0.0029120768886059523,
        -0.00465065473690629,
        -0.008261163718998432,
        -0.02492351084947586,
        0.018562614917755127,
        0.0194294024258852,
        -0.010268112644553185,
        0.009821383282542229,
        0.032191202044487,
        -0.005837488919496536,
        -0.035951729863882065,
        0.02536357194185257,
        -0.04083241894841194,
        0.018682630732655525,
        -0.0010476475581526756,
        -0.017642484977841377,
        -0.03285796195268631,
        -0.02396337501704693,
        0.0066842748783528805,
        0.0009509672527201474,
        -0.04283269867300987,
        0.008201154880225658,
        0.0008651218377053738,
        0.0027320515364408493,
        0.0012268394930288196,
        -0.017975864931941032,
        0.023629995062947273,
        -0.02835065871477127,
        -0.018842654302716255,
        0.004237263463437557,
        0.014508710242807865,
        0.015122129581868649,
        -0.01768249087035656,
        0.013508569449186325,
        -0.033258017152547836,
        -0.013895289972424507,
        0.009001268073916435,
        -0.018109217286109924,
        0.004373949486762285,
        0.018055876716971397,
        0.007707752287387848,
        0.01534882839769125,
        0.03581837937235832,
        0.0036538478452712297,
        0.01958942599594593,
        0.021763065829873085,
        -0.004887355025857687,
        0.009214631281793118,
        -0.023856693878769875,
        0.009614687412977219,
        -0.0017619148129597306,
        0.011034887284040451,
        0.00962135475128889,
        -0.01570887863636017,
        -0.01452204491943121,
        -0.0052207354456186295,
        0.019802788272500038,
        0.010608160868287086,
        -0.007567732594907284,
        -0.018042540177702904,
        -0.02867070399224758,
        -0.023403296247124672,
        -0.024776821956038475,
        0.0019369394285604358,
        -0.003133774735033512,
        -0.005497440695762634,
        -0.05158059671521187,
        -0.01812255196273327,
        0.0008013628539629281,
        0.0031587781850248575,
        0.008267831057310104,
        0.0008297002059407532,
        0.020389538258314133,
        0.0036538478452712297,
        -0.007654411252588034,
        0.009061275981366634,
        0.0030070901848375797,
        -0.014468704350292683,
        -0.007314363494515419,
        -0.02752387709915638,
        -0.011081560514867306,
        0.03640512749552727,
        0.021149644628167152,
        -0.00500737177208066,
        0.00585749140009284,
        -0.010301451198756695,
        -0.001103488728404045,
        0.013228530064225197,
        0.0029070761520415545,
        -0.013155185617506504,
        -0.025243554264307022,
        -0.014708737842738628,
        -0.021389678120613098,
        -0.030804337933659554,
        -0.012588439509272575,
        0.02904408983886242,
        0.013721932657063007,
        -0.008114475756883621,
        -0.029124101623892784,
        0.032964643090963364,
        -0.0002389919973211363,
        -0.0033037986140698195,
        0.004023900255560875,
        0.024883504956960678,
        0.032937973737716675,
        -0.001675235922448337,
        0.011381602846086025,
        0.016242288053035736,
        0.0022786541376262903,
        -0.006347560789436102,
        0.0021619710605591536,
        0.004997370298951864,
        -0.03035094030201435,
        -0.000267537689069286,
        -0.009788044728338718,
        -0.0005438266089186072,
        -0.010421467944979668,
        0.016909047961235046,
        0.01042813528329134,
        -0.03117772378027439,
        -0.02053622528910637,
        0.014322017319500446,
        -0.007281025405973196,
        0.023923369124531746,
        -0.002218645764514804,
        -0.019322721287608147,
        -0.001751913339830935,
        0.013868619687855244,
        -0.01653566211462021,
        0.02869737520813942,
        0.002815396524965763,
        0.00013929045235272497,
        0.019096022471785545,
        0.02708381414413452,
        -0.010001408867537975,
        0.022816546261310577,
        -0.01444203406572342,
        0.008687890134751797,
        0.00265037314966321,
        0.006967647932469845,
        -0.020362867042422295,
        -0.011654974892735481,
        0.006797623820602894,
        0.020749589428305626,
        -0.012535098940134048,
        0.017122412100434303,
        0.007974456064403057,
        -0.011614969000220299,
        0.00662760017439723,
        0.009134619496762753,
        -0.0014418697683140635,
        0.03083100914955139,
        -0.02787059172987938,
        -0.005124054849147797,
        -0.0007338533760048449,
        0.004497299902141094,
        0.017375780269503593,
        -0.013788608834147453,
        0.001751913339830935,
        0.0007451049168594182,
        -0.017029065638780594,
        0.01570887863636017,
        0.025563599541783333,
        0.024150067940354347,
        -0.023763347417116165,
        0.0072743576020002365,
        0.005717471707612276,
        -0.0014535380760207772,
        -0.002433676039800048,
        0.006204206962138414,
        0.01993614062666893,
        -0.03555167466402054,
        -0.013788608834147453,
        0.034111469984054565,
        0.014282011426985264,
        0.020149504765868187,
        -0.00895459484308958,
        -0.011481616646051407,
        -0.014455368742346764,
        0.02984420396387577,
        -0.04373949393630028,
        0.007947785779833794,
        0.012395079247653484,
        -0.013681926764547825,
        -0.013208527117967606,
        -0.018269238993525505,
        -0.015575527213513851,
        -0.01293515507131815,
        0.0007967788842506707,
        -0.0015035450924187899,
        -0.022229798138141632,
        -0.0019869464449584484,
        -0.005197398830205202,
        -0.005670798476785421,
        0.007994459010660648,
        -0.022136451676487923,
        -0.020896276459097862,
        0.018936000764369965,
        0.0206962488591671,
        -0.019376061856746674,
        -0.01366859208792448,
        -0.012435084208846092,
        0.0029220781289041042,
        -0.00328212883323431,
        0.01271512359380722,
        -0.027977272868156433,
        -0.01452204491943121,
        0.0016694017685949802,
        0.01758914440870285,
        -0.011928346939384937,
        -0.04101911187171936,
        -0.0011543292785063386,
        -0.006974315736442804,
        0.009461332112550735,
        -0.0018819316755980253,
        -0.015028783120214939,
        0.022083109244704247,
        -0.01878931373357773,
        -0.008787903934717178,
        0.02591031603515148,
        -0.00884124543517828,
        -0.010481475852429867,
        0.003190449206158519,
        -0.00792111549526453,
        0.013341878540813923,
        0.02479015849530697,
        -0.0097147012129426,
        -0.034964922815561295,
        -0.011754988692700863,
        -0.003883880330249667,
        -0.017882518470287323,
        -0.008487861603498459,
        -0.015548856928944588,
        0.051633939146995544,
        0.031444426625967026,
        -0.010228106752038002,
        -0.010674837045371532,
        0.0024153401609510183,
        -0.025830304250121117,
        0.003423815593123436,
        -0.005870827008038759,
        0.0070809973403811455,
        0.0010659834370017052,
        -0.021229656413197517,
        -0.0023453303147107363,
        0.017735831439495087,
        -0.0004996536881662905,
        -0.004964032210409641,
        -0.008261163718998432,
        -0.0018719303188845515,
        -0.021002957597374916,
        -0.01841592602431774,
        0.00025232721236534417,
        -0.012681785970926285,
        -0.03933887183666229,
        0.014708737842738628,
        0.028617363423109055,
        -0.006250880192965269,
        0.0172424279153347,
        0.04387284442782402,
        -0.00871456041932106,
        0.001788585213944316,
        -0.00974803976714611,
        0.016655679792165756,
        0.02186974696815014,
        0.017389114946126938,
        0.011168239638209343,
        -0.026270367205142975,
        0.01154162548482418,
        0.04171254113316536,
        -0.0037538621108978987,
        -0.02050955593585968,
        0.002850401448085904,
        0.0047906748950481415,
        0.00784110464155674,
        -0.012101704254746437,
        -0.021856412291526794,
        -0.0020186176989227533,
        0.0019469409016892314,
        -0.019856128841638565,
        0.005077381618320942,
        0.01781584322452545,
        -0.02888406813144684,
        0.0073076956905424595,
        0.0028604029212146997,
        0.00657092547044158,
        -0.03157778084278107,
        0.017722496762871742,
        -0.010734844952821732,
        -0.0012285063276067376,
        0.01917603425681591,
        -0.0012593440478667617,
        0.0016493989387527108,
        -0.016655679792165756,
        0.005594121292233467,
        0.009928064420819283,
        -0.006174203008413315,
        -0.03301798179745674,
        0.001644398202188313,
        -0.023216603323817253,
        0.008587876334786415,
        0.007734422571957111,
        -0.044966332614421844,
        -0.006710945162922144,
        -0.0020486218854784966,
        -0.019882800057530403,
        -0.017255762591958046,
        -0.004980701487511396,
        0.000838034669868648,
        -0.005024041049182415,
        0.01698905974626541,
        -0.0060508521273732185,
        -0.016709020361304283,
        -0.01949607953429222,
        0.005604122765362263,
        0.02142968401312828,
        0.00415058434009552,
        0.02695046178996563,
        0.228085458278656,
        -0.029284123331308365,
        -0.00036505141179077327,
        0.00515739293769002,
        0.003147109877318144,
        0.019802788272500038,
        -0.0013276869431138039,
        -0.0026303704362362623,
        -0.009774710051715374,
        -0.008867915719747543,
        0.04149917885661125,
        0.006914306897670031,
        -0.01312851533293724,
        -0.0017452457686886191,
        0.018242569640278816,
        -0.02336329035460949,
        -0.014148659072816372,
        -0.03421815112233162,
        -0.012601775117218494,
        0.01242841687053442,
        0.005134056322276592,
        0.01216171309351921,
        0.002530356403440237,
        -0.0008076137164607644,
        0.028937408700585365,
        0.004067239351570606,
        0.0032337887678295374,
        -0.0024486782494932413,
        0.021149644628167152,
        0.004924026783555746,
        -0.006197539623826742,
        -0.005454101599752903,
        0.011701648123562336,
        0.02085627056658268,
        -0.010248109698295593,
        -0.0009134619613178074,
        -0.029977554455399513,
        0.003218786558136344,
        0.018749307841062546,
        -0.017122412100434303,
        0.027657227590680122,
        -0.007541061844676733,
        0.010268112644553185,
        -0.04133915528655052,
        0.008041132241487503,
        -0.0009776377119123936,
        -0.004910691641271114,
        -0.006454242393374443,
        -0.004447293002158403,
        0.02202976867556572,
        -0.008947926573455334,
        0.02545691840350628,
        0.02098962292075157,
        0.005194064695388079,
        0.008361177518963814,
        0.00035546673461794853,
        -0.009854720905423164,
        0.009041273035109043,
        0.027977272868156433,
        0.005440766457468271,
        -0.008961262181401253,
        -0.0001698155829217285,
        -0.0010168098378926516,
        0.02301657572388649,
        -0.01312851533293724,
        0.022109780460596085,
        -0.02612367831170559,
        0.0231099221855402,
        0.0356316864490509,
        0.021509695798158646,
        0.007054326590150595,
        -0.023790016770362854,
        -0.007741090375930071,
        0.014895430766046047,
        -0.04389951750636101,
        -0.03424482420086861,
        0.051313892006874084,
        0.012488425709307194,
        0.039472226053476334,
        0.03003089688718319,
        0.009701366536319256,
        0.001888599363155663,
        -0.018175892531871796,
        -0.015495515428483486,
        -0.0010159764206036925,
        -0.04376616328954697,
        0.029790861532092094,
        -0.023056579753756523,
        0.001105155679397285,
        -0.024736817926168442,
        0.003963891416788101,
        -0.008494529873132706,
        -0.01133492961525917,
        -0.007301028352230787,
        0.008241160772740841,
        0.013481899164617062,
        0.017162416130304337,
        0.012588439509272575,
        -0.010454805567860603,
        -0.009167958050966263,
        -0.02545691840350628,
        0.06902305036783218,
        -0.0025470254477113485,
        0.00812781136482954,
        0.002713715424761176,
        0.01816255785524845,
        -0.013681926764547825,
        0.014055312611162663,
        0.008461191318929195,
        -0.011741654016077518,
        0.016802366822957993,
        -0.03139108791947365,
        0.008001127280294895,
        -0.0035538338124752045,
        0.011814997531473637,
        0.012115039862692356,
        0.029577499255537987,
        0.003673850791528821,
        0.018269238993525505,
        -0.0144286984577775,
        0.01733577437698841,
        -0.022403154522180557,
        0.009834717959165573,
        0.015162135474383831,
        0.005960839334875345,
        -0.004463961813598871,
        -0.018215898424386978,
        0.010241442359983921,
        -0.03221787139773369,
        -0.0032254543621093035,
        -0.014868760481476784,
        -0.02552359364926815,
        0.022109780460596085,
        -0.02835065871477127,
        -0.02536357194185257,
        -0.003680518362671137,
        -0.001108489464968443,
        -0.024963514879345894,
        -0.02416340261697769,
        0.02170972339808941,
        0.0020536226220428944,
        0.008201154880225658,
        0.016842372715473175,
        -7.469802221748978e-05,
        -0.020136170089244843,
        0.002567028161138296,
        -0.008741230703890324,
        -0.0031604450196027756,
        0.00635756179690361,
        -0.02727050706744194,
        -0.008581208065152168,
        -0.017882518470287323,
        -0.011448279023170471,
        -0.0006538420566357672,
        0.02216312102973461,
        0.010534817352890968,
        -0.006407569162547588,
        -0.008154481649398804,
        0.00683429604396224,
        0.005110719706863165,
        -0.02209644578397274,
        -0.021896416321396828,
        0.0017252429388463497,
        -0.0078077660873532295,
        -0.009794712997972965,
        -0.0016910715494304895,
        -0.16845038533210754,
        -0.0010668168542906642,
        0.025443583726882935,
        -0.046779923141002655,
        0.011995022185146809,
        0.0024936844129115343,
        0.02479015849530697,
        -0.030110906809568405,
        -0.05054045096039772,
        0.014602056704461575,
        -0.00010371252574259415,
        0.0020119501277804375,
        0.0016485655214637518,
        -0.009794712997972965,
        -0.03248457610607147,
        0.006497581955045462,
        -0.0242567490786314,
        0.01818922907114029,
        0.014588721096515656,
        -0.0027403859421610832,
        0.0377119779586792,
        -0.03800535202026367,
        0.026297036558389664,
        -0.011694980785250664,
        0.03547166287899017,
        -0.0014026975259184837,
        0.013775273226201534,
        0.019189368933439255,
        -0.01246175542473793,
        -0.04491299018263817,
        -0.0012218387564644217,
        -0.006454242393374443,
        0.03488491475582123,
        -0.022043105214834213,
        -0.009341315366327763,
        0.015148799866437912,
        -0.010774850845336914,
        -0.0057208058424293995,
        -0.00950800534337759,
        0.006914306897670031,
        0.029897544533014297,
        0.0073410337790846825,
        0.0043272762559354305,
        -0.008227825164794922,
        -0.02479015849530697,
        0.0020919612143188715,
        0.029124101623892784,
        -0.012181716039776802,
        0.004500634036958218,
        -0.011054890230298042,
        0.008987932465970516,
        -0.0006059186416678131,
        -0.002218645764514804,
        0.011148236691951752,
        0.0030404282733798027,
        0.003563835285604,
        0.028430670499801636,
        0.018469268456101418,
        -0.011301591992378235,
        -0.017189087346196175,
        -0.009121284820139408,
        -0.007814433425664902,
        0.0042939381673932076,
        0.009814715944230556,
        -0.015922242775559425,
        -0.011654974892735481,
        -0.009501338005065918,
        -0.012868478894233704,
        -0.007901112549006939,
        0.021002957597374916,
        -0.031444426625967026,
        -0.011895008385181427,
        -0.01071484200656414,
        -0.0052774096839129925,
        0.005614123772829771,
        0.017855847254395485,
        -0.017802506685256958,
        -0.022043105214834213,
        0.00307876686565578,
        -0.002470347797498107,
        -0.02237648516893387,
        0.021536367014050484,
        -0.0094279944896698,
        0.0033371367026120424,
        0.005594121292233467,
        0.016775695607066154,
        -0.0004917359328828752,
        0.02492351084947586,
        -0.019816124811768532,
        0.0009793045464903116,
        0.039125509560108185,
        -0.012721791863441467,
        0.0025470254477113485,
        -0.0034138141199946404,
        -0.010041413828730583,
        0.011214912869036198,
        0.0067742872051894665,
        0.0020119501277804375,
        0.004130581859499216,
        -0.018269238993525505,
        0.010754847899079323,
        -0.0027687232941389084,
        -0.015242146328091621,
        0.020136170089244843,
        0.03507160767912865,
        0.016175610944628716,
        -0.015562191605567932,
        0.003032093634828925,
        0.04336610808968544,
        0.00842785369604826,
        -0.01685570739209652,
        -0.004907357972115278,
        0.0009042940218932927,
        -0.0053107477724552155,
        -0.004023900255560875,
        0.03003089688718319,
        -0.0006146698724478483,
        -0.017469126731157303,
        -0.0029120768886059523,
        -0.019322721287608147,
        0.048806872218847275,
        -0.028404001146554947,
        -0.03237789496779442,
        0.029257453978061676,
        -0.011568295769393444,
        -0.0314977690577507,
        -0.11649640649557114,
        -0.025030191987752914,
        -0.011788327246904373,
        0.01285514421761036,
        -0.004233929794281721,
        0.009074611589312553,
        0.015922242775559425,
        0.024203408509492874,
        -0.03248457610607147,
        0.04189923405647278,
        -0.00016929468256421387,
        -0.01910935714840889,
        0.01704240031540394,
        -0.015175470151007175,
        -0.0073076956905424595,
        0.001351857092231512,
        0.012901817448437214,
        0.010488144122064114,
        -0.01071484200656414,
        0.028083955869078636,
        0.012068366631865501,
        -0.031071042641997337,
        0.021483026444911957,
        -0.016895713284611702,
        -0.004440625198185444,
        0.0018135887803509831,
        -0.017882518470287323,
        0.008594543673098087,
        -0.0010759849101305008,
        -0.022496500983834267,
        0.016322297975420952,
        -0.036778513342142105,
        0.0071143354289233685,
        -0.012515095993876457,
        0.01876264251768589,
        -0.01033478882163763,
        0.003907216712832451,
        -0.021483026444911957,
        -0.016802366822957993,
        -0.006744283251464367,
        -0.011421608738601208,
        0.005090716760605574,
        0.0011609968496486545,
        0.0032421231735497713,
        -0.01736244559288025,
        -0.0172424279153347,
        -0.022109780460596085,
        0.027030473574995995,
        0.0031721133273094893,
        -0.03235122188925743,
        -0.028270648792386055,
        -0.023643329739570618,
        -0.05024707689881325,
        -0.0038005353417247534,
        0.0024736816994845867,
        -0.014055312611162663,
        -0.001976944971829653,
        0.0036371788010001183,
        -0.01090153492987156,
        -0.005967507138848305,
        0.003607174614444375,
        -0.009701366536319256,
        -0.011088227853178978,
        0.029790861532092094,
        0.01476207934319973,
        -0.005707470700144768,
        -0.010001408867537975,
        -0.03240456432104111,
        -0.006384232547134161,
        -0.026110343635082245,
        -0.0015960581367835402,
        0.00963469035923481,
        0.0008342841756530106,
        0.007801098749041557,
        -0.03565835580229759,
        -0.0012410080526024103,
        -0.018709301948547363,
        -0.01910935714840889,
        -0.0038038690108805895,
        0.010368126444518566,
        -0.026577075943350792,
        -0.003317133756354451,
        0.008401183411478996,
        -0.024803493171930313,
        -0.013721932657063007,
        -0.0030054233502596617,
        -0.006550922524183989,
        -0.005297412630170584,
        -0.002998755546286702,
        -0.02708381414413452,
        0.0029920879751443863,
        0.02933746576309204,
        -0.011468281969428062,
        0.000514655839651823,
        -0.00992139708250761,
        0.021096304059028625,
        -0.0028037282172590494,
        -0.023816687986254692,
        0.0264437235891819,
        0.02308325096964836,
        -0.017949193716049194,
        -0.005944170523434877,
        -0.02301657572388649,
        0.0164423156529665,
        -0.009501338005065918,
        -0.008721228688955307,
        0.002960416954010725,
        -0.011468281969428062,
        0.04200591519474983,
        0.016709020361304283,
        -0.022563178092241287,
        0.008581208065152168,
        -0.009968070313334465,
        0.021136309951543808,
        0.0014035309432074428,
        -0.027217166498303413,
        -0.014922101981937885,
        -0.022403154522180557,
        0.014735408127307892,
        -0.022936563938856125,
        0.022149786353111267,
        0.01940273307263851,
        -0.005264074541628361,
        -0.014868760481476784,
        0.021029628813266754,
        0.0015785556752234697,
        0.007894445210695267,
        -0.002181974006816745,
        -0.00782776903361082,
        0.021056298166513443,
        -0.010708174668252468,
        -0.022176455706357956,
        0.031071042641997337,
        -0.01892266422510147,
        0.005000704433768988,
        0.014255341142416,
        -0.002030286006629467,
        0.015988918021321297,
        0.001396863372065127,
        -0.004817345179617405,
        0.022523172199726105,
        -0.006697610020637512,
        -0.005140724126249552,
        -0.02260318398475647,
        0.029070761054754257,
        -0.01850927248597145,
        0.013388551771640778,
        -0.007467718329280615,
        -0.022456495091319084,
        -0.0038738788571208715,
        0.01860261894762516,
        -0.018709301948547363,
        0.01704240031540394,
        0.0144286984577775,
        0.01222172100096941,
        -0.031444426625967026,
        -0.0016002253396436572,
        -0.004183922428637743,
        -0.016682349145412445,
        0.0001160580141004175,
        0.01542883925139904,
        0.003500493010506034,
        0.02701713889837265,
        0.004593980498611927,
        -0.009021270088851452,
        -0.010654834099113941,
        0.004697328433394432,
        0.013208527117967606,
        0.003597173374146223,
        0.013828613795340061,
        -0.003433816833421588,
        -0.012121707201004028,
        7.297902629943565e-05,
        0.011188242584466934,
        0.0011493285419419408,
        0.0016310630599036813,
        0.0344848558306694,
        0.0188559889793396,
        0.020522890612483025,
        -0.0037671972531825304,
        -0.00636422960087657,
        0.0476067028939724,
        0.014575386419892311,
        -0.019696107134222984,
        -0.016455650329589844,
        0.009374653920531273,
        0.01374860294163227,
        0.00465732254087925,
        0.006860966328531504,
        -0.02559027075767517,
        -0.011908343993127346,
        0.005557449534535408,
        0.007961121387779713,
        0.008087805472314358,
        -0.01366859208792448,
        0.03723191097378731,
        0.0037638633511960506,
        0.01720242202281952,
        0.0061842044815421104,
        0.030137578025460243,
        0.02304324507713318,
        0.006227543577551842,
        0.00256369449198246,
        -0.003430483164265752,
        -0.00278539233841002,
        0.010514814406633377,
        -0.004057237878441811,
        0.027630558237433434,
        -0.011401605792343616,
        -0.01742912083864212,
        0.019376061856746674,
        0.027230501174926758,
        0.0093146450817585,
        -0.008567873388528824,
        -0.01745579205453396,
        -0.007207681890577078,
        -0.017309105023741722,
        0.01660233736038208,
        -0.007327698636800051,
        -0.024870168417692184,
        -0.02463013492524624,
        0.005114053376019001,
        0.011975020170211792,
        -0.015562191605567932,
        0.049126919358968735,
        -0.013468563556671143,
        0.019829459488391876,
        -0.009454664774239063,
        0.025656946003437042,
        -0.020722918212413788,
        0.013681926764547825,
        0.02987087331712246,
        0.01765581965446472,
        -0.006704277358949184,
        0.010174766182899475,
        0.001715241582132876,
        -0.007134337909519672,
        -0.011434943415224552,
        0.02168305404484272,
        0.02603033185005188,
        -0.019949475303292274,
        0.07227684557437897,
        0.024483447894454002,
        0.007394374813884497,
        -0.0018469267524778843,
        -0.02349664270877838,
        0.022109780460596085,
        0.005240737926214933,
        0.015988918021321297,
        -0.023643329739570618,
        0.0017569140763953328,
        0.004317274782806635,
        0.0042572664096951485,
        0.0002967084583360702,
        -0.02361665852367878,
        0.0012376742670312524,
        -0.0051207211799919605,
        0.013175188563764095,
        0.003943888936191797,
        0.003112104954198003,
        0.00040609887219034135,
        0.03669850155711174,
        -0.009928064420819283,
        0.02244316041469574,
        0.0023703337647020817,
        -0.023643329739570618,
        0.004730666056275368,
        0.009594684466719627,
        0.03152443841099739,
        -0.0029920879751443863,
        -0.03699187561869621,
        0.016869042068719864,
        0.013301873579621315,
        -0.029470816254615784,
        -0.0031887823715806007,
        0.002603699918836355,
        -0.011301591992378235,
        -0.012708456255495548,
        -0.011701648123562336,
        0.0018019204726442695,
        0.012915152125060558,
        0.020749589428305626,
        0.03573836758732796,
        -0.021069634705781937,
        -0.023723341524600983,
        -0.0015610532136633992,
        0.002572028897702694,
        -0.01204836368560791,
        -0.02200309932231903,
        -0.006360895931720734
      ],
      "metrics": null,
      "error": null,
      "parent_run_id": "4d4879be-de55-475e-85ef-cc3de9de4117_0",
      "start_time": "2023-10-12T14:01:54.954796Z",
      "end_time": "2023-10-12T14:01:55.793973Z",
      "index": 0,
      "api_calls": [
        {
          "name": "embedQuestion",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "question": "What is the advantage of fine-tuning BERT over using feature-based approaches?"
          },
          "output": [
            -0.016869042068719864,
            -0.0003010840737260878,
            0.022069774568080902,
            -0.01818922907114029,
            -0.0022569845896214247,
            0.013475230894982815,
            -0.0065809269435703754,
            0.004023900255560875,
            -0.032884631305933,
            -0.019216040149331093,
            0.02651040069758892,
            0.030404282733798027,
            -0.024683475494384766,
            0.0007167676230892539,
            -0.018149223178625107,
            -0.015762219205498695,
            0.036565151065588,
            0.028804056346416473,
            0.005427430849522352,
            -0.025256890803575516,
            -0.011601634323596954,
            -0.009081278927624226,
            -0.024976851418614388,
            -0.011588298715651035,
            -0.02108296938240528,
            -0.0024053386878222227,
            0.045446399599313736,
            -0.020642906427383423,
            -0.01844259724020958,
            0.0037738648243248463,
            0.026403717696666718,
            0.02196309342980385,
            -0.016562333330512047,
            -0.02032286301255226,
            -0.011454946361482143,
            0.009701366536319256,
            -0.0029054090846329927,
            -0.01225505955517292,
            0.024843499064445496,
            -0.01254176627844572,
            0.01114156935364008,
            0.032751280814409256,
            0.011975020170211792,
            -0.015215476043522358,
            -0.0032737944275140762,
            0.0264437235891819,
            0.022069774568080902,
            -0.00033192173577845097,
            -0.03133774548768997,
            0.020056158304214478,
            0.002888740273192525,
            0.030564304441213608,
            -0.016975723206996918,
            0.003323801327496767,
            -0.05198065564036369,
            -0.007454383186995983,
            -0.006334225181490183,
            0.005664131138473749,
            -0.0017552472418174148,
            -0.025723623111844063,
            0.002136967610567808,
            0.006624266039580107,
            -0.00904794130474329,
            0.01246175542473793,
            -0.012735126540064812,
            -0.013095177710056305,
            -0.005047377664595842,
            0.022816546261310577,
            0.010688171721994877,
            -0.0024503450840711594,
            0.012088368646800518,
            0.02571028843522072,
            -0.01602892391383648,
            -0.006470911204814911,
            0.027577217668294907,
            -0.014828754588961601,
            -0.0028720712289214134,
            0.021576372906565666,
            0.01041479967534542,
            -0.0031987838447093964,
            -0.0014477039221674204,
            0.006234211381524801,
            -0.01090153492987156,
            0.014268675819039345,
            0.02060290239751339,
            0.014735408127307892,
            0.0026270365342497826,
            0.0010318119311705232,
            0.003170446492731571,
            0.015802225098013878,
            0.005537446588277817,
            0.018749307841062546,
            0.03416481241583824,
            0.02221646159887314,
            0.011074893176555634,
            -0.0029904211405664682,
            -0.005217401310801506,
            0.014562050811946392,
            -0.00028566524269990623,
            0.004353946540504694,
            -0.009027938358485699,
            0.009401324205100536,
            -0.029817532747983932,
            -0.011661642231047153,
            -0.004167253617197275,
            0.009067943319678307,
            0.018629290163517,
            -0.005494107026606798,
            -0.0012393412180244923,
            -0.0036205099895596504,
            -0.03037761151790619,
            0.014482039958238602,
            -0.014588721096515656,
            -0.014175329357385635,
            -0.0054140957072377205,
            0.0022019767202436924,
            0.0005113219958730042,
            -0.010294782929122448,
            -0.01401530671864748,
            0.004560642410069704,
            0.023629995062947273,
            -1.4324934454634786e-05,
            0.018815983086824417,
            -0.017189087346196175,
            0.01698905974626541,
            -0.019322721287608147,
            -0.015028783120214939,
            -0.017975864931941032,
            -0.007247687317430973,
            -0.009167958050966263,
            0.02002948708832264,
            0.021403014659881592,
            0.00963469035923481,
            -0.02092294581234455,
            -0.007207681890577078,
            0.005680799949914217,
            0.01781584322452545,
            -0.016815701499581337,
            -0.014482039958238602,
            -0.019189368933439255,
            0.023830022662878036,
            0.016175610944628716,
            -0.02591031603515148,
            -0.03267126902937889,
            -0.05371423065662384,
            0.02012283354997635,
            -0.000785110576543957,
            0.030564304441213608,
            0.008974596858024597,
            -0.022456495091319084,
            0.012995163910090923,
            -0.029284123331308365,
            -0.012835141271352768,
            0.03224454075098038,
            0.002288655610755086,
            0.0055507817305624485,
            0.0018986007198691368,
            0.006704277358949184,
            -0.014161994680762291,
            -0.0231099221855402,
            0.01802920550107956,
            0.017975864931941032,
            0.023736676201224327,
            -0.006900971755385399,
            0.017149081453680992,
            0.021949758753180504,
            0.0017919189995154738,
            -0.005604122765362263,
            0.017375780269503593,
            -0.001429367926903069,
            0.005760811269283295,
            0.030110906809568405,
            -0.014455368742346764,
            0.007487721275538206,
            -0.0011893342016264796,
            0.01284180860966444,
            0.03205784782767296,
            -0.020056158304214478,
            -0.004427290055900812,
            -0.03920552134513855,
            0.023029910400509834,
            0.010188101790845394,
            0.03371141478419304,
            -0.013248532079160213,
            -0.018642624840140343,
            0.008454523980617523,
            0.013828613795340061,
            -0.017135746777057648,
            0.007401042152196169,
            -0.036805182695388794,
            0.007361036725342274,
            0.004623984452337027,
            -0.0066509367898106575,
            -0.022496500983834267,
            -0.6383832097053528,
            -0.026083674281835556,
            0.011181574314832687,
            -0.022229798138141632,
            0.015642203390598297,
            0.0144286984577775,
            0.010294782929122448,
            0.008834577165544033,
            -0.014375357888638973,
            0.016895713284611702,
            -0.0036705168895423412,
            -0.0242567490786314,
            -0.012355073355138302,
            -0.017922524362802505,
            -0.006840963382273912,
            -0.024243414402008057,
            0.01041479967534542,
            -0.013495233841240406,
            0.005487439688295126,
            0.0028720712289214134,
            -0.015815559774637222,
            0.03184448555111885,
            -0.023856693878769875,
            -0.017122412100434303,
            -0.011888341046869755,
            -0.010554819367825985,
            0.008614546619355679,
            -0.008961262181401253,
            -0.0003694270271807909,
            -0.004160585813224316,
            -0.011228247545659542,
            0.02679044008255005,
            0.026337042450904846,
            -0.008227825164794922,
            0.03952556475996971,
            0.010241442359983921,
            -0.007987791672348976,
            0.011988354846835136,
            -9.897226846078411e-05,
            0.0432327538728714,
            -0.004000563640147448,
            -0.03136441856622696,
            0.01694905385375023,
            -0.006024181842803955,
            0.006390899885445833,
            0.009814715944230556,
            -0.0008313670987263322,
            0.020896276459097862,
            0.00922129862010479,
            -0.009908062405884266,
            -0.0033804760314524174,
            0.011508287861943245,
            -0.0031371084041893482,
            -6.230043800314888e-05,
            0.016562333330512047,
            -0.014135324396193027,
            0.02120298519730568,
            -0.01952274888753891,
            -0.0004071406729053706,
            -0.006010846700519323,
            0.010381462052464485,
            0.003970559220761061,
            -0.004810677375644445,
            -0.03733859211206436,
            -0.005260740872472525,
            0.007574399933218956,
            0.0069809830747544765,
            0.004774005617946386,
            0.007561064790934324,
            -0.04285936802625656,
            0.007201014086604118,
            0.016322297975420952,
            0.0020636238623410463,
            -0.011081560514867306,
            -0.007681081537157297,
            0.026750434190034866,
            0.01434868760406971,
            -0.007314363494515419,
            -0.006454242393374443,
            0.013001831248402596,
            0.010374794714152813,
            -0.01533549278974533,
            0.010788186453282833,
            0.027577217668294907,
            -0.023349955677986145,
            0.009194628335535526,
            -0.004710663575679064,
            -0.011328262276947498,
            -0.00980804767459631,
            0.003322134492918849,
            -0.0023269944358617067,
            0.025150207802653313,
            -0.009768042713403702,
            -0.042939379811286926,
            0.020109498873353004,
            0.004330609925091267,
            0.004727332387119532,
            -0.00904794130474329,
            0.008327839896082878,
            -0.022469831630587578,
            -0.004547307267785072,
            -0.019882800057530403,
            0.027390524744987488,
            0.015042118728160858,
            0.00933464802801609,
            -0.006587594281882048,
            0.022136451676487923,
            0.0007267690380103886,
            -0.00228532194159925,
            0.0009918063879013062,
            -8.381388761335984e-05,
            0.007681081537157297,
            -0.017055734992027283,
            -0.01434868760406971,
            -0.003103770548477769,
            -0.03584504872560501,
            0.012601775117218494,
            0.0008517866372130811,
            -0.008134478703141212,
            -0.009741371497511864,
            0.0019202703842893243,
            0.024656806141138077,
            0.008567873388528824,
            0.004580644890666008,
            0.011261586099863052,
            0.015562191605567932,
            -0.009461332112550735,
            0.012568436563014984,
            -0.00733436644077301,
            0.002718716161325574,
            -0.00025170212029479444,
            0.002108630258589983,
            0.01781584322452545,
            -0.010228106752038002,
            0.02216312102973461,
            0.02409672737121582,
            0.0046206507831811905,
            -0.0006038350402377546,
            -0.020562896504998207,
            0.0012026693439111114,
            -0.011868338100612164,
            -0.012488425709307194,
            0.006294219754636288,
            -0.014415363781154156,
            -0.04717997834086418,
            -0.016722355037927628,
            0.02098962292075157,
            0.015135465189814568,
            0.0012943489709869027,
            -0.01685570739209652,
            0.008741230703890324,
            -0.018389256671071053,
            -0.012808470986783504,
            -0.0009709700825624168,
            -0.001995281083509326,
            -0.010181433521211147,
            -0.004353946540504694,
            -0.05952838435769081,
            -0.0096013518050313,
            -0.04533971846103668,
            0.006727613974362612,
            0.03352472186088562,
            0.0033721416257321835,
            -0.00436394801363349,
            -0.007681081537157297,
            -0.033258017152547836,
            -0.005140724126249552,
            0.03653847798705101,
            -0.015082123689353466,
            -0.027950603514909744,
            0.006354228127747774,
            -0.0450730137526989,
            0.04003230482339859,
            0.01612227037549019,
            -0.01740245148539543,
            -0.0044839647598564625,
            -0.014988777227699757,
            0.004083908628672361,
            -0.031604450196027756,
            -0.015095459297299385,
            0.0010584824485704303,
            0.020109498873353004,
            0.0012585106305778027,
            -0.017189087346196175,
            0.02555026486515999,
            -0.0022236465010792017,
            -0.012055031023919582,
            -0.0006071688258089125,
            -0.029390806332230568,
            0.047473352402448654,
            0.002633704338222742,
            -0.019389396533370018,
            0.009187960997223854,
            0.017509132623672485,
            0.012181716039776802,
            -0.004353946540504694,
            0.02888406813144684,
            -0.0010501479264348745,
            -0.018562614917755127,
            0.0138552850112319,
            -0.0036605154164135456,
            0.019189368933439255,
            -0.003178780898451805,
            -0.03485824167728424,
            -0.013308540917932987,
            -0.008447856642305851,
            0.010388129390776157,
            -0.03920552134513855,
            -0.0034504858776926994,
            0.00650758296251297,
            0.011181574314832687,
            -0.01901601068675518,
            -0.012581772170960903,
            -0.021629713475704193,
            0.01392196025699377,
            0.01990947127342224,
            -0.011021552607417107,
            0.002422007732093334,
            -0.0055174436420202255,
            0.013268535025417805,
            0.012075033970177174,
            -0.014655397273600101,
            0.024270085617899895,
            -0.002778724767267704,
            -0.005184063222259283,
            -0.0003223370586056262,
            0.0032287880312651396,
            0.005104052368551493,
            0.025950321927666664,
            -0.03421815112233162,
            0.0017044066917151213,
            0.03101770207285881,
            0.007821101695299149,
            0.019136028364300728,
            0.0048806872218847275,
            -0.0024486782494932413,
            0.015962248668074608,
            -0.010881532914936543,
            0.02883072756230831,
            -0.011514955200254917,
            0.017122412100434303,
            0.001248509157449007,
            -0.009361318312585354,
            -0.011134901084005833,
            0.02469681203365326,
            0.0333380289375782,
            0.03813870623707771,
            0.03349805250763893,
            -0.017482461407780647,
            -0.006940977647900581,
            -0.012628445401787758,
            -0.013788608834147453,
            -0.004540639463812113,
            0.009768042713403702,
            0.04120580479502678,
            -0.009981405921280384,
            0.015482180751860142,
            0.005974174477159977,
            0.0027420527767390013,
            0.020309526473283768,
            0.007141005713492632,
            0.016735689714550972,
            0.005267408676445484,
            -0.005374090280383825,
            0.032937973737716675,
            0.0012951823882758617,
            0.007094332482665777,
            -0.013895289972424507,
            -0.0007371871615760028,
            0.00447729742154479,
            -0.016749026253819466,
            -0.017282433807849884,
            -0.0014060313114896417,
            -0.025123538449406624,
            0.019056016579270363,
            0.02066957764327526,
            -0.014322017319500446,
            0.03405813127756119,
            0.0005142390727996826,
            0.040112316608428955,
            -0.012008357793092728,
            -0.02021618001163006,
            0.03336469829082489,
            -0.01080818846821785,
            -0.015002112835645676,
            -0.013681926764547825,
            -0.019762782379984856,
            0.014415363781154156,
            -0.005027374718338251,
            -0.0027270507998764515,
            -0.0025253556668758392,
            -0.0008492862689308822,
            -0.023283278569579124,
            0.0034904915373772383,
            0.00037067721132189035,
            0.020816264674067497,
            0.014282011426985264,
            -0.018295910209417343,
            0.004490632563829422,
            0.011054890230298042,
            0.003813870484009385,
            0.012795135378837585,
            -0.029497487470507622,
            -0.016562333330512047,
            0.021763065829873085,
            -0.01116157229989767,
            -0.03952556475996971,
            -0.021883081644773483,
            -0.026110343635082245,
            -0.02713715471327305,
            0.040085647255182266,
            -0.011888341046869755,
            0.022136451676487923,
            -0.005554115399718285,
            0.0242567490786314,
            0.02555026486515999,
            0.009407991543412209,
            -0.0034704888239502907,
            0.027630558237433434,
            0.011928346939384937,
            -0.005354087334126234,
            -0.010068085044622421,
            -0.0007050993153825402,
            0.013168521225452423,
            0.04304606094956398,
            0.03256458789110184,
            -0.00974803976714611,
            0.010461472906172276,
            -0.01704240031540394,
            0.012875146232545376,
            -0.03320467472076416,
            -0.02536357194185257,
            -0.0011718317400664091,
            -0.017895853146910667,
            -0.011321594007313251,
            -0.007901112549006939,
            -0.0026853783056139946,
            -0.005240737926214933,
            -0.016495656222105026,
            -0.007641076110303402,
            -0.018389256671071053,
            -0.0395522378385067,
            0.019856128841638565,
            0.008367844857275486,
            -0.006880969274789095,
            0.021096304059028625,
            0.030617645010352135,
            0.027310512959957123,
            0.014402028173208237,
            0.008981265127658844,
            0.02117631584405899,
            0.02196309342980385,
            -0.005140724126249552,
            -0.019216040149331093,
            0.0032204536255449057,
            -0.016402309760451317,
            0.00904794130474329,
            0.014602056704461575,
            0.00784110464155674,
            0.020549559965729713,
            -0.00772108742967248,
            -0.009901394136250019,
            0.028057284653186798,
            0.008907921612262726,
            0.011394938454031944,
            0.01216171309351921,
            0.007207681890577078,
            -0.014148659072816372,
            0.010581490583717823,
            -0.03504493460059166,
            -0.00036525976611301303,
            0.031791143119335175,
            -0.006360895931720734,
            0.007661079056560993,
            -0.009247968904674053,
            0.01084152702242136,
            -0.027630558237433434,
            -0.006020847707986832,
            0.029630839824676514,
            -0.0015085458289831877,
            -0.018362585455179214,
            -0.0003198367194272578,
            -0.0021619710605591536,
            -0.01740245148539543,
            -0.012695121578872204,
            -0.016375640407204628,
            0.004904023837298155,
            -0.002453678986057639,
            0.002502019051462412,
            -0.0025136873591691256,
            -0.03085767850279808,
            -0.027470534667372704,
            -0.028777386993169785,
            0.019642766565084457,
            -0.001286847866140306,
            -0.024176737293601036,
            -0.049313612282276154,
            -0.014695403166115284,
            0.022069774568080902,
            0.02336329035460949,
            0.017029065638780594,
            -0.017415786162018776,
            -0.02663041651248932,
            -0.00934798363596201,
            0.007987791672348976,
            -0.021736394613981247,
            0.004193923901766539,
            -0.02400338090956211,
            -0.013008498586714268,
            -0.011534958146512508,
            -0.028937408700585365,
            -0.004750669002532959,
            -0.03299131244421005,
            0.013828613795340061,
            -0.009294642135500908,
            0.01660233736038208,
            0.018709301948547363,
            -0.019416067749261856,
            0.017189087346196175,
            0.01101488433778286,
            0.008341174572706223,
            0.011261586099863052,
            0.02142968401312828,
            -0.030937690287828445,
            0.018055876716971397,
            -0.010294782929122448,
            -0.023549983277916908,
            -0.017122412100434303,
            0.006437573116272688,
            0.009481335058808327,
            0.00430060550570488,
            -0.02920411340892315,
            0.011948348954319954,
            -0.014735408127307892,
            0.02057623118162155,
            -0.02467014081776142,
            -7.1364214818459e-05,
            0.005497440695762634,
            0.03685852512717247,
            0.016482321545481682,
            0.016895713284611702,
            0.022803211584687233,
            0.0029120768886059523,
            -0.00465065473690629,
            -0.008261163718998432,
            -0.02492351084947586,
            0.018562614917755127,
            0.0194294024258852,
            -0.010268112644553185,
            0.009821383282542229,
            0.032191202044487,
            -0.005837488919496536,
            -0.035951729863882065,
            0.02536357194185257,
            -0.04083241894841194,
            0.018682630732655525,
            -0.0010476475581526756,
            -0.017642484977841377,
            -0.03285796195268631,
            -0.02396337501704693,
            0.0066842748783528805,
            0.0009509672527201474,
            -0.04283269867300987,
            0.008201154880225658,
            0.0008651218377053738,
            0.0027320515364408493,
            0.0012268394930288196,
            -0.017975864931941032,
            0.023629995062947273,
            -0.02835065871477127,
            -0.018842654302716255,
            0.004237263463437557,
            0.014508710242807865,
            0.015122129581868649,
            -0.01768249087035656,
            0.013508569449186325,
            -0.033258017152547836,
            -0.013895289972424507,
            0.009001268073916435,
            -0.018109217286109924,
            0.004373949486762285,
            0.018055876716971397,
            0.007707752287387848,
            0.01534882839769125,
            0.03581837937235832,
            0.0036538478452712297,
            0.01958942599594593,
            0.021763065829873085,
            -0.004887355025857687,
            0.009214631281793118,
            -0.023856693878769875,
            0.009614687412977219,
            -0.0017619148129597306,
            0.011034887284040451,
            0.00962135475128889,
            -0.01570887863636017,
            -0.01452204491943121,
            -0.0052207354456186295,
            0.019802788272500038,
            0.010608160868287086,
            -0.007567732594907284,
            -0.018042540177702904,
            -0.02867070399224758,
            -0.023403296247124672,
            -0.024776821956038475,
            0.0019369394285604358,
            -0.003133774735033512,
            -0.005497440695762634,
            -0.05158059671521187,
            -0.01812255196273327,
            0.0008013628539629281,
            0.0031587781850248575,
            0.008267831057310104,
            0.0008297002059407532,
            0.020389538258314133,
            0.0036538478452712297,
            -0.007654411252588034,
            0.009061275981366634,
            0.0030070901848375797,
            -0.014468704350292683,
            -0.007314363494515419,
            -0.02752387709915638,
            -0.011081560514867306,
            0.03640512749552727,
            0.021149644628167152,
            -0.00500737177208066,
            0.00585749140009284,
            -0.010301451198756695,
            -0.001103488728404045,
            0.013228530064225197,
            0.0029070761520415545,
            -0.013155185617506504,
            -0.025243554264307022,
            -0.014708737842738628,
            -0.021389678120613098,
            -0.030804337933659554,
            -0.012588439509272575,
            0.02904408983886242,
            0.013721932657063007,
            -0.008114475756883621,
            -0.029124101623892784,
            0.032964643090963364,
            -0.0002389919973211363,
            -0.0033037986140698195,
            0.004023900255560875,
            0.024883504956960678,
            0.032937973737716675,
            -0.001675235922448337,
            0.011381602846086025,
            0.016242288053035736,
            0.0022786541376262903,
            -0.006347560789436102,
            0.0021619710605591536,
            0.004997370298951864,
            -0.03035094030201435,
            -0.000267537689069286,
            -0.009788044728338718,
            -0.0005438266089186072,
            -0.010421467944979668,
            0.016909047961235046,
            0.01042813528329134,
            -0.03117772378027439,
            -0.02053622528910637,
            0.014322017319500446,
            -0.007281025405973196,
            0.023923369124531746,
            -0.002218645764514804,
            -0.019322721287608147,
            -0.001751913339830935,
            0.013868619687855244,
            -0.01653566211462021,
            0.02869737520813942,
            0.002815396524965763,
            0.00013929045235272497,
            0.019096022471785545,
            0.02708381414413452,
            -0.010001408867537975,
            0.022816546261310577,
            -0.01444203406572342,
            0.008687890134751797,
            0.00265037314966321,
            0.006967647932469845,
            -0.020362867042422295,
            -0.011654974892735481,
            0.006797623820602894,
            0.020749589428305626,
            -0.012535098940134048,
            0.017122412100434303,
            0.007974456064403057,
            -0.011614969000220299,
            0.00662760017439723,
            0.009134619496762753,
            -0.0014418697683140635,
            0.03083100914955139,
            -0.02787059172987938,
            -0.005124054849147797,
            -0.0007338533760048449,
            0.004497299902141094,
            0.017375780269503593,
            -0.013788608834147453,
            0.001751913339830935,
            0.0007451049168594182,
            -0.017029065638780594,
            0.01570887863636017,
            0.025563599541783333,
            0.024150067940354347,
            -0.023763347417116165,
            0.0072743576020002365,
            0.005717471707612276,
            -0.0014535380760207772,
            -0.002433676039800048,
            0.006204206962138414,
            0.01993614062666893,
            -0.03555167466402054,
            -0.013788608834147453,
            0.034111469984054565,
            0.014282011426985264,
            0.020149504765868187,
            -0.00895459484308958,
            -0.011481616646051407,
            -0.014455368742346764,
            0.02984420396387577,
            -0.04373949393630028,
            0.007947785779833794,
            0.012395079247653484,
            -0.013681926764547825,
            -0.013208527117967606,
            -0.018269238993525505,
            -0.015575527213513851,
            -0.01293515507131815,
            0.0007967788842506707,
            -0.0015035450924187899,
            -0.022229798138141632,
            -0.0019869464449584484,
            -0.005197398830205202,
            -0.005670798476785421,
            0.007994459010660648,
            -0.022136451676487923,
            -0.020896276459097862,
            0.018936000764369965,
            0.0206962488591671,
            -0.019376061856746674,
            -0.01366859208792448,
            -0.012435084208846092,
            0.0029220781289041042,
            -0.00328212883323431,
            0.01271512359380722,
            -0.027977272868156433,
            -0.01452204491943121,
            0.0016694017685949802,
            0.01758914440870285,
            -0.011928346939384937,
            -0.04101911187171936,
            -0.0011543292785063386,
            -0.006974315736442804,
            0.009461332112550735,
            -0.0018819316755980253,
            -0.015028783120214939,
            0.022083109244704247,
            -0.01878931373357773,
            -0.008787903934717178,
            0.02591031603515148,
            -0.00884124543517828,
            -0.010481475852429867,
            0.003190449206158519,
            -0.00792111549526453,
            0.013341878540813923,
            0.02479015849530697,
            -0.0097147012129426,
            -0.034964922815561295,
            -0.011754988692700863,
            -0.003883880330249667,
            -0.017882518470287323,
            -0.008487861603498459,
            -0.015548856928944588,
            0.051633939146995544,
            0.031444426625967026,
            -0.010228106752038002,
            -0.010674837045371532,
            0.0024153401609510183,
            -0.025830304250121117,
            0.003423815593123436,
            -0.005870827008038759,
            0.0070809973403811455,
            0.0010659834370017052,
            -0.021229656413197517,
            -0.0023453303147107363,
            0.017735831439495087,
            -0.0004996536881662905,
            -0.004964032210409641,
            -0.008261163718998432,
            -0.0018719303188845515,
            -0.021002957597374916,
            -0.01841592602431774,
            0.00025232721236534417,
            -0.012681785970926285,
            -0.03933887183666229,
            0.014708737842738628,
            0.028617363423109055,
            -0.006250880192965269,
            0.0172424279153347,
            0.04387284442782402,
            -0.00871456041932106,
            0.001788585213944316,
            -0.00974803976714611,
            0.016655679792165756,
            0.02186974696815014,
            0.017389114946126938,
            0.011168239638209343,
            -0.026270367205142975,
            0.01154162548482418,
            0.04171254113316536,
            -0.0037538621108978987,
            -0.02050955593585968,
            0.002850401448085904,
            0.0047906748950481415,
            0.00784110464155674,
            -0.012101704254746437,
            -0.021856412291526794,
            -0.0020186176989227533,
            0.0019469409016892314,
            -0.019856128841638565,
            0.005077381618320942,
            0.01781584322452545,
            -0.02888406813144684,
            0.0073076956905424595,
            0.0028604029212146997,
            0.00657092547044158,
            -0.03157778084278107,
            0.017722496762871742,
            -0.010734844952821732,
            -0.0012285063276067376,
            0.01917603425681591,
            -0.0012593440478667617,
            0.0016493989387527108,
            -0.016655679792165756,
            0.005594121292233467,
            0.009928064420819283,
            -0.006174203008413315,
            -0.03301798179745674,
            0.001644398202188313,
            -0.023216603323817253,
            0.008587876334786415,
            0.007734422571957111,
            -0.044966332614421844,
            -0.006710945162922144,
            -0.0020486218854784966,
            -0.019882800057530403,
            -0.017255762591958046,
            -0.004980701487511396,
            0.000838034669868648,
            -0.005024041049182415,
            0.01698905974626541,
            -0.0060508521273732185,
            -0.016709020361304283,
            -0.01949607953429222,
            0.005604122765362263,
            0.02142968401312828,
            0.00415058434009552,
            0.02695046178996563,
            0.228085458278656,
            -0.029284123331308365,
            -0.00036505141179077327,
            0.00515739293769002,
            0.003147109877318144,
            0.019802788272500038,
            -0.0013276869431138039,
            -0.0026303704362362623,
            -0.009774710051715374,
            -0.008867915719747543,
            0.04149917885661125,
            0.006914306897670031,
            -0.01312851533293724,
            -0.0017452457686886191,
            0.018242569640278816,
            -0.02336329035460949,
            -0.014148659072816372,
            -0.03421815112233162,
            -0.012601775117218494,
            0.01242841687053442,
            0.005134056322276592,
            0.01216171309351921,
            0.002530356403440237,
            -0.0008076137164607644,
            0.028937408700585365,
            0.004067239351570606,
            0.0032337887678295374,
            -0.0024486782494932413,
            0.021149644628167152,
            0.004924026783555746,
            -0.006197539623826742,
            -0.005454101599752903,
            0.011701648123562336,
            0.02085627056658268,
            -0.010248109698295593,
            -0.0009134619613178074,
            -0.029977554455399513,
            0.003218786558136344,
            0.018749307841062546,
            -0.017122412100434303,
            0.027657227590680122,
            -0.007541061844676733,
            0.010268112644553185,
            -0.04133915528655052,
            0.008041132241487503,
            -0.0009776377119123936,
            -0.004910691641271114,
            -0.006454242393374443,
            -0.004447293002158403,
            0.02202976867556572,
            -0.008947926573455334,
            0.02545691840350628,
            0.02098962292075157,
            0.005194064695388079,
            0.008361177518963814,
            0.00035546673461794853,
            -0.009854720905423164,
            0.009041273035109043,
            0.027977272868156433,
            0.005440766457468271,
            -0.008961262181401253,
            -0.0001698155829217285,
            -0.0010168098378926516,
            0.02301657572388649,
            -0.01312851533293724,
            0.022109780460596085,
            -0.02612367831170559,
            0.0231099221855402,
            0.0356316864490509,
            0.021509695798158646,
            0.007054326590150595,
            -0.023790016770362854,
            -0.007741090375930071,
            0.014895430766046047,
            -0.04389951750636101,
            -0.03424482420086861,
            0.051313892006874084,
            0.012488425709307194,
            0.039472226053476334,
            0.03003089688718319,
            0.009701366536319256,
            0.001888599363155663,
            -0.018175892531871796,
            -0.015495515428483486,
            -0.0010159764206036925,
            -0.04376616328954697,
            0.029790861532092094,
            -0.023056579753756523,
            0.001105155679397285,
            -0.024736817926168442,
            0.003963891416788101,
            -0.008494529873132706,
            -0.01133492961525917,
            -0.007301028352230787,
            0.008241160772740841,
            0.013481899164617062,
            0.017162416130304337,
            0.012588439509272575,
            -0.010454805567860603,
            -0.009167958050966263,
            -0.02545691840350628,
            0.06902305036783218,
            -0.0025470254477113485,
            0.00812781136482954,
            0.002713715424761176,
            0.01816255785524845,
            -0.013681926764547825,
            0.014055312611162663,
            0.008461191318929195,
            -0.011741654016077518,
            0.016802366822957993,
            -0.03139108791947365,
            0.008001127280294895,
            -0.0035538338124752045,
            0.011814997531473637,
            0.012115039862692356,
            0.029577499255537987,
            0.003673850791528821,
            0.018269238993525505,
            -0.0144286984577775,
            0.01733577437698841,
            -0.022403154522180557,
            0.009834717959165573,
            0.015162135474383831,
            0.005960839334875345,
            -0.004463961813598871,
            -0.018215898424386978,
            0.010241442359983921,
            -0.03221787139773369,
            -0.0032254543621093035,
            -0.014868760481476784,
            -0.02552359364926815,
            0.022109780460596085,
            -0.02835065871477127,
            -0.02536357194185257,
            -0.003680518362671137,
            -0.001108489464968443,
            -0.024963514879345894,
            -0.02416340261697769,
            0.02170972339808941,
            0.0020536226220428944,
            0.008201154880225658,
            0.016842372715473175,
            -7.469802221748978e-05,
            -0.020136170089244843,
            0.002567028161138296,
            -0.008741230703890324,
            -0.0031604450196027756,
            0.00635756179690361,
            -0.02727050706744194,
            -0.008581208065152168,
            -0.017882518470287323,
            -0.011448279023170471,
            -0.0006538420566357672,
            0.02216312102973461,
            0.010534817352890968,
            -0.006407569162547588,
            -0.008154481649398804,
            0.00683429604396224,
            0.005110719706863165,
            -0.02209644578397274,
            -0.021896416321396828,
            0.0017252429388463497,
            -0.0078077660873532295,
            -0.009794712997972965,
            -0.0016910715494304895,
            -0.16845038533210754,
            -0.0010668168542906642,
            0.025443583726882935,
            -0.046779923141002655,
            0.011995022185146809,
            0.0024936844129115343,
            0.02479015849530697,
            -0.030110906809568405,
            -0.05054045096039772,
            0.014602056704461575,
            -0.00010371252574259415,
            0.0020119501277804375,
            0.0016485655214637518,
            -0.009794712997972965,
            -0.03248457610607147,
            0.006497581955045462,
            -0.0242567490786314,
            0.01818922907114029,
            0.014588721096515656,
            -0.0027403859421610832,
            0.0377119779586792,
            -0.03800535202026367,
            0.026297036558389664,
            -0.011694980785250664,
            0.03547166287899017,
            -0.0014026975259184837,
            0.013775273226201534,
            0.019189368933439255,
            -0.01246175542473793,
            -0.04491299018263817,
            -0.0012218387564644217,
            -0.006454242393374443,
            0.03488491475582123,
            -0.022043105214834213,
            -0.009341315366327763,
            0.015148799866437912,
            -0.010774850845336914,
            -0.0057208058424293995,
            -0.00950800534337759,
            0.006914306897670031,
            0.029897544533014297,
            0.0073410337790846825,
            0.0043272762559354305,
            -0.008227825164794922,
            -0.02479015849530697,
            0.0020919612143188715,
            0.029124101623892784,
            -0.012181716039776802,
            0.004500634036958218,
            -0.011054890230298042,
            0.008987932465970516,
            -0.0006059186416678131,
            -0.002218645764514804,
            0.011148236691951752,
            0.0030404282733798027,
            0.003563835285604,
            0.028430670499801636,
            0.018469268456101418,
            -0.011301591992378235,
            -0.017189087346196175,
            -0.009121284820139408,
            -0.007814433425664902,
            0.0042939381673932076,
            0.009814715944230556,
            -0.015922242775559425,
            -0.011654974892735481,
            -0.009501338005065918,
            -0.012868478894233704,
            -0.007901112549006939,
            0.021002957597374916,
            -0.031444426625967026,
            -0.011895008385181427,
            -0.01071484200656414,
            -0.0052774096839129925,
            0.005614123772829771,
            0.017855847254395485,
            -0.017802506685256958,
            -0.022043105214834213,
            0.00307876686565578,
            -0.002470347797498107,
            -0.02237648516893387,
            0.021536367014050484,
            -0.0094279944896698,
            0.0033371367026120424,
            0.005594121292233467,
            0.016775695607066154,
            -0.0004917359328828752,
            0.02492351084947586,
            -0.019816124811768532,
            0.0009793045464903116,
            0.039125509560108185,
            -0.012721791863441467,
            0.0025470254477113485,
            -0.0034138141199946404,
            -0.010041413828730583,
            0.011214912869036198,
            0.0067742872051894665,
            0.0020119501277804375,
            0.004130581859499216,
            -0.018269238993525505,
            0.010754847899079323,
            -0.0027687232941389084,
            -0.015242146328091621,
            0.020136170089244843,
            0.03507160767912865,
            0.016175610944628716,
            -0.015562191605567932,
            0.003032093634828925,
            0.04336610808968544,
            0.00842785369604826,
            -0.01685570739209652,
            -0.004907357972115278,
            0.0009042940218932927,
            -0.0053107477724552155,
            -0.004023900255560875,
            0.03003089688718319,
            -0.0006146698724478483,
            -0.017469126731157303,
            -0.0029120768886059523,
            -0.019322721287608147,
            0.048806872218847275,
            -0.028404001146554947,
            -0.03237789496779442,
            0.029257453978061676,
            -0.011568295769393444,
            -0.0314977690577507,
            -0.11649640649557114,
            -0.025030191987752914,
            -0.011788327246904373,
            0.01285514421761036,
            -0.004233929794281721,
            0.009074611589312553,
            0.015922242775559425,
            0.024203408509492874,
            -0.03248457610607147,
            0.04189923405647278,
            -0.00016929468256421387,
            -0.01910935714840889,
            0.01704240031540394,
            -0.015175470151007175,
            -0.0073076956905424595,
            0.001351857092231512,
            0.012901817448437214,
            0.010488144122064114,
            -0.01071484200656414,
            0.028083955869078636,
            0.012068366631865501,
            -0.031071042641997337,
            0.021483026444911957,
            -0.016895713284611702,
            -0.004440625198185444,
            0.0018135887803509831,
            -0.017882518470287323,
            0.008594543673098087,
            -0.0010759849101305008,
            -0.022496500983834267,
            0.016322297975420952,
            -0.036778513342142105,
            0.0071143354289233685,
            -0.012515095993876457,
            0.01876264251768589,
            -0.01033478882163763,
            0.003907216712832451,
            -0.021483026444911957,
            -0.016802366822957993,
            -0.006744283251464367,
            -0.011421608738601208,
            0.005090716760605574,
            0.0011609968496486545,
            0.0032421231735497713,
            -0.01736244559288025,
            -0.0172424279153347,
            -0.022109780460596085,
            0.027030473574995995,
            0.0031721133273094893,
            -0.03235122188925743,
            -0.028270648792386055,
            -0.023643329739570618,
            -0.05024707689881325,
            -0.0038005353417247534,
            0.0024736816994845867,
            -0.014055312611162663,
            -0.001976944971829653,
            0.0036371788010001183,
            -0.01090153492987156,
            -0.005967507138848305,
            0.003607174614444375,
            -0.009701366536319256,
            -0.011088227853178978,
            0.029790861532092094,
            0.01476207934319973,
            -0.005707470700144768,
            -0.010001408867537975,
            -0.03240456432104111,
            -0.006384232547134161,
            -0.026110343635082245,
            -0.0015960581367835402,
            0.00963469035923481,
            0.0008342841756530106,
            0.007801098749041557,
            -0.03565835580229759,
            -0.0012410080526024103,
            -0.018709301948547363,
            -0.01910935714840889,
            -0.0038038690108805895,
            0.010368126444518566,
            -0.026577075943350792,
            -0.003317133756354451,
            0.008401183411478996,
            -0.024803493171930313,
            -0.013721932657063007,
            -0.0030054233502596617,
            -0.006550922524183989,
            -0.005297412630170584,
            -0.002998755546286702,
            -0.02708381414413452,
            0.0029920879751443863,
            0.02933746576309204,
            -0.011468281969428062,
            0.000514655839651823,
            -0.00992139708250761,
            0.021096304059028625,
            -0.0028037282172590494,
            -0.023816687986254692,
            0.0264437235891819,
            0.02308325096964836,
            -0.017949193716049194,
            -0.005944170523434877,
            -0.02301657572388649,
            0.0164423156529665,
            -0.009501338005065918,
            -0.008721228688955307,
            0.002960416954010725,
            -0.011468281969428062,
            0.04200591519474983,
            0.016709020361304283,
            -0.022563178092241287,
            0.008581208065152168,
            -0.009968070313334465,
            0.021136309951543808,
            0.0014035309432074428,
            -0.027217166498303413,
            -0.014922101981937885,
            -0.022403154522180557,
            0.014735408127307892,
            -0.022936563938856125,
            0.022149786353111267,
            0.01940273307263851,
            -0.005264074541628361,
            -0.014868760481476784,
            0.021029628813266754,
            0.0015785556752234697,
            0.007894445210695267,
            -0.002181974006816745,
            -0.00782776903361082,
            0.021056298166513443,
            -0.010708174668252468,
            -0.022176455706357956,
            0.031071042641997337,
            -0.01892266422510147,
            0.005000704433768988,
            0.014255341142416,
            -0.002030286006629467,
            0.015988918021321297,
            0.001396863372065127,
            -0.004817345179617405,
            0.022523172199726105,
            -0.006697610020637512,
            -0.005140724126249552,
            -0.02260318398475647,
            0.029070761054754257,
            -0.01850927248597145,
            0.013388551771640778,
            -0.007467718329280615,
            -0.022456495091319084,
            -0.0038738788571208715,
            0.01860261894762516,
            -0.018709301948547363,
            0.01704240031540394,
            0.0144286984577775,
            0.01222172100096941,
            -0.031444426625967026,
            -0.0016002253396436572,
            -0.004183922428637743,
            -0.016682349145412445,
            0.0001160580141004175,
            0.01542883925139904,
            0.003500493010506034,
            0.02701713889837265,
            0.004593980498611927,
            -0.009021270088851452,
            -0.010654834099113941,
            0.004697328433394432,
            0.013208527117967606,
            0.003597173374146223,
            0.013828613795340061,
            -0.003433816833421588,
            -0.012121707201004028,
            7.297902629943565e-05,
            0.011188242584466934,
            0.0011493285419419408,
            0.0016310630599036813,
            0.0344848558306694,
            0.0188559889793396,
            0.020522890612483025,
            -0.0037671972531825304,
            -0.00636422960087657,
            0.0476067028939724,
            0.014575386419892311,
            -0.019696107134222984,
            -0.016455650329589844,
            0.009374653920531273,
            0.01374860294163227,
            0.00465732254087925,
            0.006860966328531504,
            -0.02559027075767517,
            -0.011908343993127346,
            0.005557449534535408,
            0.007961121387779713,
            0.008087805472314358,
            -0.01366859208792448,
            0.03723191097378731,
            0.0037638633511960506,
            0.01720242202281952,
            0.0061842044815421104,
            0.030137578025460243,
            0.02304324507713318,
            0.006227543577551842,
            0.00256369449198246,
            -0.003430483164265752,
            -0.00278539233841002,
            0.010514814406633377,
            -0.004057237878441811,
            0.027630558237433434,
            -0.011401605792343616,
            -0.01742912083864212,
            0.019376061856746674,
            0.027230501174926758,
            0.0093146450817585,
            -0.008567873388528824,
            -0.01745579205453396,
            -0.007207681890577078,
            -0.017309105023741722,
            0.01660233736038208,
            -0.007327698636800051,
            -0.024870168417692184,
            -0.02463013492524624,
            0.005114053376019001,
            0.011975020170211792,
            -0.015562191605567932,
            0.049126919358968735,
            -0.013468563556671143,
            0.019829459488391876,
            -0.009454664774239063,
            0.025656946003437042,
            -0.020722918212413788,
            0.013681926764547825,
            0.02987087331712246,
            0.01765581965446472,
            -0.006704277358949184,
            0.010174766182899475,
            0.001715241582132876,
            -0.007134337909519672,
            -0.011434943415224552,
            0.02168305404484272,
            0.02603033185005188,
            -0.019949475303292274,
            0.07227684557437897,
            0.024483447894454002,
            0.007394374813884497,
            -0.0018469267524778843,
            -0.02349664270877838,
            0.022109780460596085,
            0.005240737926214933,
            0.015988918021321297,
            -0.023643329739570618,
            0.0017569140763953328,
            0.004317274782806635,
            0.0042572664096951485,
            0.0002967084583360702,
            -0.02361665852367878,
            0.0012376742670312524,
            -0.0051207211799919605,
            0.013175188563764095,
            0.003943888936191797,
            0.003112104954198003,
            0.00040609887219034135,
            0.03669850155711174,
            -0.009928064420819283,
            0.02244316041469574,
            0.0023703337647020817,
            -0.023643329739570618,
            0.004730666056275368,
            0.009594684466719627,
            0.03152443841099739,
            -0.0029920879751443863,
            -0.03699187561869621,
            0.016869042068719864,
            0.013301873579621315,
            -0.029470816254615784,
            -0.0031887823715806007,
            0.002603699918836355,
            -0.011301591992378235,
            -0.012708456255495548,
            -0.011701648123562336,
            0.0018019204726442695,
            0.012915152125060558,
            0.020749589428305626,
            0.03573836758732796,
            -0.021069634705781937,
            -0.023723341524600983,
            -0.0015610532136633992,
            0.002572028897702694,
            -0.01204836368560791,
            -0.02200309932231903,
            -0.006360895931720734
          ],
          "start_time": 1697137314.954796,
          "end_time": 1697137315.767159,
          "error": null,
          "children": [
            {
              "name": "openai.api_resources.embedding.Embedding.create",
              "type": "LLM",
              "inputs": {
                "input": "What is the advantage of fine-tuning BERT over using feature-based approaches?",
                "engine": "embedding"
              },
              "output": {
                "object": "list",
                "data": [
                  {
                    "object": "embedding",
                    "index": 0,
                    "embedding": [
                      -0.016869042068719864,
                      -0.0003010840737260878,
                      0.022069774568080902,
                      -0.01818922907114029,
                      -0.0022569845896214247,
                      0.013475230894982815,
                      -0.0065809269435703754,
                      0.004023900255560875,
                      -0.032884631305933,
                      -0.019216040149331093,
                      0.02651040069758892,
                      0.030404282733798027,
                      -0.024683475494384766,
                      0.0007167676230892539,
                      -0.018149223178625107,
                      -0.015762219205498695,
                      0.036565151065588,
                      0.028804056346416473,
                      0.005427430849522352,
                      -0.025256890803575516,
                      -0.011601634323596954,
                      -0.009081278927624226,
                      -0.024976851418614388,
                      -0.011588298715651035,
                      -0.02108296938240528,
                      -0.0024053386878222227,
                      0.045446399599313736,
                      -0.020642906427383423,
                      -0.01844259724020958,
                      0.0037738648243248463,
                      0.026403717696666718,
                      0.02196309342980385,
                      -0.016562333330512047,
                      -0.02032286301255226,
                      -0.011454946361482143,
                      0.009701366536319256,
                      -0.0029054090846329927,
                      -0.01225505955517292,
                      0.024843499064445496,
                      -0.01254176627844572,
                      0.01114156935364008,
                      0.032751280814409256,
                      0.011975020170211792,
                      -0.015215476043522358,
                      -0.0032737944275140762,
                      0.0264437235891819,
                      0.022069774568080902,
                      -0.00033192173577845097,
                      -0.03133774548768997,
                      0.020056158304214478,
                      0.002888740273192525,
                      0.030564304441213608,
                      -0.016975723206996918,
                      0.003323801327496767,
                      -0.05198065564036369,
                      -0.007454383186995983,
                      -0.006334225181490183,
                      0.005664131138473749,
                      -0.0017552472418174148,
                      -0.025723623111844063,
                      0.002136967610567808,
                      0.006624266039580107,
                      -0.00904794130474329,
                      0.01246175542473793,
                      -0.012735126540064812,
                      -0.013095177710056305,
                      -0.005047377664595842,
                      0.022816546261310577,
                      0.010688171721994877,
                      -0.0024503450840711594,
                      0.012088368646800518,
                      0.02571028843522072,
                      -0.01602892391383648,
                      -0.006470911204814911,
                      0.027577217668294907,
                      -0.014828754588961601,
                      -0.0028720712289214134,
                      0.021576372906565666,
                      0.01041479967534542,
                      -0.0031987838447093964,
                      -0.0014477039221674204,
                      0.006234211381524801,
                      -0.01090153492987156,
                      0.014268675819039345,
                      0.02060290239751339,
                      0.014735408127307892,
                      0.0026270365342497826,
                      0.0010318119311705232,
                      0.003170446492731571,
                      0.015802225098013878,
                      0.005537446588277817,
                      0.018749307841062546,
                      0.03416481241583824,
                      0.02221646159887314,
                      0.011074893176555634,
                      -0.0029904211405664682,
                      -0.005217401310801506,
                      0.014562050811946392,
                      -0.00028566524269990623,
                      0.004353946540504694,
                      -0.009027938358485699,
                      0.009401324205100536,
                      -0.029817532747983932,
                      -0.011661642231047153,
                      -0.004167253617197275,
                      0.009067943319678307,
                      0.018629290163517,
                      -0.005494107026606798,
                      -0.0012393412180244923,
                      -0.0036205099895596504,
                      -0.03037761151790619,
                      0.014482039958238602,
                      -0.014588721096515656,
                      -0.014175329357385635,
                      -0.0054140957072377205,
                      0.0022019767202436924,
                      0.0005113219958730042,
                      -0.010294782929122448,
                      -0.01401530671864748,
                      0.004560642410069704,
                      0.023629995062947273,
                      -1.4324934454634786e-05,
                      0.018815983086824417,
                      -0.017189087346196175,
                      0.01698905974626541,
                      -0.019322721287608147,
                      -0.015028783120214939,
                      -0.017975864931941032,
                      -0.007247687317430973,
                      -0.009167958050966263,
                      0.02002948708832264,
                      0.021403014659881592,
                      0.00963469035923481,
                      -0.02092294581234455,
                      -0.007207681890577078,
                      0.005680799949914217,
                      0.01781584322452545,
                      -0.016815701499581337,
                      -0.014482039958238602,
                      -0.019189368933439255,
                      0.023830022662878036,
                      0.016175610944628716,
                      -0.02591031603515148,
                      -0.03267126902937889,
                      -0.05371423065662384,
                      0.02012283354997635,
                      -0.000785110576543957,
                      0.030564304441213608,
                      0.008974596858024597,
                      -0.022456495091319084,
                      0.012995163910090923,
                      -0.029284123331308365,
                      -0.012835141271352768,
                      0.03224454075098038,
                      0.002288655610755086,
                      0.0055507817305624485,
                      0.0018986007198691368,
                      0.006704277358949184,
                      -0.014161994680762291,
                      -0.0231099221855402,
                      0.01802920550107956,
                      0.017975864931941032,
                      0.023736676201224327,
                      -0.006900971755385399,
                      0.017149081453680992,
                      0.021949758753180504,
                      0.0017919189995154738,
                      -0.005604122765362263,
                      0.017375780269503593,
                      -0.001429367926903069,
                      0.005760811269283295,
                      0.030110906809568405,
                      -0.014455368742346764,
                      0.007487721275538206,
                      -0.0011893342016264796,
                      0.01284180860966444,
                      0.03205784782767296,
                      -0.020056158304214478,
                      -0.004427290055900812,
                      -0.03920552134513855,
                      0.023029910400509834,
                      0.010188101790845394,
                      0.03371141478419304,
                      -0.013248532079160213,
                      -0.018642624840140343,
                      0.008454523980617523,
                      0.013828613795340061,
                      -0.017135746777057648,
                      0.007401042152196169,
                      -0.036805182695388794,
                      0.007361036725342274,
                      0.004623984452337027,
                      -0.0066509367898106575,
                      -0.022496500983834267,
                      -0.6383832097053528,
                      -0.026083674281835556,
                      0.011181574314832687,
                      -0.022229798138141632,
                      0.015642203390598297,
                      0.0144286984577775,
                      0.010294782929122448,
                      0.008834577165544033,
                      -0.014375357888638973,
                      0.016895713284611702,
                      -0.0036705168895423412,
                      -0.0242567490786314,
                      -0.012355073355138302,
                      -0.017922524362802505,
                      -0.006840963382273912,
                      -0.024243414402008057,
                      0.01041479967534542,
                      -0.013495233841240406,
                      0.005487439688295126,
                      0.0028720712289214134,
                      -0.015815559774637222,
                      0.03184448555111885,
                      -0.023856693878769875,
                      -0.017122412100434303,
                      -0.011888341046869755,
                      -0.010554819367825985,
                      0.008614546619355679,
                      -0.008961262181401253,
                      -0.0003694270271807909,
                      -0.004160585813224316,
                      -0.011228247545659542,
                      0.02679044008255005,
                      0.026337042450904846,
                      -0.008227825164794922,
                      0.03952556475996971,
                      0.010241442359983921,
                      -0.007987791672348976,
                      0.011988354846835136,
                      -9.897226846078411e-05,
                      0.0432327538728714,
                      -0.004000563640147448,
                      -0.03136441856622696,
                      0.01694905385375023,
                      -0.006024181842803955,
                      0.006390899885445833,
                      0.009814715944230556,
                      -0.0008313670987263322,
                      0.020896276459097862,
                      0.00922129862010479,
                      -0.009908062405884266,
                      -0.0033804760314524174,
                      0.011508287861943245,
                      -0.0031371084041893482,
                      -6.230043800314888e-05,
                      0.016562333330512047,
                      -0.014135324396193027,
                      0.02120298519730568,
                      -0.01952274888753891,
                      -0.0004071406729053706,
                      -0.006010846700519323,
                      0.010381462052464485,
                      0.003970559220761061,
                      -0.004810677375644445,
                      -0.03733859211206436,
                      -0.005260740872472525,
                      0.007574399933218956,
                      0.0069809830747544765,
                      0.004774005617946386,
                      0.007561064790934324,
                      -0.04285936802625656,
                      0.007201014086604118,
                      0.016322297975420952,
                      0.0020636238623410463,
                      -0.011081560514867306,
                      -0.007681081537157297,
                      0.026750434190034866,
                      0.01434868760406971,
                      -0.007314363494515419,
                      -0.006454242393374443,
                      0.013001831248402596,
                      0.010374794714152813,
                      -0.01533549278974533,
                      0.010788186453282833,
                      0.027577217668294907,
                      -0.023349955677986145,
                      0.009194628335535526,
                      -0.004710663575679064,
                      -0.011328262276947498,
                      -0.00980804767459631,
                      0.003322134492918849,
                      -0.0023269944358617067,
                      0.025150207802653313,
                      -0.009768042713403702,
                      -0.042939379811286926,
                      0.020109498873353004,
                      0.004330609925091267,
                      0.004727332387119532,
                      -0.00904794130474329,
                      0.008327839896082878,
                      -0.022469831630587578,
                      -0.004547307267785072,
                      -0.019882800057530403,
                      0.027390524744987488,
                      0.015042118728160858,
                      0.00933464802801609,
                      -0.006587594281882048,
                      0.022136451676487923,
                      0.0007267690380103886,
                      -0.00228532194159925,
                      0.0009918063879013062,
                      -8.381388761335984e-05,
                      0.007681081537157297,
                      -0.017055734992027283,
                      -0.01434868760406971,
                      -0.003103770548477769,
                      -0.03584504872560501,
                      0.012601775117218494,
                      0.0008517866372130811,
                      -0.008134478703141212,
                      -0.009741371497511864,
                      0.0019202703842893243,
                      0.024656806141138077,
                      0.008567873388528824,
                      0.004580644890666008,
                      0.011261586099863052,
                      0.015562191605567932,
                      -0.009461332112550735,
                      0.012568436563014984,
                      -0.00733436644077301,
                      0.002718716161325574,
                      -0.00025170212029479444,
                      0.002108630258589983,
                      0.01781584322452545,
                      -0.010228106752038002,
                      0.02216312102973461,
                      0.02409672737121582,
                      0.0046206507831811905,
                      -0.0006038350402377546,
                      -0.020562896504998207,
                      0.0012026693439111114,
                      -0.011868338100612164,
                      -0.012488425709307194,
                      0.006294219754636288,
                      -0.014415363781154156,
                      -0.04717997834086418,
                      -0.016722355037927628,
                      0.02098962292075157,
                      0.015135465189814568,
                      0.0012943489709869027,
                      -0.01685570739209652,
                      0.008741230703890324,
                      -0.018389256671071053,
                      -0.012808470986783504,
                      -0.0009709700825624168,
                      -0.001995281083509326,
                      -0.010181433521211147,
                      -0.004353946540504694,
                      -0.05952838435769081,
                      -0.0096013518050313,
                      -0.04533971846103668,
                      0.006727613974362612,
                      0.03352472186088562,
                      0.0033721416257321835,
                      -0.00436394801363349,
                      -0.007681081537157297,
                      -0.033258017152547836,
                      -0.005140724126249552,
                      0.03653847798705101,
                      -0.015082123689353466,
                      -0.027950603514909744,
                      0.006354228127747774,
                      -0.0450730137526989,
                      0.04003230482339859,
                      0.01612227037549019,
                      -0.01740245148539543,
                      -0.0044839647598564625,
                      -0.014988777227699757,
                      0.004083908628672361,
                      -0.031604450196027756,
                      -0.015095459297299385,
                      0.0010584824485704303,
                      0.020109498873353004,
                      0.0012585106305778027,
                      -0.017189087346196175,
                      0.02555026486515999,
                      -0.0022236465010792017,
                      -0.012055031023919582,
                      -0.0006071688258089125,
                      -0.029390806332230568,
                      0.047473352402448654,
                      0.002633704338222742,
                      -0.019389396533370018,
                      0.009187960997223854,
                      0.017509132623672485,
                      0.012181716039776802,
                      -0.004353946540504694,
                      0.02888406813144684,
                      -0.0010501479264348745,
                      -0.018562614917755127,
                      0.0138552850112319,
                      -0.0036605154164135456,
                      0.019189368933439255,
                      -0.003178780898451805,
                      -0.03485824167728424,
                      -0.013308540917932987,
                      -0.008447856642305851,
                      0.010388129390776157,
                      -0.03920552134513855,
                      -0.0034504858776926994,
                      0.00650758296251297,
                      0.011181574314832687,
                      -0.01901601068675518,
                      -0.012581772170960903,
                      -0.021629713475704193,
                      0.01392196025699377,
                      0.01990947127342224,
                      -0.011021552607417107,
                      0.002422007732093334,
                      -0.0055174436420202255,
                      0.013268535025417805,
                      0.012075033970177174,
                      -0.014655397273600101,
                      0.024270085617899895,
                      -0.002778724767267704,
                      -0.005184063222259283,
                      -0.0003223370586056262,
                      0.0032287880312651396,
                      0.005104052368551493,
                      0.025950321927666664,
                      -0.03421815112233162,
                      0.0017044066917151213,
                      0.03101770207285881,
                      0.007821101695299149,
                      0.019136028364300728,
                      0.0048806872218847275,
                      -0.0024486782494932413,
                      0.015962248668074608,
                      -0.010881532914936543,
                      0.02883072756230831,
                      -0.011514955200254917,
                      0.017122412100434303,
                      0.001248509157449007,
                      -0.009361318312585354,
                      -0.011134901084005833,
                      0.02469681203365326,
                      0.0333380289375782,
                      0.03813870623707771,
                      0.03349805250763893,
                      -0.017482461407780647,
                      -0.006940977647900581,
                      -0.012628445401787758,
                      -0.013788608834147453,
                      -0.004540639463812113,
                      0.009768042713403702,
                      0.04120580479502678,
                      -0.009981405921280384,
                      0.015482180751860142,
                      0.005974174477159977,
                      0.0027420527767390013,
                      0.020309526473283768,
                      0.007141005713492632,
                      0.016735689714550972,
                      0.005267408676445484,
                      -0.005374090280383825,
                      0.032937973737716675,
                      0.0012951823882758617,
                      0.007094332482665777,
                      -0.013895289972424507,
                      -0.0007371871615760028,
                      0.00447729742154479,
                      -0.016749026253819466,
                      -0.017282433807849884,
                      -0.0014060313114896417,
                      -0.025123538449406624,
                      0.019056016579270363,
                      0.02066957764327526,
                      -0.014322017319500446,
                      0.03405813127756119,
                      0.0005142390727996826,
                      0.040112316608428955,
                      -0.012008357793092728,
                      -0.02021618001163006,
                      0.03336469829082489,
                      -0.01080818846821785,
                      -0.015002112835645676,
                      -0.013681926764547825,
                      -0.019762782379984856,
                      0.014415363781154156,
                      -0.005027374718338251,
                      -0.0027270507998764515,
                      -0.0025253556668758392,
                      -0.0008492862689308822,
                      -0.023283278569579124,
                      0.0034904915373772383,
                      0.00037067721132189035,
                      0.020816264674067497,
                      0.014282011426985264,
                      -0.018295910209417343,
                      0.004490632563829422,
                      0.011054890230298042,
                      0.003813870484009385,
                      0.012795135378837585,
                      -0.029497487470507622,
                      -0.016562333330512047,
                      0.021763065829873085,
                      -0.01116157229989767,
                      -0.03952556475996971,
                      -0.021883081644773483,
                      -0.026110343635082245,
                      -0.02713715471327305,
                      0.040085647255182266,
                      -0.011888341046869755,
                      0.022136451676487923,
                      -0.005554115399718285,
                      0.0242567490786314,
                      0.02555026486515999,
                      0.009407991543412209,
                      -0.0034704888239502907,
                      0.027630558237433434,
                      0.011928346939384937,
                      -0.005354087334126234,
                      -0.010068085044622421,
                      -0.0007050993153825402,
                      0.013168521225452423,
                      0.04304606094956398,
                      0.03256458789110184,
                      -0.00974803976714611,
                      0.010461472906172276,
                      -0.01704240031540394,
                      0.012875146232545376,
                      -0.03320467472076416,
                      -0.02536357194185257,
                      -0.0011718317400664091,
                      -0.017895853146910667,
                      -0.011321594007313251,
                      -0.007901112549006939,
                      -0.0026853783056139946,
                      -0.005240737926214933,
                      -0.016495656222105026,
                      -0.007641076110303402,
                      -0.018389256671071053,
                      -0.0395522378385067,
                      0.019856128841638565,
                      0.008367844857275486,
                      -0.006880969274789095,
                      0.021096304059028625,
                      0.030617645010352135,
                      0.027310512959957123,
                      0.014402028173208237,
                      0.008981265127658844,
                      0.02117631584405899,
                      0.02196309342980385,
                      -0.005140724126249552,
                      -0.019216040149331093,
                      0.0032204536255449057,
                      -0.016402309760451317,
                      0.00904794130474329,
                      0.014602056704461575,
                      0.00784110464155674,
                      0.020549559965729713,
                      -0.00772108742967248,
                      -0.009901394136250019,
                      0.028057284653186798,
                      0.008907921612262726,
                      0.011394938454031944,
                      0.01216171309351921,
                      0.007207681890577078,
                      -0.014148659072816372,
                      0.010581490583717823,
                      -0.03504493460059166,
                      -0.00036525976611301303,
                      0.031791143119335175,
                      -0.006360895931720734,
                      0.007661079056560993,
                      -0.009247968904674053,
                      0.01084152702242136,
                      -0.027630558237433434,
                      -0.006020847707986832,
                      0.029630839824676514,
                      -0.0015085458289831877,
                      -0.018362585455179214,
                      -0.0003198367194272578,
                      -0.0021619710605591536,
                      -0.01740245148539543,
                      -0.012695121578872204,
                      -0.016375640407204628,
                      0.004904023837298155,
                      -0.002453678986057639,
                      0.002502019051462412,
                      -0.0025136873591691256,
                      -0.03085767850279808,
                      -0.027470534667372704,
                      -0.028777386993169785,
                      0.019642766565084457,
                      -0.001286847866140306,
                      -0.024176737293601036,
                      -0.049313612282276154,
                      -0.014695403166115284,
                      0.022069774568080902,
                      0.02336329035460949,
                      0.017029065638780594,
                      -0.017415786162018776,
                      -0.02663041651248932,
                      -0.00934798363596201,
                      0.007987791672348976,
                      -0.021736394613981247,
                      0.004193923901766539,
                      -0.02400338090956211,
                      -0.013008498586714268,
                      -0.011534958146512508,
                      -0.028937408700585365,
                      -0.004750669002532959,
                      -0.03299131244421005,
                      0.013828613795340061,
                      -0.009294642135500908,
                      0.01660233736038208,
                      0.018709301948547363,
                      -0.019416067749261856,
                      0.017189087346196175,
                      0.01101488433778286,
                      0.008341174572706223,
                      0.011261586099863052,
                      0.02142968401312828,
                      -0.030937690287828445,
                      0.018055876716971397,
                      -0.010294782929122448,
                      -0.023549983277916908,
                      -0.017122412100434303,
                      0.006437573116272688,
                      0.009481335058808327,
                      0.00430060550570488,
                      -0.02920411340892315,
                      0.011948348954319954,
                      -0.014735408127307892,
                      0.02057623118162155,
                      -0.02467014081776142,
                      -7.1364214818459e-05,
                      0.005497440695762634,
                      0.03685852512717247,
                      0.016482321545481682,
                      0.016895713284611702,
                      0.022803211584687233,
                      0.0029120768886059523,
                      -0.00465065473690629,
                      -0.008261163718998432,
                      -0.02492351084947586,
                      0.018562614917755127,
                      0.0194294024258852,
                      -0.010268112644553185,
                      0.009821383282542229,
                      0.032191202044487,
                      -0.005837488919496536,
                      -0.035951729863882065,
                      0.02536357194185257,
                      -0.04083241894841194,
                      0.018682630732655525,
                      -0.0010476475581526756,
                      -0.017642484977841377,
                      -0.03285796195268631,
                      -0.02396337501704693,
                      0.0066842748783528805,
                      0.0009509672527201474,
                      -0.04283269867300987,
                      0.008201154880225658,
                      0.0008651218377053738,
                      0.0027320515364408493,
                      0.0012268394930288196,
                      -0.017975864931941032,
                      0.023629995062947273,
                      -0.02835065871477127,
                      -0.018842654302716255,
                      0.004237263463437557,
                      0.014508710242807865,
                      0.015122129581868649,
                      -0.01768249087035656,
                      0.013508569449186325,
                      -0.033258017152547836,
                      -0.013895289972424507,
                      0.009001268073916435,
                      -0.018109217286109924,
                      0.004373949486762285,
                      0.018055876716971397,
                      0.007707752287387848,
                      0.01534882839769125,
                      0.03581837937235832,
                      0.0036538478452712297,
                      0.01958942599594593,
                      0.021763065829873085,
                      -0.004887355025857687,
                      0.009214631281793118,
                      -0.023856693878769875,
                      0.009614687412977219,
                      -0.0017619148129597306,
                      0.011034887284040451,
                      0.00962135475128889,
                      -0.01570887863636017,
                      -0.01452204491943121,
                      -0.0052207354456186295,
                      0.019802788272500038,
                      0.010608160868287086,
                      -0.007567732594907284,
                      -0.018042540177702904,
                      -0.02867070399224758,
                      -0.023403296247124672,
                      -0.024776821956038475,
                      0.0019369394285604358,
                      -0.003133774735033512,
                      -0.005497440695762634,
                      -0.05158059671521187,
                      -0.01812255196273327,
                      0.0008013628539629281,
                      0.0031587781850248575,
                      0.008267831057310104,
                      0.0008297002059407532,
                      0.020389538258314133,
                      0.0036538478452712297,
                      -0.007654411252588034,
                      0.009061275981366634,
                      0.0030070901848375797,
                      -0.014468704350292683,
                      -0.007314363494515419,
                      -0.02752387709915638,
                      -0.011081560514867306,
                      0.03640512749552727,
                      0.021149644628167152,
                      -0.00500737177208066,
                      0.00585749140009284,
                      -0.010301451198756695,
                      -0.001103488728404045,
                      0.013228530064225197,
                      0.0029070761520415545,
                      -0.013155185617506504,
                      -0.025243554264307022,
                      -0.014708737842738628,
                      -0.021389678120613098,
                      -0.030804337933659554,
                      -0.012588439509272575,
                      0.02904408983886242,
                      0.013721932657063007,
                      -0.008114475756883621,
                      -0.029124101623892784,
                      0.032964643090963364,
                      -0.0002389919973211363,
                      -0.0033037986140698195,
                      0.004023900255560875,
                      0.024883504956960678,
                      0.032937973737716675,
                      -0.001675235922448337,
                      0.011381602846086025,
                      0.016242288053035736,
                      0.0022786541376262903,
                      -0.006347560789436102,
                      0.0021619710605591536,
                      0.004997370298951864,
                      -0.03035094030201435,
                      -0.000267537689069286,
                      -0.009788044728338718,
                      -0.0005438266089186072,
                      -0.010421467944979668,
                      0.016909047961235046,
                      0.01042813528329134,
                      -0.03117772378027439,
                      -0.02053622528910637,
                      0.014322017319500446,
                      -0.007281025405973196,
                      0.023923369124531746,
                      -0.002218645764514804,
                      -0.019322721287608147,
                      -0.001751913339830935,
                      0.013868619687855244,
                      -0.01653566211462021,
                      0.02869737520813942,
                      0.002815396524965763,
                      0.00013929045235272497,
                      0.019096022471785545,
                      0.02708381414413452,
                      -0.010001408867537975,
                      0.022816546261310577,
                      -0.01444203406572342,
                      0.008687890134751797,
                      0.00265037314966321,
                      0.006967647932469845,
                      -0.020362867042422295,
                      -0.011654974892735481,
                      0.006797623820602894,
                      0.020749589428305626,
                      -0.012535098940134048,
                      0.017122412100434303,
                      0.007974456064403057,
                      -0.011614969000220299,
                      0.00662760017439723,
                      0.009134619496762753,
                      -0.0014418697683140635,
                      0.03083100914955139,
                      -0.02787059172987938,
                      -0.005124054849147797,
                      -0.0007338533760048449,
                      0.004497299902141094,
                      0.017375780269503593,
                      -0.013788608834147453,
                      0.001751913339830935,
                      0.0007451049168594182,
                      -0.017029065638780594,
                      0.01570887863636017,
                      0.025563599541783333,
                      0.024150067940354347,
                      -0.023763347417116165,
                      0.0072743576020002365,
                      0.005717471707612276,
                      -0.0014535380760207772,
                      -0.002433676039800048,
                      0.006204206962138414,
                      0.01993614062666893,
                      -0.03555167466402054,
                      -0.013788608834147453,
                      0.034111469984054565,
                      0.014282011426985264,
                      0.020149504765868187,
                      -0.00895459484308958,
                      -0.011481616646051407,
                      -0.014455368742346764,
                      0.02984420396387577,
                      -0.04373949393630028,
                      0.007947785779833794,
                      0.012395079247653484,
                      -0.013681926764547825,
                      -0.013208527117967606,
                      -0.018269238993525505,
                      -0.015575527213513851,
                      -0.01293515507131815,
                      0.0007967788842506707,
                      -0.0015035450924187899,
                      -0.022229798138141632,
                      -0.0019869464449584484,
                      -0.005197398830205202,
                      -0.005670798476785421,
                      0.007994459010660648,
                      -0.022136451676487923,
                      -0.020896276459097862,
                      0.018936000764369965,
                      0.0206962488591671,
                      -0.019376061856746674,
                      -0.01366859208792448,
                      -0.012435084208846092,
                      0.0029220781289041042,
                      -0.00328212883323431,
                      0.01271512359380722,
                      -0.027977272868156433,
                      -0.01452204491943121,
                      0.0016694017685949802,
                      0.01758914440870285,
                      -0.011928346939384937,
                      -0.04101911187171936,
                      -0.0011543292785063386,
                      -0.006974315736442804,
                      0.009461332112550735,
                      -0.0018819316755980253,
                      -0.015028783120214939,
                      0.022083109244704247,
                      -0.01878931373357773,
                      -0.008787903934717178,
                      0.02591031603515148,
                      -0.00884124543517828,
                      -0.010481475852429867,
                      0.003190449206158519,
                      -0.00792111549526453,
                      0.013341878540813923,
                      0.02479015849530697,
                      -0.0097147012129426,
                      -0.034964922815561295,
                      -0.011754988692700863,
                      -0.003883880330249667,
                      -0.017882518470287323,
                      -0.008487861603498459,
                      -0.015548856928944588,
                      0.051633939146995544,
                      0.031444426625967026,
                      -0.010228106752038002,
                      -0.010674837045371532,
                      0.0024153401609510183,
                      -0.025830304250121117,
                      0.003423815593123436,
                      -0.005870827008038759,
                      0.0070809973403811455,
                      0.0010659834370017052,
                      -0.021229656413197517,
                      -0.0023453303147107363,
                      0.017735831439495087,
                      -0.0004996536881662905,
                      -0.004964032210409641,
                      -0.008261163718998432,
                      -0.0018719303188845515,
                      -0.021002957597374916,
                      -0.01841592602431774,
                      0.00025232721236534417,
                      -0.012681785970926285,
                      -0.03933887183666229,
                      0.014708737842738628,
                      0.028617363423109055,
                      -0.006250880192965269,
                      0.0172424279153347,
                      0.04387284442782402,
                      -0.00871456041932106,
                      0.001788585213944316,
                      -0.00974803976714611,
                      0.016655679792165756,
                      0.02186974696815014,
                      0.017389114946126938,
                      0.011168239638209343,
                      -0.026270367205142975,
                      0.01154162548482418,
                      0.04171254113316536,
                      -0.0037538621108978987,
                      -0.02050955593585968,
                      0.002850401448085904,
                      0.0047906748950481415,
                      0.00784110464155674,
                      -0.012101704254746437,
                      -0.021856412291526794,
                      -0.0020186176989227533,
                      0.0019469409016892314,
                      -0.019856128841638565,
                      0.005077381618320942,
                      0.01781584322452545,
                      -0.02888406813144684,
                      0.0073076956905424595,
                      0.0028604029212146997,
                      0.00657092547044158,
                      -0.03157778084278107,
                      0.017722496762871742,
                      -0.010734844952821732,
                      -0.0012285063276067376,
                      0.01917603425681591,
                      -0.0012593440478667617,
                      0.0016493989387527108,
                      -0.016655679792165756,
                      0.005594121292233467,
                      0.009928064420819283,
                      -0.006174203008413315,
                      -0.03301798179745674,
                      0.001644398202188313,
                      -0.023216603323817253,
                      0.008587876334786415,
                      0.007734422571957111,
                      -0.044966332614421844,
                      -0.006710945162922144,
                      -0.0020486218854784966,
                      -0.019882800057530403,
                      -0.017255762591958046,
                      -0.004980701487511396,
                      0.000838034669868648,
                      -0.005024041049182415,
                      0.01698905974626541,
                      -0.0060508521273732185,
                      -0.016709020361304283,
                      -0.01949607953429222,
                      0.005604122765362263,
                      0.02142968401312828,
                      0.00415058434009552,
                      0.02695046178996563,
                      0.228085458278656,
                      -0.029284123331308365,
                      -0.00036505141179077327,
                      0.00515739293769002,
                      0.003147109877318144,
                      0.019802788272500038,
                      -0.0013276869431138039,
                      -0.0026303704362362623,
                      -0.009774710051715374,
                      -0.008867915719747543,
                      0.04149917885661125,
                      0.006914306897670031,
                      -0.01312851533293724,
                      -0.0017452457686886191,
                      0.018242569640278816,
                      -0.02336329035460949,
                      -0.014148659072816372,
                      -0.03421815112233162,
                      -0.012601775117218494,
                      0.01242841687053442,
                      0.005134056322276592,
                      0.01216171309351921,
                      0.002530356403440237,
                      -0.0008076137164607644,
                      0.028937408700585365,
                      0.004067239351570606,
                      0.0032337887678295374,
                      -0.0024486782494932413,
                      0.021149644628167152,
                      0.004924026783555746,
                      -0.006197539623826742,
                      -0.005454101599752903,
                      0.011701648123562336,
                      0.02085627056658268,
                      -0.010248109698295593,
                      -0.0009134619613178074,
                      -0.029977554455399513,
                      0.003218786558136344,
                      0.018749307841062546,
                      -0.017122412100434303,
                      0.027657227590680122,
                      -0.007541061844676733,
                      0.010268112644553185,
                      -0.04133915528655052,
                      0.008041132241487503,
                      -0.0009776377119123936,
                      -0.004910691641271114,
                      -0.006454242393374443,
                      -0.004447293002158403,
                      0.02202976867556572,
                      -0.008947926573455334,
                      0.02545691840350628,
                      0.02098962292075157,
                      0.005194064695388079,
                      0.008361177518963814,
                      0.00035546673461794853,
                      -0.009854720905423164,
                      0.009041273035109043,
                      0.027977272868156433,
                      0.005440766457468271,
                      -0.008961262181401253,
                      -0.0001698155829217285,
                      -0.0010168098378926516,
                      0.02301657572388649,
                      -0.01312851533293724,
                      0.022109780460596085,
                      -0.02612367831170559,
                      0.0231099221855402,
                      0.0356316864490509,
                      0.021509695798158646,
                      0.007054326590150595,
                      -0.023790016770362854,
                      -0.007741090375930071,
                      0.014895430766046047,
                      -0.04389951750636101,
                      -0.03424482420086861,
                      0.051313892006874084,
                      0.012488425709307194,
                      0.039472226053476334,
                      0.03003089688718319,
                      0.009701366536319256,
                      0.001888599363155663,
                      -0.018175892531871796,
                      -0.015495515428483486,
                      -0.0010159764206036925,
                      -0.04376616328954697,
                      0.029790861532092094,
                      -0.023056579753756523,
                      0.001105155679397285,
                      -0.024736817926168442,
                      0.003963891416788101,
                      -0.008494529873132706,
                      -0.01133492961525917,
                      -0.007301028352230787,
                      0.008241160772740841,
                      0.013481899164617062,
                      0.017162416130304337,
                      0.012588439509272575,
                      -0.010454805567860603,
                      -0.009167958050966263,
                      -0.02545691840350628,
                      0.06902305036783218,
                      -0.0025470254477113485,
                      0.00812781136482954,
                      0.002713715424761176,
                      0.01816255785524845,
                      -0.013681926764547825,
                      0.014055312611162663,
                      0.008461191318929195,
                      -0.011741654016077518,
                      0.016802366822957993,
                      -0.03139108791947365,
                      0.008001127280294895,
                      -0.0035538338124752045,
                      0.011814997531473637,
                      0.012115039862692356,
                      0.029577499255537987,
                      0.003673850791528821,
                      0.018269238993525505,
                      -0.0144286984577775,
                      0.01733577437698841,
                      -0.022403154522180557,
                      0.009834717959165573,
                      0.015162135474383831,
                      0.005960839334875345,
                      -0.004463961813598871,
                      -0.018215898424386978,
                      0.010241442359983921,
                      -0.03221787139773369,
                      -0.0032254543621093035,
                      -0.014868760481476784,
                      -0.02552359364926815,
                      0.022109780460596085,
                      -0.02835065871477127,
                      -0.02536357194185257,
                      -0.003680518362671137,
                      -0.001108489464968443,
                      -0.024963514879345894,
                      -0.02416340261697769,
                      0.02170972339808941,
                      0.0020536226220428944,
                      0.008201154880225658,
                      0.016842372715473175,
                      -7.469802221748978e-05,
                      -0.020136170089244843,
                      0.002567028161138296,
                      -0.008741230703890324,
                      -0.0031604450196027756,
                      0.00635756179690361,
                      -0.02727050706744194,
                      -0.008581208065152168,
                      -0.017882518470287323,
                      -0.011448279023170471,
                      -0.0006538420566357672,
                      0.02216312102973461,
                      0.010534817352890968,
                      -0.006407569162547588,
                      -0.008154481649398804,
                      0.00683429604396224,
                      0.005110719706863165,
                      -0.02209644578397274,
                      -0.021896416321396828,
                      0.0017252429388463497,
                      -0.0078077660873532295,
                      -0.009794712997972965,
                      -0.0016910715494304895,
                      -0.16845038533210754,
                      -0.0010668168542906642,
                      0.025443583726882935,
                      -0.046779923141002655,
                      0.011995022185146809,
                      0.0024936844129115343,
                      0.02479015849530697,
                      -0.030110906809568405,
                      -0.05054045096039772,
                      0.014602056704461575,
                      -0.00010371252574259415,
                      0.0020119501277804375,
                      0.0016485655214637518,
                      -0.009794712997972965,
                      -0.03248457610607147,
                      0.006497581955045462,
                      -0.0242567490786314,
                      0.01818922907114029,
                      0.014588721096515656,
                      -0.0027403859421610832,
                      0.0377119779586792,
                      -0.03800535202026367,
                      0.026297036558389664,
                      -0.011694980785250664,
                      0.03547166287899017,
                      -0.0014026975259184837,
                      0.013775273226201534,
                      0.019189368933439255,
                      -0.01246175542473793,
                      -0.04491299018263817,
                      -0.0012218387564644217,
                      -0.006454242393374443,
                      0.03488491475582123,
                      -0.022043105214834213,
                      -0.009341315366327763,
                      0.015148799866437912,
                      -0.010774850845336914,
                      -0.0057208058424293995,
                      -0.00950800534337759,
                      0.006914306897670031,
                      0.029897544533014297,
                      0.0073410337790846825,
                      0.0043272762559354305,
                      -0.008227825164794922,
                      -0.02479015849530697,
                      0.0020919612143188715,
                      0.029124101623892784,
                      -0.012181716039776802,
                      0.004500634036958218,
                      -0.011054890230298042,
                      0.008987932465970516,
                      -0.0006059186416678131,
                      -0.002218645764514804,
                      0.011148236691951752,
                      0.0030404282733798027,
                      0.003563835285604,
                      0.028430670499801636,
                      0.018469268456101418,
                      -0.011301591992378235,
                      -0.017189087346196175,
                      -0.009121284820139408,
                      -0.007814433425664902,
                      0.0042939381673932076,
                      0.009814715944230556,
                      -0.015922242775559425,
                      -0.011654974892735481,
                      -0.009501338005065918,
                      -0.012868478894233704,
                      -0.007901112549006939,
                      0.021002957597374916,
                      -0.031444426625967026,
                      -0.011895008385181427,
                      -0.01071484200656414,
                      -0.0052774096839129925,
                      0.005614123772829771,
                      0.017855847254395485,
                      -0.017802506685256958,
                      -0.022043105214834213,
                      0.00307876686565578,
                      -0.002470347797498107,
                      -0.02237648516893387,
                      0.021536367014050484,
                      -0.0094279944896698,
                      0.0033371367026120424,
                      0.005594121292233467,
                      0.016775695607066154,
                      -0.0004917359328828752,
                      0.02492351084947586,
                      -0.019816124811768532,
                      0.0009793045464903116,
                      0.039125509560108185,
                      -0.012721791863441467,
                      0.0025470254477113485,
                      -0.0034138141199946404,
                      -0.010041413828730583,
                      0.011214912869036198,
                      0.0067742872051894665,
                      0.0020119501277804375,
                      0.004130581859499216,
                      -0.018269238993525505,
                      0.010754847899079323,
                      -0.0027687232941389084,
                      -0.015242146328091621,
                      0.020136170089244843,
                      0.03507160767912865,
                      0.016175610944628716,
                      -0.015562191605567932,
                      0.003032093634828925,
                      0.04336610808968544,
                      0.00842785369604826,
                      -0.01685570739209652,
                      -0.004907357972115278,
                      0.0009042940218932927,
                      -0.0053107477724552155,
                      -0.004023900255560875,
                      0.03003089688718319,
                      -0.0006146698724478483,
                      -0.017469126731157303,
                      -0.0029120768886059523,
                      -0.019322721287608147,
                      0.048806872218847275,
                      -0.028404001146554947,
                      -0.03237789496779442,
                      0.029257453978061676,
                      -0.011568295769393444,
                      -0.0314977690577507,
                      -0.11649640649557114,
                      -0.025030191987752914,
                      -0.011788327246904373,
                      0.01285514421761036,
                      -0.004233929794281721,
                      0.009074611589312553,
                      0.015922242775559425,
                      0.024203408509492874,
                      -0.03248457610607147,
                      0.04189923405647278,
                      -0.00016929468256421387,
                      -0.01910935714840889,
                      0.01704240031540394,
                      -0.015175470151007175,
                      -0.0073076956905424595,
                      0.001351857092231512,
                      0.012901817448437214,
                      0.010488144122064114,
                      -0.01071484200656414,
                      0.028083955869078636,
                      0.012068366631865501,
                      -0.031071042641997337,
                      0.021483026444911957,
                      -0.016895713284611702,
                      -0.004440625198185444,
                      0.0018135887803509831,
                      -0.017882518470287323,
                      0.008594543673098087,
                      -0.0010759849101305008,
                      -0.022496500983834267,
                      0.016322297975420952,
                      -0.036778513342142105,
                      0.0071143354289233685,
                      -0.012515095993876457,
                      0.01876264251768589,
                      -0.01033478882163763,
                      0.003907216712832451,
                      -0.021483026444911957,
                      -0.016802366822957993,
                      -0.006744283251464367,
                      -0.011421608738601208,
                      0.005090716760605574,
                      0.0011609968496486545,
                      0.0032421231735497713,
                      -0.01736244559288025,
                      -0.0172424279153347,
                      -0.022109780460596085,
                      0.027030473574995995,
                      0.0031721133273094893,
                      -0.03235122188925743,
                      -0.028270648792386055,
                      -0.023643329739570618,
                      -0.05024707689881325,
                      -0.0038005353417247534,
                      0.0024736816994845867,
                      -0.014055312611162663,
                      -0.001976944971829653,
                      0.0036371788010001183,
                      -0.01090153492987156,
                      -0.005967507138848305,
                      0.003607174614444375,
                      -0.009701366536319256,
                      -0.011088227853178978,
                      0.029790861532092094,
                      0.01476207934319973,
                      -0.005707470700144768,
                      -0.010001408867537975,
                      -0.03240456432104111,
                      -0.006384232547134161,
                      -0.026110343635082245,
                      -0.0015960581367835402,
                      0.00963469035923481,
                      0.0008342841756530106,
                      0.007801098749041557,
                      -0.03565835580229759,
                      -0.0012410080526024103,
                      -0.018709301948547363,
                      -0.01910935714840889,
                      -0.0038038690108805895,
                      0.010368126444518566,
                      -0.026577075943350792,
                      -0.003317133756354451,
                      0.008401183411478996,
                      -0.024803493171930313,
                      -0.013721932657063007,
                      -0.0030054233502596617,
                      -0.006550922524183989,
                      -0.005297412630170584,
                      -0.002998755546286702,
                      -0.02708381414413452,
                      0.0029920879751443863,
                      0.02933746576309204,
                      -0.011468281969428062,
                      0.000514655839651823,
                      -0.00992139708250761,
                      0.021096304059028625,
                      -0.0028037282172590494,
                      -0.023816687986254692,
                      0.0264437235891819,
                      0.02308325096964836,
                      -0.017949193716049194,
                      -0.005944170523434877,
                      -0.02301657572388649,
                      0.0164423156529665,
                      -0.009501338005065918,
                      -0.008721228688955307,
                      0.002960416954010725,
                      -0.011468281969428062,
                      0.04200591519474983,
                      0.016709020361304283,
                      -0.022563178092241287,
                      0.008581208065152168,
                      -0.009968070313334465,
                      0.021136309951543808,
                      0.0014035309432074428,
                      -0.027217166498303413,
                      -0.014922101981937885,
                      -0.022403154522180557,
                      0.014735408127307892,
                      -0.022936563938856125,
                      0.022149786353111267,
                      0.01940273307263851,
                      -0.005264074541628361,
                      -0.014868760481476784,
                      0.021029628813266754,
                      0.0015785556752234697,
                      0.007894445210695267,
                      -0.002181974006816745,
                      -0.00782776903361082,
                      0.021056298166513443,
                      -0.010708174668252468,
                      -0.022176455706357956,
                      0.031071042641997337,
                      -0.01892266422510147,
                      0.005000704433768988,
                      0.014255341142416,
                      -0.002030286006629467,
                      0.015988918021321297,
                      0.001396863372065127,
                      -0.004817345179617405,
                      0.022523172199726105,
                      -0.006697610020637512,
                      -0.005140724126249552,
                      -0.02260318398475647,
                      0.029070761054754257,
                      -0.01850927248597145,
                      0.013388551771640778,
                      -0.007467718329280615,
                      -0.022456495091319084,
                      -0.0038738788571208715,
                      0.01860261894762516,
                      -0.018709301948547363,
                      0.01704240031540394,
                      0.0144286984577775,
                      0.01222172100096941,
                      -0.031444426625967026,
                      -0.0016002253396436572,
                      -0.004183922428637743,
                      -0.016682349145412445,
                      0.0001160580141004175,
                      0.01542883925139904,
                      0.003500493010506034,
                      0.02701713889837265,
                      0.004593980498611927,
                      -0.009021270088851452,
                      -0.010654834099113941,
                      0.004697328433394432,
                      0.013208527117967606,
                      0.003597173374146223,
                      0.013828613795340061,
                      -0.003433816833421588,
                      -0.012121707201004028,
                      7.297902629943565e-05,
                      0.011188242584466934,
                      0.0011493285419419408,
                      0.0016310630599036813,
                      0.0344848558306694,
                      0.0188559889793396,
                      0.020522890612483025,
                      -0.0037671972531825304,
                      -0.00636422960087657,
                      0.0476067028939724,
                      0.014575386419892311,
                      -0.019696107134222984,
                      -0.016455650329589844,
                      0.009374653920531273,
                      0.01374860294163227,
                      0.00465732254087925,
                      0.006860966328531504,
                      -0.02559027075767517,
                      -0.011908343993127346,
                      0.005557449534535408,
                      0.007961121387779713,
                      0.008087805472314358,
                      -0.01366859208792448,
                      0.03723191097378731,
                      0.0037638633511960506,
                      0.01720242202281952,
                      0.0061842044815421104,
                      0.030137578025460243,
                      0.02304324507713318,
                      0.006227543577551842,
                      0.00256369449198246,
                      -0.003430483164265752,
                      -0.00278539233841002,
                      0.010514814406633377,
                      -0.004057237878441811,
                      0.027630558237433434,
                      -0.011401605792343616,
                      -0.01742912083864212,
                      0.019376061856746674,
                      0.027230501174926758,
                      0.0093146450817585,
                      -0.008567873388528824,
                      -0.01745579205453396,
                      -0.007207681890577078,
                      -0.017309105023741722,
                      0.01660233736038208,
                      -0.007327698636800051,
                      -0.024870168417692184,
                      -0.02463013492524624,
                      0.005114053376019001,
                      0.011975020170211792,
                      -0.015562191605567932,
                      0.049126919358968735,
                      -0.013468563556671143,
                      0.019829459488391876,
                      -0.009454664774239063,
                      0.025656946003437042,
                      -0.020722918212413788,
                      0.013681926764547825,
                      0.02987087331712246,
                      0.01765581965446472,
                      -0.006704277358949184,
                      0.010174766182899475,
                      0.001715241582132876,
                      -0.007134337909519672,
                      -0.011434943415224552,
                      0.02168305404484272,
                      0.02603033185005188,
                      -0.019949475303292274,
                      0.07227684557437897,
                      0.024483447894454002,
                      0.007394374813884497,
                      -0.0018469267524778843,
                      -0.02349664270877838,
                      0.022109780460596085,
                      0.005240737926214933,
                      0.015988918021321297,
                      -0.023643329739570618,
                      0.0017569140763953328,
                      0.004317274782806635,
                      0.0042572664096951485,
                      0.0002967084583360702,
                      -0.02361665852367878,
                      0.0012376742670312524,
                      -0.0051207211799919605,
                      0.013175188563764095,
                      0.003943888936191797,
                      0.003112104954198003,
                      0.00040609887219034135,
                      0.03669850155711174,
                      -0.009928064420819283,
                      0.02244316041469574,
                      0.0023703337647020817,
                      -0.023643329739570618,
                      0.004730666056275368,
                      0.009594684466719627,
                      0.03152443841099739,
                      -0.0029920879751443863,
                      -0.03699187561869621,
                      0.016869042068719864,
                      0.013301873579621315,
                      -0.029470816254615784,
                      -0.0031887823715806007,
                      0.002603699918836355,
                      -0.011301591992378235,
                      -0.012708456255495548,
                      -0.011701648123562336,
                      0.0018019204726442695,
                      0.012915152125060558,
                      0.020749589428305626,
                      0.03573836758732796,
                      -0.021069634705781937,
                      -0.023723341524600983,
                      -0.0015610532136633992,
                      0.002572028897702694,
                      -0.01204836368560791,
                      -0.02200309932231903,
                      -0.006360895931720734
                    ]
                  }
                ],
                "model": "ada",
                "usage": {
                  "prompt_tokens": 16,
                  "total_tokens": 16
                }
              },
              "start_time": 1697137314.957793,
              "end_time": 1697137315.754634,
              "error": null,
              "children": null,
              "node_name": null
            }
          ],
          "node_name": "embed_the_question"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "",
        "stderr": ""
      },
      "system_metrics": {
        "prompt_tokens": 16,
        "total_tokens": 16,
        "duration": 0.839177
      },
      "result": [
        -0.016869042068719864,
        -0.0003010840737260878,
        0.022069774568080902,
        -0.01818922907114029,
        -0.0022569845896214247,
        0.013475230894982815,
        -0.0065809269435703754,
        0.004023900255560875,
        -0.032884631305933,
        -0.019216040149331093,
        0.02651040069758892,
        0.030404282733798027,
        -0.024683475494384766,
        0.0007167676230892539,
        -0.018149223178625107,
        -0.015762219205498695,
        0.036565151065588,
        0.028804056346416473,
        0.005427430849522352,
        -0.025256890803575516,
        -0.011601634323596954,
        -0.009081278927624226,
        -0.024976851418614388,
        -0.011588298715651035,
        -0.02108296938240528,
        -0.0024053386878222227,
        0.045446399599313736,
        -0.020642906427383423,
        -0.01844259724020958,
        0.0037738648243248463,
        0.026403717696666718,
        0.02196309342980385,
        -0.016562333330512047,
        -0.02032286301255226,
        -0.011454946361482143,
        0.009701366536319256,
        -0.0029054090846329927,
        -0.01225505955517292,
        0.024843499064445496,
        -0.01254176627844572,
        0.01114156935364008,
        0.032751280814409256,
        0.011975020170211792,
        -0.015215476043522358,
        -0.0032737944275140762,
        0.0264437235891819,
        0.022069774568080902,
        -0.00033192173577845097,
        -0.03133774548768997,
        0.020056158304214478,
        0.002888740273192525,
        0.030564304441213608,
        -0.016975723206996918,
        0.003323801327496767,
        -0.05198065564036369,
        -0.007454383186995983,
        -0.006334225181490183,
        0.005664131138473749,
        -0.0017552472418174148,
        -0.025723623111844063,
        0.002136967610567808,
        0.006624266039580107,
        -0.00904794130474329,
        0.01246175542473793,
        -0.012735126540064812,
        -0.013095177710056305,
        -0.005047377664595842,
        0.022816546261310577,
        0.010688171721994877,
        -0.0024503450840711594,
        0.012088368646800518,
        0.02571028843522072,
        -0.01602892391383648,
        -0.006470911204814911,
        0.027577217668294907,
        -0.014828754588961601,
        -0.0028720712289214134,
        0.021576372906565666,
        0.01041479967534542,
        -0.0031987838447093964,
        -0.0014477039221674204,
        0.006234211381524801,
        -0.01090153492987156,
        0.014268675819039345,
        0.02060290239751339,
        0.014735408127307892,
        0.0026270365342497826,
        0.0010318119311705232,
        0.003170446492731571,
        0.015802225098013878,
        0.005537446588277817,
        0.018749307841062546,
        0.03416481241583824,
        0.02221646159887314,
        0.011074893176555634,
        -0.0029904211405664682,
        -0.005217401310801506,
        0.014562050811946392,
        -0.00028566524269990623,
        0.004353946540504694,
        -0.009027938358485699,
        0.009401324205100536,
        -0.029817532747983932,
        -0.011661642231047153,
        -0.004167253617197275,
        0.009067943319678307,
        0.018629290163517,
        -0.005494107026606798,
        -0.0012393412180244923,
        -0.0036205099895596504,
        -0.03037761151790619,
        0.014482039958238602,
        -0.014588721096515656,
        -0.014175329357385635,
        -0.0054140957072377205,
        0.0022019767202436924,
        0.0005113219958730042,
        -0.010294782929122448,
        -0.01401530671864748,
        0.004560642410069704,
        0.023629995062947273,
        -1.4324934454634786e-05,
        0.018815983086824417,
        -0.017189087346196175,
        0.01698905974626541,
        -0.019322721287608147,
        -0.015028783120214939,
        -0.017975864931941032,
        -0.007247687317430973,
        -0.009167958050966263,
        0.02002948708832264,
        0.021403014659881592,
        0.00963469035923481,
        -0.02092294581234455,
        -0.007207681890577078,
        0.005680799949914217,
        0.01781584322452545,
        -0.016815701499581337,
        -0.014482039958238602,
        -0.019189368933439255,
        0.023830022662878036,
        0.016175610944628716,
        -0.02591031603515148,
        -0.03267126902937889,
        -0.05371423065662384,
        0.02012283354997635,
        -0.000785110576543957,
        0.030564304441213608,
        0.008974596858024597,
        -0.022456495091319084,
        0.012995163910090923,
        -0.029284123331308365,
        -0.012835141271352768,
        0.03224454075098038,
        0.002288655610755086,
        0.0055507817305624485,
        0.0018986007198691368,
        0.006704277358949184,
        -0.014161994680762291,
        -0.0231099221855402,
        0.01802920550107956,
        0.017975864931941032,
        0.023736676201224327,
        -0.006900971755385399,
        0.017149081453680992,
        0.021949758753180504,
        0.0017919189995154738,
        -0.005604122765362263,
        0.017375780269503593,
        -0.001429367926903069,
        0.005760811269283295,
        0.030110906809568405,
        -0.014455368742346764,
        0.007487721275538206,
        -0.0011893342016264796,
        0.01284180860966444,
        0.03205784782767296,
        -0.020056158304214478,
        -0.004427290055900812,
        -0.03920552134513855,
        0.023029910400509834,
        0.010188101790845394,
        0.03371141478419304,
        -0.013248532079160213,
        -0.018642624840140343,
        0.008454523980617523,
        0.013828613795340061,
        -0.017135746777057648,
        0.007401042152196169,
        -0.036805182695388794,
        0.007361036725342274,
        0.004623984452337027,
        -0.0066509367898106575,
        -0.022496500983834267,
        -0.6383832097053528,
        -0.026083674281835556,
        0.011181574314832687,
        -0.022229798138141632,
        0.015642203390598297,
        0.0144286984577775,
        0.010294782929122448,
        0.008834577165544033,
        -0.014375357888638973,
        0.016895713284611702,
        -0.0036705168895423412,
        -0.0242567490786314,
        -0.012355073355138302,
        -0.017922524362802505,
        -0.006840963382273912,
        -0.024243414402008057,
        0.01041479967534542,
        -0.013495233841240406,
        0.005487439688295126,
        0.0028720712289214134,
        -0.015815559774637222,
        0.03184448555111885,
        -0.023856693878769875,
        -0.017122412100434303,
        -0.011888341046869755,
        -0.010554819367825985,
        0.008614546619355679,
        -0.008961262181401253,
        -0.0003694270271807909,
        -0.004160585813224316,
        -0.011228247545659542,
        0.02679044008255005,
        0.026337042450904846,
        -0.008227825164794922,
        0.03952556475996971,
        0.010241442359983921,
        -0.007987791672348976,
        0.011988354846835136,
        -9.897226846078411e-05,
        0.0432327538728714,
        -0.004000563640147448,
        -0.03136441856622696,
        0.01694905385375023,
        -0.006024181842803955,
        0.006390899885445833,
        0.009814715944230556,
        -0.0008313670987263322,
        0.020896276459097862,
        0.00922129862010479,
        -0.009908062405884266,
        -0.0033804760314524174,
        0.011508287861943245,
        -0.0031371084041893482,
        -6.230043800314888e-05,
        0.016562333330512047,
        -0.014135324396193027,
        0.02120298519730568,
        -0.01952274888753891,
        -0.0004071406729053706,
        -0.006010846700519323,
        0.010381462052464485,
        0.003970559220761061,
        -0.004810677375644445,
        -0.03733859211206436,
        -0.005260740872472525,
        0.007574399933218956,
        0.0069809830747544765,
        0.004774005617946386,
        0.007561064790934324,
        -0.04285936802625656,
        0.007201014086604118,
        0.016322297975420952,
        0.0020636238623410463,
        -0.011081560514867306,
        -0.007681081537157297,
        0.026750434190034866,
        0.01434868760406971,
        -0.007314363494515419,
        -0.006454242393374443,
        0.013001831248402596,
        0.010374794714152813,
        -0.01533549278974533,
        0.010788186453282833,
        0.027577217668294907,
        -0.023349955677986145,
        0.009194628335535526,
        -0.004710663575679064,
        -0.011328262276947498,
        -0.00980804767459631,
        0.003322134492918849,
        -0.0023269944358617067,
        0.025150207802653313,
        -0.009768042713403702,
        -0.042939379811286926,
        0.020109498873353004,
        0.004330609925091267,
        0.004727332387119532,
        -0.00904794130474329,
        0.008327839896082878,
        -0.022469831630587578,
        -0.004547307267785072,
        -0.019882800057530403,
        0.027390524744987488,
        0.015042118728160858,
        0.00933464802801609,
        -0.006587594281882048,
        0.022136451676487923,
        0.0007267690380103886,
        -0.00228532194159925,
        0.0009918063879013062,
        -8.381388761335984e-05,
        0.007681081537157297,
        -0.017055734992027283,
        -0.01434868760406971,
        -0.003103770548477769,
        -0.03584504872560501,
        0.012601775117218494,
        0.0008517866372130811,
        -0.008134478703141212,
        -0.009741371497511864,
        0.0019202703842893243,
        0.024656806141138077,
        0.008567873388528824,
        0.004580644890666008,
        0.011261586099863052,
        0.015562191605567932,
        -0.009461332112550735,
        0.012568436563014984,
        -0.00733436644077301,
        0.002718716161325574,
        -0.00025170212029479444,
        0.002108630258589983,
        0.01781584322452545,
        -0.010228106752038002,
        0.02216312102973461,
        0.02409672737121582,
        0.0046206507831811905,
        -0.0006038350402377546,
        -0.020562896504998207,
        0.0012026693439111114,
        -0.011868338100612164,
        -0.012488425709307194,
        0.006294219754636288,
        -0.014415363781154156,
        -0.04717997834086418,
        -0.016722355037927628,
        0.02098962292075157,
        0.015135465189814568,
        0.0012943489709869027,
        -0.01685570739209652,
        0.008741230703890324,
        -0.018389256671071053,
        -0.012808470986783504,
        -0.0009709700825624168,
        -0.001995281083509326,
        -0.010181433521211147,
        -0.004353946540504694,
        -0.05952838435769081,
        -0.0096013518050313,
        -0.04533971846103668,
        0.006727613974362612,
        0.03352472186088562,
        0.0033721416257321835,
        -0.00436394801363349,
        -0.007681081537157297,
        -0.033258017152547836,
        -0.005140724126249552,
        0.03653847798705101,
        -0.015082123689353466,
        -0.027950603514909744,
        0.006354228127747774,
        -0.0450730137526989,
        0.04003230482339859,
        0.01612227037549019,
        -0.01740245148539543,
        -0.0044839647598564625,
        -0.014988777227699757,
        0.004083908628672361,
        -0.031604450196027756,
        -0.015095459297299385,
        0.0010584824485704303,
        0.020109498873353004,
        0.0012585106305778027,
        -0.017189087346196175,
        0.02555026486515999,
        -0.0022236465010792017,
        -0.012055031023919582,
        -0.0006071688258089125,
        -0.029390806332230568,
        0.047473352402448654,
        0.002633704338222742,
        -0.019389396533370018,
        0.009187960997223854,
        0.017509132623672485,
        0.012181716039776802,
        -0.004353946540504694,
        0.02888406813144684,
        -0.0010501479264348745,
        -0.018562614917755127,
        0.0138552850112319,
        -0.0036605154164135456,
        0.019189368933439255,
        -0.003178780898451805,
        -0.03485824167728424,
        -0.013308540917932987,
        -0.008447856642305851,
        0.010388129390776157,
        -0.03920552134513855,
        -0.0034504858776926994,
        0.00650758296251297,
        0.011181574314832687,
        -0.01901601068675518,
        -0.012581772170960903,
        -0.021629713475704193,
        0.01392196025699377,
        0.01990947127342224,
        -0.011021552607417107,
        0.002422007732093334,
        -0.0055174436420202255,
        0.013268535025417805,
        0.012075033970177174,
        -0.014655397273600101,
        0.024270085617899895,
        -0.002778724767267704,
        -0.005184063222259283,
        -0.0003223370586056262,
        0.0032287880312651396,
        0.005104052368551493,
        0.025950321927666664,
        -0.03421815112233162,
        0.0017044066917151213,
        0.03101770207285881,
        0.007821101695299149,
        0.019136028364300728,
        0.0048806872218847275,
        -0.0024486782494932413,
        0.015962248668074608,
        -0.010881532914936543,
        0.02883072756230831,
        -0.011514955200254917,
        0.017122412100434303,
        0.001248509157449007,
        -0.009361318312585354,
        -0.011134901084005833,
        0.02469681203365326,
        0.0333380289375782,
        0.03813870623707771,
        0.03349805250763893,
        -0.017482461407780647,
        -0.006940977647900581,
        -0.012628445401787758,
        -0.013788608834147453,
        -0.004540639463812113,
        0.009768042713403702,
        0.04120580479502678,
        -0.009981405921280384,
        0.015482180751860142,
        0.005974174477159977,
        0.0027420527767390013,
        0.020309526473283768,
        0.007141005713492632,
        0.016735689714550972,
        0.005267408676445484,
        -0.005374090280383825,
        0.032937973737716675,
        0.0012951823882758617,
        0.007094332482665777,
        -0.013895289972424507,
        -0.0007371871615760028,
        0.00447729742154479,
        -0.016749026253819466,
        -0.017282433807849884,
        -0.0014060313114896417,
        -0.025123538449406624,
        0.019056016579270363,
        0.02066957764327526,
        -0.014322017319500446,
        0.03405813127756119,
        0.0005142390727996826,
        0.040112316608428955,
        -0.012008357793092728,
        -0.02021618001163006,
        0.03336469829082489,
        -0.01080818846821785,
        -0.015002112835645676,
        -0.013681926764547825,
        -0.019762782379984856,
        0.014415363781154156,
        -0.005027374718338251,
        -0.0027270507998764515,
        -0.0025253556668758392,
        -0.0008492862689308822,
        -0.023283278569579124,
        0.0034904915373772383,
        0.00037067721132189035,
        0.020816264674067497,
        0.014282011426985264,
        -0.018295910209417343,
        0.004490632563829422,
        0.011054890230298042,
        0.003813870484009385,
        0.012795135378837585,
        -0.029497487470507622,
        -0.016562333330512047,
        0.021763065829873085,
        -0.01116157229989767,
        -0.03952556475996971,
        -0.021883081644773483,
        -0.026110343635082245,
        -0.02713715471327305,
        0.040085647255182266,
        -0.011888341046869755,
        0.022136451676487923,
        -0.005554115399718285,
        0.0242567490786314,
        0.02555026486515999,
        0.009407991543412209,
        -0.0034704888239502907,
        0.027630558237433434,
        0.011928346939384937,
        -0.005354087334126234,
        -0.010068085044622421,
        -0.0007050993153825402,
        0.013168521225452423,
        0.04304606094956398,
        0.03256458789110184,
        -0.00974803976714611,
        0.010461472906172276,
        -0.01704240031540394,
        0.012875146232545376,
        -0.03320467472076416,
        -0.02536357194185257,
        -0.0011718317400664091,
        -0.017895853146910667,
        -0.011321594007313251,
        -0.007901112549006939,
        -0.0026853783056139946,
        -0.005240737926214933,
        -0.016495656222105026,
        -0.007641076110303402,
        -0.018389256671071053,
        -0.0395522378385067,
        0.019856128841638565,
        0.008367844857275486,
        -0.006880969274789095,
        0.021096304059028625,
        0.030617645010352135,
        0.027310512959957123,
        0.014402028173208237,
        0.008981265127658844,
        0.02117631584405899,
        0.02196309342980385,
        -0.005140724126249552,
        -0.019216040149331093,
        0.0032204536255449057,
        -0.016402309760451317,
        0.00904794130474329,
        0.014602056704461575,
        0.00784110464155674,
        0.020549559965729713,
        -0.00772108742967248,
        -0.009901394136250019,
        0.028057284653186798,
        0.008907921612262726,
        0.011394938454031944,
        0.01216171309351921,
        0.007207681890577078,
        -0.014148659072816372,
        0.010581490583717823,
        -0.03504493460059166,
        -0.00036525976611301303,
        0.031791143119335175,
        -0.006360895931720734,
        0.007661079056560993,
        -0.009247968904674053,
        0.01084152702242136,
        -0.027630558237433434,
        -0.006020847707986832,
        0.029630839824676514,
        -0.0015085458289831877,
        -0.018362585455179214,
        -0.0003198367194272578,
        -0.0021619710605591536,
        -0.01740245148539543,
        -0.012695121578872204,
        -0.016375640407204628,
        0.004904023837298155,
        -0.002453678986057639,
        0.002502019051462412,
        -0.0025136873591691256,
        -0.03085767850279808,
        -0.027470534667372704,
        -0.028777386993169785,
        0.019642766565084457,
        -0.001286847866140306,
        -0.024176737293601036,
        -0.049313612282276154,
        -0.014695403166115284,
        0.022069774568080902,
        0.02336329035460949,
        0.017029065638780594,
        -0.017415786162018776,
        -0.02663041651248932,
        -0.00934798363596201,
        0.007987791672348976,
        -0.021736394613981247,
        0.004193923901766539,
        -0.02400338090956211,
        -0.013008498586714268,
        -0.011534958146512508,
        -0.028937408700585365,
        -0.004750669002532959,
        -0.03299131244421005,
        0.013828613795340061,
        -0.009294642135500908,
        0.01660233736038208,
        0.018709301948547363,
        -0.019416067749261856,
        0.017189087346196175,
        0.01101488433778286,
        0.008341174572706223,
        0.011261586099863052,
        0.02142968401312828,
        -0.030937690287828445,
        0.018055876716971397,
        -0.010294782929122448,
        -0.023549983277916908,
        -0.017122412100434303,
        0.006437573116272688,
        0.009481335058808327,
        0.00430060550570488,
        -0.02920411340892315,
        0.011948348954319954,
        -0.014735408127307892,
        0.02057623118162155,
        -0.02467014081776142,
        -7.1364214818459e-05,
        0.005497440695762634,
        0.03685852512717247,
        0.016482321545481682,
        0.016895713284611702,
        0.022803211584687233,
        0.0029120768886059523,
        -0.00465065473690629,
        -0.008261163718998432,
        -0.02492351084947586,
        0.018562614917755127,
        0.0194294024258852,
        -0.010268112644553185,
        0.009821383282542229,
        0.032191202044487,
        -0.005837488919496536,
        -0.035951729863882065,
        0.02536357194185257,
        -0.04083241894841194,
        0.018682630732655525,
        -0.0010476475581526756,
        -0.017642484977841377,
        -0.03285796195268631,
        -0.02396337501704693,
        0.0066842748783528805,
        0.0009509672527201474,
        -0.04283269867300987,
        0.008201154880225658,
        0.0008651218377053738,
        0.0027320515364408493,
        0.0012268394930288196,
        -0.017975864931941032,
        0.023629995062947273,
        -0.02835065871477127,
        -0.018842654302716255,
        0.004237263463437557,
        0.014508710242807865,
        0.015122129581868649,
        -0.01768249087035656,
        0.013508569449186325,
        -0.033258017152547836,
        -0.013895289972424507,
        0.009001268073916435,
        -0.018109217286109924,
        0.004373949486762285,
        0.018055876716971397,
        0.007707752287387848,
        0.01534882839769125,
        0.03581837937235832,
        0.0036538478452712297,
        0.01958942599594593,
        0.021763065829873085,
        -0.004887355025857687,
        0.009214631281793118,
        -0.023856693878769875,
        0.009614687412977219,
        -0.0017619148129597306,
        0.011034887284040451,
        0.00962135475128889,
        -0.01570887863636017,
        -0.01452204491943121,
        -0.0052207354456186295,
        0.019802788272500038,
        0.010608160868287086,
        -0.007567732594907284,
        -0.018042540177702904,
        -0.02867070399224758,
        -0.023403296247124672,
        -0.024776821956038475,
        0.0019369394285604358,
        -0.003133774735033512,
        -0.005497440695762634,
        -0.05158059671521187,
        -0.01812255196273327,
        0.0008013628539629281,
        0.0031587781850248575,
        0.008267831057310104,
        0.0008297002059407532,
        0.020389538258314133,
        0.0036538478452712297,
        -0.007654411252588034,
        0.009061275981366634,
        0.0030070901848375797,
        -0.014468704350292683,
        -0.007314363494515419,
        -0.02752387709915638,
        -0.011081560514867306,
        0.03640512749552727,
        0.021149644628167152,
        -0.00500737177208066,
        0.00585749140009284,
        -0.010301451198756695,
        -0.001103488728404045,
        0.013228530064225197,
        0.0029070761520415545,
        -0.013155185617506504,
        -0.025243554264307022,
        -0.014708737842738628,
        -0.021389678120613098,
        -0.030804337933659554,
        -0.012588439509272575,
        0.02904408983886242,
        0.013721932657063007,
        -0.008114475756883621,
        -0.029124101623892784,
        0.032964643090963364,
        -0.0002389919973211363,
        -0.0033037986140698195,
        0.004023900255560875,
        0.024883504956960678,
        0.032937973737716675,
        -0.001675235922448337,
        0.011381602846086025,
        0.016242288053035736,
        0.0022786541376262903,
        -0.006347560789436102,
        0.0021619710605591536,
        0.004997370298951864,
        -0.03035094030201435,
        -0.000267537689069286,
        -0.009788044728338718,
        -0.0005438266089186072,
        -0.010421467944979668,
        0.016909047961235046,
        0.01042813528329134,
        -0.03117772378027439,
        -0.02053622528910637,
        0.014322017319500446,
        -0.007281025405973196,
        0.023923369124531746,
        -0.002218645764514804,
        -0.019322721287608147,
        -0.001751913339830935,
        0.013868619687855244,
        -0.01653566211462021,
        0.02869737520813942,
        0.002815396524965763,
        0.00013929045235272497,
        0.019096022471785545,
        0.02708381414413452,
        -0.010001408867537975,
        0.022816546261310577,
        -0.01444203406572342,
        0.008687890134751797,
        0.00265037314966321,
        0.006967647932469845,
        -0.020362867042422295,
        -0.011654974892735481,
        0.006797623820602894,
        0.020749589428305626,
        -0.012535098940134048,
        0.017122412100434303,
        0.007974456064403057,
        -0.011614969000220299,
        0.00662760017439723,
        0.009134619496762753,
        -0.0014418697683140635,
        0.03083100914955139,
        -0.02787059172987938,
        -0.005124054849147797,
        -0.0007338533760048449,
        0.004497299902141094,
        0.017375780269503593,
        -0.013788608834147453,
        0.001751913339830935,
        0.0007451049168594182,
        -0.017029065638780594,
        0.01570887863636017,
        0.025563599541783333,
        0.024150067940354347,
        -0.023763347417116165,
        0.0072743576020002365,
        0.005717471707612276,
        -0.0014535380760207772,
        -0.002433676039800048,
        0.006204206962138414,
        0.01993614062666893,
        -0.03555167466402054,
        -0.013788608834147453,
        0.034111469984054565,
        0.014282011426985264,
        0.020149504765868187,
        -0.00895459484308958,
        -0.011481616646051407,
        -0.014455368742346764,
        0.02984420396387577,
        -0.04373949393630028,
        0.007947785779833794,
        0.012395079247653484,
        -0.013681926764547825,
        -0.013208527117967606,
        -0.018269238993525505,
        -0.015575527213513851,
        -0.01293515507131815,
        0.0007967788842506707,
        -0.0015035450924187899,
        -0.022229798138141632,
        -0.0019869464449584484,
        -0.005197398830205202,
        -0.005670798476785421,
        0.007994459010660648,
        -0.022136451676487923,
        -0.020896276459097862,
        0.018936000764369965,
        0.0206962488591671,
        -0.019376061856746674,
        -0.01366859208792448,
        -0.012435084208846092,
        0.0029220781289041042,
        -0.00328212883323431,
        0.01271512359380722,
        -0.027977272868156433,
        -0.01452204491943121,
        0.0016694017685949802,
        0.01758914440870285,
        -0.011928346939384937,
        -0.04101911187171936,
        -0.0011543292785063386,
        -0.006974315736442804,
        0.009461332112550735,
        -0.0018819316755980253,
        -0.015028783120214939,
        0.022083109244704247,
        -0.01878931373357773,
        -0.008787903934717178,
        0.02591031603515148,
        -0.00884124543517828,
        -0.010481475852429867,
        0.003190449206158519,
        -0.00792111549526453,
        0.013341878540813923,
        0.02479015849530697,
        -0.0097147012129426,
        -0.034964922815561295,
        -0.011754988692700863,
        -0.003883880330249667,
        -0.017882518470287323,
        -0.008487861603498459,
        -0.015548856928944588,
        0.051633939146995544,
        0.031444426625967026,
        -0.010228106752038002,
        -0.010674837045371532,
        0.0024153401609510183,
        -0.025830304250121117,
        0.003423815593123436,
        -0.005870827008038759,
        0.0070809973403811455,
        0.0010659834370017052,
        -0.021229656413197517,
        -0.0023453303147107363,
        0.017735831439495087,
        -0.0004996536881662905,
        -0.004964032210409641,
        -0.008261163718998432,
        -0.0018719303188845515,
        -0.021002957597374916,
        -0.01841592602431774,
        0.00025232721236534417,
        -0.012681785970926285,
        -0.03933887183666229,
        0.014708737842738628,
        0.028617363423109055,
        -0.006250880192965269,
        0.0172424279153347,
        0.04387284442782402,
        -0.00871456041932106,
        0.001788585213944316,
        -0.00974803976714611,
        0.016655679792165756,
        0.02186974696815014,
        0.017389114946126938,
        0.011168239638209343,
        -0.026270367205142975,
        0.01154162548482418,
        0.04171254113316536,
        -0.0037538621108978987,
        -0.02050955593585968,
        0.002850401448085904,
        0.0047906748950481415,
        0.00784110464155674,
        -0.012101704254746437,
        -0.021856412291526794,
        -0.0020186176989227533,
        0.0019469409016892314,
        -0.019856128841638565,
        0.005077381618320942,
        0.01781584322452545,
        -0.02888406813144684,
        0.0073076956905424595,
        0.0028604029212146997,
        0.00657092547044158,
        -0.03157778084278107,
        0.017722496762871742,
        -0.010734844952821732,
        -0.0012285063276067376,
        0.01917603425681591,
        -0.0012593440478667617,
        0.0016493989387527108,
        -0.016655679792165756,
        0.005594121292233467,
        0.009928064420819283,
        -0.006174203008413315,
        -0.03301798179745674,
        0.001644398202188313,
        -0.023216603323817253,
        0.008587876334786415,
        0.007734422571957111,
        -0.044966332614421844,
        -0.006710945162922144,
        -0.0020486218854784966,
        -0.019882800057530403,
        -0.017255762591958046,
        -0.004980701487511396,
        0.000838034669868648,
        -0.005024041049182415,
        0.01698905974626541,
        -0.0060508521273732185,
        -0.016709020361304283,
        -0.01949607953429222,
        0.005604122765362263,
        0.02142968401312828,
        0.00415058434009552,
        0.02695046178996563,
        0.228085458278656,
        -0.029284123331308365,
        -0.00036505141179077327,
        0.00515739293769002,
        0.003147109877318144,
        0.019802788272500038,
        -0.0013276869431138039,
        -0.0026303704362362623,
        -0.009774710051715374,
        -0.008867915719747543,
        0.04149917885661125,
        0.006914306897670031,
        -0.01312851533293724,
        -0.0017452457686886191,
        0.018242569640278816,
        -0.02336329035460949,
        -0.014148659072816372,
        -0.03421815112233162,
        -0.012601775117218494,
        0.01242841687053442,
        0.005134056322276592,
        0.01216171309351921,
        0.002530356403440237,
        -0.0008076137164607644,
        0.028937408700585365,
        0.004067239351570606,
        0.0032337887678295374,
        -0.0024486782494932413,
        0.021149644628167152,
        0.004924026783555746,
        -0.006197539623826742,
        -0.005454101599752903,
        0.011701648123562336,
        0.02085627056658268,
        -0.010248109698295593,
        -0.0009134619613178074,
        -0.029977554455399513,
        0.003218786558136344,
        0.018749307841062546,
        -0.017122412100434303,
        0.027657227590680122,
        -0.007541061844676733,
        0.010268112644553185,
        -0.04133915528655052,
        0.008041132241487503,
        -0.0009776377119123936,
        -0.004910691641271114,
        -0.006454242393374443,
        -0.004447293002158403,
        0.02202976867556572,
        -0.008947926573455334,
        0.02545691840350628,
        0.02098962292075157,
        0.005194064695388079,
        0.008361177518963814,
        0.00035546673461794853,
        -0.009854720905423164,
        0.009041273035109043,
        0.027977272868156433,
        0.005440766457468271,
        -0.008961262181401253,
        -0.0001698155829217285,
        -0.0010168098378926516,
        0.02301657572388649,
        -0.01312851533293724,
        0.022109780460596085,
        -0.02612367831170559,
        0.0231099221855402,
        0.0356316864490509,
        0.021509695798158646,
        0.007054326590150595,
        -0.023790016770362854,
        -0.007741090375930071,
        0.014895430766046047,
        -0.04389951750636101,
        -0.03424482420086861,
        0.051313892006874084,
        0.012488425709307194,
        0.039472226053476334,
        0.03003089688718319,
        0.009701366536319256,
        0.001888599363155663,
        -0.018175892531871796,
        -0.015495515428483486,
        -0.0010159764206036925,
        -0.04376616328954697,
        0.029790861532092094,
        -0.023056579753756523,
        0.001105155679397285,
        -0.024736817926168442,
        0.003963891416788101,
        -0.008494529873132706,
        -0.01133492961525917,
        -0.007301028352230787,
        0.008241160772740841,
        0.013481899164617062,
        0.017162416130304337,
        0.012588439509272575,
        -0.010454805567860603,
        -0.009167958050966263,
        -0.02545691840350628,
        0.06902305036783218,
        -0.0025470254477113485,
        0.00812781136482954,
        0.002713715424761176,
        0.01816255785524845,
        -0.013681926764547825,
        0.014055312611162663,
        0.008461191318929195,
        -0.011741654016077518,
        0.016802366822957993,
        -0.03139108791947365,
        0.008001127280294895,
        -0.0035538338124752045,
        0.011814997531473637,
        0.012115039862692356,
        0.029577499255537987,
        0.003673850791528821,
        0.018269238993525505,
        -0.0144286984577775,
        0.01733577437698841,
        -0.022403154522180557,
        0.009834717959165573,
        0.015162135474383831,
        0.005960839334875345,
        -0.004463961813598871,
        -0.018215898424386978,
        0.010241442359983921,
        -0.03221787139773369,
        -0.0032254543621093035,
        -0.014868760481476784,
        -0.02552359364926815,
        0.022109780460596085,
        -0.02835065871477127,
        -0.02536357194185257,
        -0.003680518362671137,
        -0.001108489464968443,
        -0.024963514879345894,
        -0.02416340261697769,
        0.02170972339808941,
        0.0020536226220428944,
        0.008201154880225658,
        0.016842372715473175,
        -7.469802221748978e-05,
        -0.020136170089244843,
        0.002567028161138296,
        -0.008741230703890324,
        -0.0031604450196027756,
        0.00635756179690361,
        -0.02727050706744194,
        -0.008581208065152168,
        -0.017882518470287323,
        -0.011448279023170471,
        -0.0006538420566357672,
        0.02216312102973461,
        0.010534817352890968,
        -0.006407569162547588,
        -0.008154481649398804,
        0.00683429604396224,
        0.005110719706863165,
        -0.02209644578397274,
        -0.021896416321396828,
        0.0017252429388463497,
        -0.0078077660873532295,
        -0.009794712997972965,
        -0.0016910715494304895,
        -0.16845038533210754,
        -0.0010668168542906642,
        0.025443583726882935,
        -0.046779923141002655,
        0.011995022185146809,
        0.0024936844129115343,
        0.02479015849530697,
        -0.030110906809568405,
        -0.05054045096039772,
        0.014602056704461575,
        -0.00010371252574259415,
        0.0020119501277804375,
        0.0016485655214637518,
        -0.009794712997972965,
        -0.03248457610607147,
        0.006497581955045462,
        -0.0242567490786314,
        0.01818922907114029,
        0.014588721096515656,
        -0.0027403859421610832,
        0.0377119779586792,
        -0.03800535202026367,
        0.026297036558389664,
        -0.011694980785250664,
        0.03547166287899017,
        -0.0014026975259184837,
        0.013775273226201534,
        0.019189368933439255,
        -0.01246175542473793,
        -0.04491299018263817,
        -0.0012218387564644217,
        -0.006454242393374443,
        0.03488491475582123,
        -0.022043105214834213,
        -0.009341315366327763,
        0.015148799866437912,
        -0.010774850845336914,
        -0.0057208058424293995,
        -0.00950800534337759,
        0.006914306897670031,
        0.029897544533014297,
        0.0073410337790846825,
        0.0043272762559354305,
        -0.008227825164794922,
        -0.02479015849530697,
        0.0020919612143188715,
        0.029124101623892784,
        -0.012181716039776802,
        0.004500634036958218,
        -0.011054890230298042,
        0.008987932465970516,
        -0.0006059186416678131,
        -0.002218645764514804,
        0.011148236691951752,
        0.0030404282733798027,
        0.003563835285604,
        0.028430670499801636,
        0.018469268456101418,
        -0.011301591992378235,
        -0.017189087346196175,
        -0.009121284820139408,
        -0.007814433425664902,
        0.0042939381673932076,
        0.009814715944230556,
        -0.015922242775559425,
        -0.011654974892735481,
        -0.009501338005065918,
        -0.012868478894233704,
        -0.007901112549006939,
        0.021002957597374916,
        -0.031444426625967026,
        -0.011895008385181427,
        -0.01071484200656414,
        -0.0052774096839129925,
        0.005614123772829771,
        0.017855847254395485,
        -0.017802506685256958,
        -0.022043105214834213,
        0.00307876686565578,
        -0.002470347797498107,
        -0.02237648516893387,
        0.021536367014050484,
        -0.0094279944896698,
        0.0033371367026120424,
        0.005594121292233467,
        0.016775695607066154,
        -0.0004917359328828752,
        0.02492351084947586,
        -0.019816124811768532,
        0.0009793045464903116,
        0.039125509560108185,
        -0.012721791863441467,
        0.0025470254477113485,
        -0.0034138141199946404,
        -0.010041413828730583,
        0.011214912869036198,
        0.0067742872051894665,
        0.0020119501277804375,
        0.004130581859499216,
        -0.018269238993525505,
        0.010754847899079323,
        -0.0027687232941389084,
        -0.015242146328091621,
        0.020136170089244843,
        0.03507160767912865,
        0.016175610944628716,
        -0.015562191605567932,
        0.003032093634828925,
        0.04336610808968544,
        0.00842785369604826,
        -0.01685570739209652,
        -0.004907357972115278,
        0.0009042940218932927,
        -0.0053107477724552155,
        -0.004023900255560875,
        0.03003089688718319,
        -0.0006146698724478483,
        -0.017469126731157303,
        -0.0029120768886059523,
        -0.019322721287608147,
        0.048806872218847275,
        -0.028404001146554947,
        -0.03237789496779442,
        0.029257453978061676,
        -0.011568295769393444,
        -0.0314977690577507,
        -0.11649640649557114,
        -0.025030191987752914,
        -0.011788327246904373,
        0.01285514421761036,
        -0.004233929794281721,
        0.009074611589312553,
        0.015922242775559425,
        0.024203408509492874,
        -0.03248457610607147,
        0.04189923405647278,
        -0.00016929468256421387,
        -0.01910935714840889,
        0.01704240031540394,
        -0.015175470151007175,
        -0.0073076956905424595,
        0.001351857092231512,
        0.012901817448437214,
        0.010488144122064114,
        -0.01071484200656414,
        0.028083955869078636,
        0.012068366631865501,
        -0.031071042641997337,
        0.021483026444911957,
        -0.016895713284611702,
        -0.004440625198185444,
        0.0018135887803509831,
        -0.017882518470287323,
        0.008594543673098087,
        -0.0010759849101305008,
        -0.022496500983834267,
        0.016322297975420952,
        -0.036778513342142105,
        0.0071143354289233685,
        -0.012515095993876457,
        0.01876264251768589,
        -0.01033478882163763,
        0.003907216712832451,
        -0.021483026444911957,
        -0.016802366822957993,
        -0.006744283251464367,
        -0.011421608738601208,
        0.005090716760605574,
        0.0011609968496486545,
        0.0032421231735497713,
        -0.01736244559288025,
        -0.0172424279153347,
        -0.022109780460596085,
        0.027030473574995995,
        0.0031721133273094893,
        -0.03235122188925743,
        -0.028270648792386055,
        -0.023643329739570618,
        -0.05024707689881325,
        -0.0038005353417247534,
        0.0024736816994845867,
        -0.014055312611162663,
        -0.001976944971829653,
        0.0036371788010001183,
        -0.01090153492987156,
        -0.005967507138848305,
        0.003607174614444375,
        -0.009701366536319256,
        -0.011088227853178978,
        0.029790861532092094,
        0.01476207934319973,
        -0.005707470700144768,
        -0.010001408867537975,
        -0.03240456432104111,
        -0.006384232547134161,
        -0.026110343635082245,
        -0.0015960581367835402,
        0.00963469035923481,
        0.0008342841756530106,
        0.007801098749041557,
        -0.03565835580229759,
        -0.0012410080526024103,
        -0.018709301948547363,
        -0.01910935714840889,
        -0.0038038690108805895,
        0.010368126444518566,
        -0.026577075943350792,
        -0.003317133756354451,
        0.008401183411478996,
        -0.024803493171930313,
        -0.013721932657063007,
        -0.0030054233502596617,
        -0.006550922524183989,
        -0.005297412630170584,
        -0.002998755546286702,
        -0.02708381414413452,
        0.0029920879751443863,
        0.02933746576309204,
        -0.011468281969428062,
        0.000514655839651823,
        -0.00992139708250761,
        0.021096304059028625,
        -0.0028037282172590494,
        -0.023816687986254692,
        0.0264437235891819,
        0.02308325096964836,
        -0.017949193716049194,
        -0.005944170523434877,
        -0.02301657572388649,
        0.0164423156529665,
        -0.009501338005065918,
        -0.008721228688955307,
        0.002960416954010725,
        -0.011468281969428062,
        0.04200591519474983,
        0.016709020361304283,
        -0.022563178092241287,
        0.008581208065152168,
        -0.009968070313334465,
        0.021136309951543808,
        0.0014035309432074428,
        -0.027217166498303413,
        -0.014922101981937885,
        -0.022403154522180557,
        0.014735408127307892,
        -0.022936563938856125,
        0.022149786353111267,
        0.01940273307263851,
        -0.005264074541628361,
        -0.014868760481476784,
        0.021029628813266754,
        0.0015785556752234697,
        0.007894445210695267,
        -0.002181974006816745,
        -0.00782776903361082,
        0.021056298166513443,
        -0.010708174668252468,
        -0.022176455706357956,
        0.031071042641997337,
        -0.01892266422510147,
        0.005000704433768988,
        0.014255341142416,
        -0.002030286006629467,
        0.015988918021321297,
        0.001396863372065127,
        -0.004817345179617405,
        0.022523172199726105,
        -0.006697610020637512,
        -0.005140724126249552,
        -0.02260318398475647,
        0.029070761054754257,
        -0.01850927248597145,
        0.013388551771640778,
        -0.007467718329280615,
        -0.022456495091319084,
        -0.0038738788571208715,
        0.01860261894762516,
        -0.018709301948547363,
        0.01704240031540394,
        0.0144286984577775,
        0.01222172100096941,
        -0.031444426625967026,
        -0.0016002253396436572,
        -0.004183922428637743,
        -0.016682349145412445,
        0.0001160580141004175,
        0.01542883925139904,
        0.003500493010506034,
        0.02701713889837265,
        0.004593980498611927,
        -0.009021270088851452,
        -0.010654834099113941,
        0.004697328433394432,
        0.013208527117967606,
        0.003597173374146223,
        0.013828613795340061,
        -0.003433816833421588,
        -0.012121707201004028,
        7.297902629943565e-05,
        0.011188242584466934,
        0.0011493285419419408,
        0.0016310630599036813,
        0.0344848558306694,
        0.0188559889793396,
        0.020522890612483025,
        -0.0037671972531825304,
        -0.00636422960087657,
        0.0476067028939724,
        0.014575386419892311,
        -0.019696107134222984,
        -0.016455650329589844,
        0.009374653920531273,
        0.01374860294163227,
        0.00465732254087925,
        0.006860966328531504,
        -0.02559027075767517,
        -0.011908343993127346,
        0.005557449534535408,
        0.007961121387779713,
        0.008087805472314358,
        -0.01366859208792448,
        0.03723191097378731,
        0.0037638633511960506,
        0.01720242202281952,
        0.0061842044815421104,
        0.030137578025460243,
        0.02304324507713318,
        0.006227543577551842,
        0.00256369449198246,
        -0.003430483164265752,
        -0.00278539233841002,
        0.010514814406633377,
        -0.004057237878441811,
        0.027630558237433434,
        -0.011401605792343616,
        -0.01742912083864212,
        0.019376061856746674,
        0.027230501174926758,
        0.0093146450817585,
        -0.008567873388528824,
        -0.01745579205453396,
        -0.007207681890577078,
        -0.017309105023741722,
        0.01660233736038208,
        -0.007327698636800051,
        -0.024870168417692184,
        -0.02463013492524624,
        0.005114053376019001,
        0.011975020170211792,
        -0.015562191605567932,
        0.049126919358968735,
        -0.013468563556671143,
        0.019829459488391876,
        -0.009454664774239063,
        0.025656946003437042,
        -0.020722918212413788,
        0.013681926764547825,
        0.02987087331712246,
        0.01765581965446472,
        -0.006704277358949184,
        0.010174766182899475,
        0.001715241582132876,
        -0.007134337909519672,
        -0.011434943415224552,
        0.02168305404484272,
        0.02603033185005188,
        -0.019949475303292274,
        0.07227684557437897,
        0.024483447894454002,
        0.007394374813884497,
        -0.0018469267524778843,
        -0.02349664270877838,
        0.022109780460596085,
        0.005240737926214933,
        0.015988918021321297,
        -0.023643329739570618,
        0.0017569140763953328,
        0.004317274782806635,
        0.0042572664096951485,
        0.0002967084583360702,
        -0.02361665852367878,
        0.0012376742670312524,
        -0.0051207211799919605,
        0.013175188563764095,
        0.003943888936191797,
        0.003112104954198003,
        0.00040609887219034135,
        0.03669850155711174,
        -0.009928064420819283,
        0.02244316041469574,
        0.0023703337647020817,
        -0.023643329739570618,
        0.004730666056275368,
        0.009594684466719627,
        0.03152443841099739,
        -0.0029920879751443863,
        -0.03699187561869621,
        0.016869042068719864,
        0.013301873579621315,
        -0.029470816254615784,
        -0.0031887823715806007,
        0.002603699918836355,
        -0.011301591992378235,
        -0.012708456255495548,
        -0.011701648123562336,
        0.0018019204726442695,
        0.012915152125060558,
        0.020749589428305626,
        0.03573836758732796,
        -0.021069634705781937,
        -0.023723341524600983,
        -0.0015610532136633992,
        0.002572028897702694,
        -0.01204836368560791,
        -0.02200309932231903,
        -0.006360895931720734
      ]
    },
    {
      "node": "check_cache_answer",
      "flow_run_id": "4d4879be-de55-475e-85ef-cc3de9de4117",
      "run_id": "4d4879be-de55-475e-85ef-cc3de9de4117_check_cache_answer_0",
      "status": "Completed",
      "inputs": {
        "conn": "entaoai",
        "embeddedQuestion": [
          -0.016869042068719864,
          -0.0003010840737260878,
          0.022069774568080902,
          -0.01818922907114029,
          -0.0022569845896214247,
          0.013475230894982815,
          -0.0065809269435703754,
          0.004023900255560875,
          -0.032884631305933,
          -0.019216040149331093,
          0.02651040069758892,
          0.030404282733798027,
          -0.024683475494384766,
          0.0007167676230892539,
          -0.018149223178625107,
          -0.015762219205498695,
          0.036565151065588,
          0.028804056346416473,
          0.005427430849522352,
          -0.025256890803575516,
          -0.011601634323596954,
          -0.009081278927624226,
          -0.024976851418614388,
          -0.011588298715651035,
          -0.02108296938240528,
          -0.0024053386878222227,
          0.045446399599313736,
          -0.020642906427383423,
          -0.01844259724020958,
          0.0037738648243248463,
          0.026403717696666718,
          0.02196309342980385,
          -0.016562333330512047,
          -0.02032286301255226,
          -0.011454946361482143,
          0.009701366536319256,
          -0.0029054090846329927,
          -0.01225505955517292,
          0.024843499064445496,
          -0.01254176627844572,
          0.01114156935364008,
          0.032751280814409256,
          0.011975020170211792,
          -0.015215476043522358,
          -0.0032737944275140762,
          0.0264437235891819,
          0.022069774568080902,
          -0.00033192173577845097,
          -0.03133774548768997,
          0.020056158304214478,
          0.002888740273192525,
          0.030564304441213608,
          -0.016975723206996918,
          0.003323801327496767,
          -0.05198065564036369,
          -0.007454383186995983,
          -0.006334225181490183,
          0.005664131138473749,
          -0.0017552472418174148,
          -0.025723623111844063,
          0.002136967610567808,
          0.006624266039580107,
          -0.00904794130474329,
          0.01246175542473793,
          -0.012735126540064812,
          -0.013095177710056305,
          -0.005047377664595842,
          0.022816546261310577,
          0.010688171721994877,
          -0.0024503450840711594,
          0.012088368646800518,
          0.02571028843522072,
          -0.01602892391383648,
          -0.006470911204814911,
          0.027577217668294907,
          -0.014828754588961601,
          -0.0028720712289214134,
          0.021576372906565666,
          0.01041479967534542,
          -0.0031987838447093964,
          -0.0014477039221674204,
          0.006234211381524801,
          -0.01090153492987156,
          0.014268675819039345,
          0.02060290239751339,
          0.014735408127307892,
          0.0026270365342497826,
          0.0010318119311705232,
          0.003170446492731571,
          0.015802225098013878,
          0.005537446588277817,
          0.018749307841062546,
          0.03416481241583824,
          0.02221646159887314,
          0.011074893176555634,
          -0.0029904211405664682,
          -0.005217401310801506,
          0.014562050811946392,
          -0.00028566524269990623,
          0.004353946540504694,
          -0.009027938358485699,
          0.009401324205100536,
          -0.029817532747983932,
          -0.011661642231047153,
          -0.004167253617197275,
          0.009067943319678307,
          0.018629290163517,
          -0.005494107026606798,
          -0.0012393412180244923,
          -0.0036205099895596504,
          -0.03037761151790619,
          0.014482039958238602,
          -0.014588721096515656,
          -0.014175329357385635,
          -0.0054140957072377205,
          0.0022019767202436924,
          0.0005113219958730042,
          -0.010294782929122448,
          -0.01401530671864748,
          0.004560642410069704,
          0.023629995062947273,
          -1.4324934454634786e-05,
          0.018815983086824417,
          -0.017189087346196175,
          0.01698905974626541,
          -0.019322721287608147,
          -0.015028783120214939,
          -0.017975864931941032,
          -0.007247687317430973,
          -0.009167958050966263,
          0.02002948708832264,
          0.021403014659881592,
          0.00963469035923481,
          -0.02092294581234455,
          -0.007207681890577078,
          0.005680799949914217,
          0.01781584322452545,
          -0.016815701499581337,
          -0.014482039958238602,
          -0.019189368933439255,
          0.023830022662878036,
          0.016175610944628716,
          -0.02591031603515148,
          -0.03267126902937889,
          -0.05371423065662384,
          0.02012283354997635,
          -0.000785110576543957,
          0.030564304441213608,
          0.008974596858024597,
          -0.022456495091319084,
          0.012995163910090923,
          -0.029284123331308365,
          -0.012835141271352768,
          0.03224454075098038,
          0.002288655610755086,
          0.0055507817305624485,
          0.0018986007198691368,
          0.006704277358949184,
          -0.014161994680762291,
          -0.0231099221855402,
          0.01802920550107956,
          0.017975864931941032,
          0.023736676201224327,
          -0.006900971755385399,
          0.017149081453680992,
          0.021949758753180504,
          0.0017919189995154738,
          -0.005604122765362263,
          0.017375780269503593,
          -0.001429367926903069,
          0.005760811269283295,
          0.030110906809568405,
          -0.014455368742346764,
          0.007487721275538206,
          -0.0011893342016264796,
          0.01284180860966444,
          0.03205784782767296,
          -0.020056158304214478,
          -0.004427290055900812,
          -0.03920552134513855,
          0.023029910400509834,
          0.010188101790845394,
          0.03371141478419304,
          -0.013248532079160213,
          -0.018642624840140343,
          0.008454523980617523,
          0.013828613795340061,
          -0.017135746777057648,
          0.007401042152196169,
          -0.036805182695388794,
          0.007361036725342274,
          0.004623984452337027,
          -0.0066509367898106575,
          -0.022496500983834267,
          -0.6383832097053528,
          -0.026083674281835556,
          0.011181574314832687,
          -0.022229798138141632,
          0.015642203390598297,
          0.0144286984577775,
          0.010294782929122448,
          0.008834577165544033,
          -0.014375357888638973,
          0.016895713284611702,
          -0.0036705168895423412,
          -0.0242567490786314,
          -0.012355073355138302,
          -0.017922524362802505,
          -0.006840963382273912,
          -0.024243414402008057,
          0.01041479967534542,
          -0.013495233841240406,
          0.005487439688295126,
          0.0028720712289214134,
          -0.015815559774637222,
          0.03184448555111885,
          -0.023856693878769875,
          -0.017122412100434303,
          -0.011888341046869755,
          -0.010554819367825985,
          0.008614546619355679,
          -0.008961262181401253,
          -0.0003694270271807909,
          -0.004160585813224316,
          -0.011228247545659542,
          0.02679044008255005,
          0.026337042450904846,
          -0.008227825164794922,
          0.03952556475996971,
          0.010241442359983921,
          -0.007987791672348976,
          0.011988354846835136,
          -9.897226846078411e-05,
          0.0432327538728714,
          -0.004000563640147448,
          -0.03136441856622696,
          0.01694905385375023,
          -0.006024181842803955,
          0.006390899885445833,
          0.009814715944230556,
          -0.0008313670987263322,
          0.020896276459097862,
          0.00922129862010479,
          -0.009908062405884266,
          -0.0033804760314524174,
          0.011508287861943245,
          -0.0031371084041893482,
          -6.230043800314888e-05,
          0.016562333330512047,
          -0.014135324396193027,
          0.02120298519730568,
          -0.01952274888753891,
          -0.0004071406729053706,
          -0.006010846700519323,
          0.010381462052464485,
          0.003970559220761061,
          -0.004810677375644445,
          -0.03733859211206436,
          -0.005260740872472525,
          0.007574399933218956,
          0.0069809830747544765,
          0.004774005617946386,
          0.007561064790934324,
          -0.04285936802625656,
          0.007201014086604118,
          0.016322297975420952,
          0.0020636238623410463,
          -0.011081560514867306,
          -0.007681081537157297,
          0.026750434190034866,
          0.01434868760406971,
          -0.007314363494515419,
          -0.006454242393374443,
          0.013001831248402596,
          0.010374794714152813,
          -0.01533549278974533,
          0.010788186453282833,
          0.027577217668294907,
          -0.023349955677986145,
          0.009194628335535526,
          -0.004710663575679064,
          -0.011328262276947498,
          -0.00980804767459631,
          0.003322134492918849,
          -0.0023269944358617067,
          0.025150207802653313,
          -0.009768042713403702,
          -0.042939379811286926,
          0.020109498873353004,
          0.004330609925091267,
          0.004727332387119532,
          -0.00904794130474329,
          0.008327839896082878,
          -0.022469831630587578,
          -0.004547307267785072,
          -0.019882800057530403,
          0.027390524744987488,
          0.015042118728160858,
          0.00933464802801609,
          -0.006587594281882048,
          0.022136451676487923,
          0.0007267690380103886,
          -0.00228532194159925,
          0.0009918063879013062,
          -8.381388761335984e-05,
          0.007681081537157297,
          -0.017055734992027283,
          -0.01434868760406971,
          -0.003103770548477769,
          -0.03584504872560501,
          0.012601775117218494,
          0.0008517866372130811,
          -0.008134478703141212,
          -0.009741371497511864,
          0.0019202703842893243,
          0.024656806141138077,
          0.008567873388528824,
          0.004580644890666008,
          0.011261586099863052,
          0.015562191605567932,
          -0.009461332112550735,
          0.012568436563014984,
          -0.00733436644077301,
          0.002718716161325574,
          -0.00025170212029479444,
          0.002108630258589983,
          0.01781584322452545,
          -0.010228106752038002,
          0.02216312102973461,
          0.02409672737121582,
          0.0046206507831811905,
          -0.0006038350402377546,
          -0.020562896504998207,
          0.0012026693439111114,
          -0.011868338100612164,
          -0.012488425709307194,
          0.006294219754636288,
          -0.014415363781154156,
          -0.04717997834086418,
          -0.016722355037927628,
          0.02098962292075157,
          0.015135465189814568,
          0.0012943489709869027,
          -0.01685570739209652,
          0.008741230703890324,
          -0.018389256671071053,
          -0.012808470986783504,
          -0.0009709700825624168,
          -0.001995281083509326,
          -0.010181433521211147,
          -0.004353946540504694,
          -0.05952838435769081,
          -0.0096013518050313,
          -0.04533971846103668,
          0.006727613974362612,
          0.03352472186088562,
          0.0033721416257321835,
          -0.00436394801363349,
          -0.007681081537157297,
          -0.033258017152547836,
          -0.005140724126249552,
          0.03653847798705101,
          -0.015082123689353466,
          -0.027950603514909744,
          0.006354228127747774,
          -0.0450730137526989,
          0.04003230482339859,
          0.01612227037549019,
          -0.01740245148539543,
          -0.0044839647598564625,
          -0.014988777227699757,
          0.004083908628672361,
          -0.031604450196027756,
          -0.015095459297299385,
          0.0010584824485704303,
          0.020109498873353004,
          0.0012585106305778027,
          -0.017189087346196175,
          0.02555026486515999,
          -0.0022236465010792017,
          -0.012055031023919582,
          -0.0006071688258089125,
          -0.029390806332230568,
          0.047473352402448654,
          0.002633704338222742,
          -0.019389396533370018,
          0.009187960997223854,
          0.017509132623672485,
          0.012181716039776802,
          -0.004353946540504694,
          0.02888406813144684,
          -0.0010501479264348745,
          -0.018562614917755127,
          0.0138552850112319,
          -0.0036605154164135456,
          0.019189368933439255,
          -0.003178780898451805,
          -0.03485824167728424,
          -0.013308540917932987,
          -0.008447856642305851,
          0.010388129390776157,
          -0.03920552134513855,
          -0.0034504858776926994,
          0.00650758296251297,
          0.011181574314832687,
          -0.01901601068675518,
          -0.012581772170960903,
          -0.021629713475704193,
          0.01392196025699377,
          0.01990947127342224,
          -0.011021552607417107,
          0.002422007732093334,
          -0.0055174436420202255,
          0.013268535025417805,
          0.012075033970177174,
          -0.014655397273600101,
          0.024270085617899895,
          -0.002778724767267704,
          -0.005184063222259283,
          -0.0003223370586056262,
          0.0032287880312651396,
          0.005104052368551493,
          0.025950321927666664,
          -0.03421815112233162,
          0.0017044066917151213,
          0.03101770207285881,
          0.007821101695299149,
          0.019136028364300728,
          0.0048806872218847275,
          -0.0024486782494932413,
          0.015962248668074608,
          -0.010881532914936543,
          0.02883072756230831,
          -0.011514955200254917,
          0.017122412100434303,
          0.001248509157449007,
          -0.009361318312585354,
          -0.011134901084005833,
          0.02469681203365326,
          0.0333380289375782,
          0.03813870623707771,
          0.03349805250763893,
          -0.017482461407780647,
          -0.006940977647900581,
          -0.012628445401787758,
          -0.013788608834147453,
          -0.004540639463812113,
          0.009768042713403702,
          0.04120580479502678,
          -0.009981405921280384,
          0.015482180751860142,
          0.005974174477159977,
          0.0027420527767390013,
          0.020309526473283768,
          0.007141005713492632,
          0.016735689714550972,
          0.005267408676445484,
          -0.005374090280383825,
          0.032937973737716675,
          0.0012951823882758617,
          0.007094332482665777,
          -0.013895289972424507,
          -0.0007371871615760028,
          0.00447729742154479,
          -0.016749026253819466,
          -0.017282433807849884,
          -0.0014060313114896417,
          -0.025123538449406624,
          0.019056016579270363,
          0.02066957764327526,
          -0.014322017319500446,
          0.03405813127756119,
          0.0005142390727996826,
          0.040112316608428955,
          -0.012008357793092728,
          -0.02021618001163006,
          0.03336469829082489,
          -0.01080818846821785,
          -0.015002112835645676,
          -0.013681926764547825,
          -0.019762782379984856,
          0.014415363781154156,
          -0.005027374718338251,
          -0.0027270507998764515,
          -0.0025253556668758392,
          -0.0008492862689308822,
          -0.023283278569579124,
          0.0034904915373772383,
          0.00037067721132189035,
          0.020816264674067497,
          0.014282011426985264,
          -0.018295910209417343,
          0.004490632563829422,
          0.011054890230298042,
          0.003813870484009385,
          0.012795135378837585,
          -0.029497487470507622,
          -0.016562333330512047,
          0.021763065829873085,
          -0.01116157229989767,
          -0.03952556475996971,
          -0.021883081644773483,
          -0.026110343635082245,
          -0.02713715471327305,
          0.040085647255182266,
          -0.011888341046869755,
          0.022136451676487923,
          -0.005554115399718285,
          0.0242567490786314,
          0.02555026486515999,
          0.009407991543412209,
          -0.0034704888239502907,
          0.027630558237433434,
          0.011928346939384937,
          -0.005354087334126234,
          -0.010068085044622421,
          -0.0007050993153825402,
          0.013168521225452423,
          0.04304606094956398,
          0.03256458789110184,
          -0.00974803976714611,
          0.010461472906172276,
          -0.01704240031540394,
          0.012875146232545376,
          -0.03320467472076416,
          -0.02536357194185257,
          -0.0011718317400664091,
          -0.017895853146910667,
          -0.011321594007313251,
          -0.007901112549006939,
          -0.0026853783056139946,
          -0.005240737926214933,
          -0.016495656222105026,
          -0.007641076110303402,
          -0.018389256671071053,
          -0.0395522378385067,
          0.019856128841638565,
          0.008367844857275486,
          -0.006880969274789095,
          0.021096304059028625,
          0.030617645010352135,
          0.027310512959957123,
          0.014402028173208237,
          0.008981265127658844,
          0.02117631584405899,
          0.02196309342980385,
          -0.005140724126249552,
          -0.019216040149331093,
          0.0032204536255449057,
          -0.016402309760451317,
          0.00904794130474329,
          0.014602056704461575,
          0.00784110464155674,
          0.020549559965729713,
          -0.00772108742967248,
          -0.009901394136250019,
          0.028057284653186798,
          0.008907921612262726,
          0.011394938454031944,
          0.01216171309351921,
          0.007207681890577078,
          -0.014148659072816372,
          0.010581490583717823,
          -0.03504493460059166,
          -0.00036525976611301303,
          0.031791143119335175,
          -0.006360895931720734,
          0.007661079056560993,
          -0.009247968904674053,
          0.01084152702242136,
          -0.027630558237433434,
          -0.006020847707986832,
          0.029630839824676514,
          -0.0015085458289831877,
          -0.018362585455179214,
          -0.0003198367194272578,
          -0.0021619710605591536,
          -0.01740245148539543,
          -0.012695121578872204,
          -0.016375640407204628,
          0.004904023837298155,
          -0.002453678986057639,
          0.002502019051462412,
          -0.0025136873591691256,
          -0.03085767850279808,
          -0.027470534667372704,
          -0.028777386993169785,
          0.019642766565084457,
          -0.001286847866140306,
          -0.024176737293601036,
          -0.049313612282276154,
          -0.014695403166115284,
          0.022069774568080902,
          0.02336329035460949,
          0.017029065638780594,
          -0.017415786162018776,
          -0.02663041651248932,
          -0.00934798363596201,
          0.007987791672348976,
          -0.021736394613981247,
          0.004193923901766539,
          -0.02400338090956211,
          -0.013008498586714268,
          -0.011534958146512508,
          -0.028937408700585365,
          -0.004750669002532959,
          -0.03299131244421005,
          0.013828613795340061,
          -0.009294642135500908,
          0.01660233736038208,
          0.018709301948547363,
          -0.019416067749261856,
          0.017189087346196175,
          0.01101488433778286,
          0.008341174572706223,
          0.011261586099863052,
          0.02142968401312828,
          -0.030937690287828445,
          0.018055876716971397,
          -0.010294782929122448,
          -0.023549983277916908,
          -0.017122412100434303,
          0.006437573116272688,
          0.009481335058808327,
          0.00430060550570488,
          -0.02920411340892315,
          0.011948348954319954,
          -0.014735408127307892,
          0.02057623118162155,
          -0.02467014081776142,
          -7.1364214818459e-05,
          0.005497440695762634,
          0.03685852512717247,
          0.016482321545481682,
          0.016895713284611702,
          0.022803211584687233,
          0.0029120768886059523,
          -0.00465065473690629,
          -0.008261163718998432,
          -0.02492351084947586,
          0.018562614917755127,
          0.0194294024258852,
          -0.010268112644553185,
          0.009821383282542229,
          0.032191202044487,
          -0.005837488919496536,
          -0.035951729863882065,
          0.02536357194185257,
          -0.04083241894841194,
          0.018682630732655525,
          -0.0010476475581526756,
          -0.017642484977841377,
          -0.03285796195268631,
          -0.02396337501704693,
          0.0066842748783528805,
          0.0009509672527201474,
          -0.04283269867300987,
          0.008201154880225658,
          0.0008651218377053738,
          0.0027320515364408493,
          0.0012268394930288196,
          -0.017975864931941032,
          0.023629995062947273,
          -0.02835065871477127,
          -0.018842654302716255,
          0.004237263463437557,
          0.014508710242807865,
          0.015122129581868649,
          -0.01768249087035656,
          0.013508569449186325,
          -0.033258017152547836,
          -0.013895289972424507,
          0.009001268073916435,
          -0.018109217286109924,
          0.004373949486762285,
          0.018055876716971397,
          0.007707752287387848,
          0.01534882839769125,
          0.03581837937235832,
          0.0036538478452712297,
          0.01958942599594593,
          0.021763065829873085,
          -0.004887355025857687,
          0.009214631281793118,
          -0.023856693878769875,
          0.009614687412977219,
          -0.0017619148129597306,
          0.011034887284040451,
          0.00962135475128889,
          -0.01570887863636017,
          -0.01452204491943121,
          -0.0052207354456186295,
          0.019802788272500038,
          0.010608160868287086,
          -0.007567732594907284,
          -0.018042540177702904,
          -0.02867070399224758,
          -0.023403296247124672,
          -0.024776821956038475,
          0.0019369394285604358,
          -0.003133774735033512,
          -0.005497440695762634,
          -0.05158059671521187,
          -0.01812255196273327,
          0.0008013628539629281,
          0.0031587781850248575,
          0.008267831057310104,
          0.0008297002059407532,
          0.020389538258314133,
          0.0036538478452712297,
          -0.007654411252588034,
          0.009061275981366634,
          0.0030070901848375797,
          -0.014468704350292683,
          -0.007314363494515419,
          -0.02752387709915638,
          -0.011081560514867306,
          0.03640512749552727,
          0.021149644628167152,
          -0.00500737177208066,
          0.00585749140009284,
          -0.010301451198756695,
          -0.001103488728404045,
          0.013228530064225197,
          0.0029070761520415545,
          -0.013155185617506504,
          -0.025243554264307022,
          -0.014708737842738628,
          -0.021389678120613098,
          -0.030804337933659554,
          -0.012588439509272575,
          0.02904408983886242,
          0.013721932657063007,
          -0.008114475756883621,
          -0.029124101623892784,
          0.032964643090963364,
          -0.0002389919973211363,
          -0.0033037986140698195,
          0.004023900255560875,
          0.024883504956960678,
          0.032937973737716675,
          -0.001675235922448337,
          0.011381602846086025,
          0.016242288053035736,
          0.0022786541376262903,
          -0.006347560789436102,
          0.0021619710605591536,
          0.004997370298951864,
          -0.03035094030201435,
          -0.000267537689069286,
          -0.009788044728338718,
          -0.0005438266089186072,
          -0.010421467944979668,
          0.016909047961235046,
          0.01042813528329134,
          -0.03117772378027439,
          -0.02053622528910637,
          0.014322017319500446,
          -0.007281025405973196,
          0.023923369124531746,
          -0.002218645764514804,
          -0.019322721287608147,
          -0.001751913339830935,
          0.013868619687855244,
          -0.01653566211462021,
          0.02869737520813942,
          0.002815396524965763,
          0.00013929045235272497,
          0.019096022471785545,
          0.02708381414413452,
          -0.010001408867537975,
          0.022816546261310577,
          -0.01444203406572342,
          0.008687890134751797,
          0.00265037314966321,
          0.006967647932469845,
          -0.020362867042422295,
          -0.011654974892735481,
          0.006797623820602894,
          0.020749589428305626,
          -0.012535098940134048,
          0.017122412100434303,
          0.007974456064403057,
          -0.011614969000220299,
          0.00662760017439723,
          0.009134619496762753,
          -0.0014418697683140635,
          0.03083100914955139,
          -0.02787059172987938,
          -0.005124054849147797,
          -0.0007338533760048449,
          0.004497299902141094,
          0.017375780269503593,
          -0.013788608834147453,
          0.001751913339830935,
          0.0007451049168594182,
          -0.017029065638780594,
          0.01570887863636017,
          0.025563599541783333,
          0.024150067940354347,
          -0.023763347417116165,
          0.0072743576020002365,
          0.005717471707612276,
          -0.0014535380760207772,
          -0.002433676039800048,
          0.006204206962138414,
          0.01993614062666893,
          -0.03555167466402054,
          -0.013788608834147453,
          0.034111469984054565,
          0.014282011426985264,
          0.020149504765868187,
          -0.00895459484308958,
          -0.011481616646051407,
          -0.014455368742346764,
          0.02984420396387577,
          -0.04373949393630028,
          0.007947785779833794,
          0.012395079247653484,
          -0.013681926764547825,
          -0.013208527117967606,
          -0.018269238993525505,
          -0.015575527213513851,
          -0.01293515507131815,
          0.0007967788842506707,
          -0.0015035450924187899,
          -0.022229798138141632,
          -0.0019869464449584484,
          -0.005197398830205202,
          -0.005670798476785421,
          0.007994459010660648,
          -0.022136451676487923,
          -0.020896276459097862,
          0.018936000764369965,
          0.0206962488591671,
          -0.019376061856746674,
          -0.01366859208792448,
          -0.012435084208846092,
          0.0029220781289041042,
          -0.00328212883323431,
          0.01271512359380722,
          -0.027977272868156433,
          -0.01452204491943121,
          0.0016694017685949802,
          0.01758914440870285,
          -0.011928346939384937,
          -0.04101911187171936,
          -0.0011543292785063386,
          -0.006974315736442804,
          0.009461332112550735,
          -0.0018819316755980253,
          -0.015028783120214939,
          0.022083109244704247,
          -0.01878931373357773,
          -0.008787903934717178,
          0.02591031603515148,
          -0.00884124543517828,
          -0.010481475852429867,
          0.003190449206158519,
          -0.00792111549526453,
          0.013341878540813923,
          0.02479015849530697,
          -0.0097147012129426,
          -0.034964922815561295,
          -0.011754988692700863,
          -0.003883880330249667,
          -0.017882518470287323,
          -0.008487861603498459,
          -0.015548856928944588,
          0.051633939146995544,
          0.031444426625967026,
          -0.010228106752038002,
          -0.010674837045371532,
          0.0024153401609510183,
          -0.025830304250121117,
          0.003423815593123436,
          -0.005870827008038759,
          0.0070809973403811455,
          0.0010659834370017052,
          -0.021229656413197517,
          -0.0023453303147107363,
          0.017735831439495087,
          -0.0004996536881662905,
          -0.004964032210409641,
          -0.008261163718998432,
          -0.0018719303188845515,
          -0.021002957597374916,
          -0.01841592602431774,
          0.00025232721236534417,
          -0.012681785970926285,
          -0.03933887183666229,
          0.014708737842738628,
          0.028617363423109055,
          -0.006250880192965269,
          0.0172424279153347,
          0.04387284442782402,
          -0.00871456041932106,
          0.001788585213944316,
          -0.00974803976714611,
          0.016655679792165756,
          0.02186974696815014,
          0.017389114946126938,
          0.011168239638209343,
          -0.026270367205142975,
          0.01154162548482418,
          0.04171254113316536,
          -0.0037538621108978987,
          -0.02050955593585968,
          0.002850401448085904,
          0.0047906748950481415,
          0.00784110464155674,
          -0.012101704254746437,
          -0.021856412291526794,
          -0.0020186176989227533,
          0.0019469409016892314,
          -0.019856128841638565,
          0.005077381618320942,
          0.01781584322452545,
          -0.02888406813144684,
          0.0073076956905424595,
          0.0028604029212146997,
          0.00657092547044158,
          -0.03157778084278107,
          0.017722496762871742,
          -0.010734844952821732,
          -0.0012285063276067376,
          0.01917603425681591,
          -0.0012593440478667617,
          0.0016493989387527108,
          -0.016655679792165756,
          0.005594121292233467,
          0.009928064420819283,
          -0.006174203008413315,
          -0.03301798179745674,
          0.001644398202188313,
          -0.023216603323817253,
          0.008587876334786415,
          0.007734422571957111,
          -0.044966332614421844,
          -0.006710945162922144,
          -0.0020486218854784966,
          -0.019882800057530403,
          -0.017255762591958046,
          -0.004980701487511396,
          0.000838034669868648,
          -0.005024041049182415,
          0.01698905974626541,
          -0.0060508521273732185,
          -0.016709020361304283,
          -0.01949607953429222,
          0.005604122765362263,
          0.02142968401312828,
          0.00415058434009552,
          0.02695046178996563,
          0.228085458278656,
          -0.029284123331308365,
          -0.00036505141179077327,
          0.00515739293769002,
          0.003147109877318144,
          0.019802788272500038,
          -0.0013276869431138039,
          -0.0026303704362362623,
          -0.009774710051715374,
          -0.008867915719747543,
          0.04149917885661125,
          0.006914306897670031,
          -0.01312851533293724,
          -0.0017452457686886191,
          0.018242569640278816,
          -0.02336329035460949,
          -0.014148659072816372,
          -0.03421815112233162,
          -0.012601775117218494,
          0.01242841687053442,
          0.005134056322276592,
          0.01216171309351921,
          0.002530356403440237,
          -0.0008076137164607644,
          0.028937408700585365,
          0.004067239351570606,
          0.0032337887678295374,
          -0.0024486782494932413,
          0.021149644628167152,
          0.004924026783555746,
          -0.006197539623826742,
          -0.005454101599752903,
          0.011701648123562336,
          0.02085627056658268,
          -0.010248109698295593,
          -0.0009134619613178074,
          -0.029977554455399513,
          0.003218786558136344,
          0.018749307841062546,
          -0.017122412100434303,
          0.027657227590680122,
          -0.007541061844676733,
          0.010268112644553185,
          -0.04133915528655052,
          0.008041132241487503,
          -0.0009776377119123936,
          -0.004910691641271114,
          -0.006454242393374443,
          -0.004447293002158403,
          0.02202976867556572,
          -0.008947926573455334,
          0.02545691840350628,
          0.02098962292075157,
          0.005194064695388079,
          0.008361177518963814,
          0.00035546673461794853,
          -0.009854720905423164,
          0.009041273035109043,
          0.027977272868156433,
          0.005440766457468271,
          -0.008961262181401253,
          -0.0001698155829217285,
          -0.0010168098378926516,
          0.02301657572388649,
          -0.01312851533293724,
          0.022109780460596085,
          -0.02612367831170559,
          0.0231099221855402,
          0.0356316864490509,
          0.021509695798158646,
          0.007054326590150595,
          -0.023790016770362854,
          -0.007741090375930071,
          0.014895430766046047,
          -0.04389951750636101,
          -0.03424482420086861,
          0.051313892006874084,
          0.012488425709307194,
          0.039472226053476334,
          0.03003089688718319,
          0.009701366536319256,
          0.001888599363155663,
          -0.018175892531871796,
          -0.015495515428483486,
          -0.0010159764206036925,
          -0.04376616328954697,
          0.029790861532092094,
          -0.023056579753756523,
          0.001105155679397285,
          -0.024736817926168442,
          0.003963891416788101,
          -0.008494529873132706,
          -0.01133492961525917,
          -0.007301028352230787,
          0.008241160772740841,
          0.013481899164617062,
          0.017162416130304337,
          0.012588439509272575,
          -0.010454805567860603,
          -0.009167958050966263,
          -0.02545691840350628,
          0.06902305036783218,
          -0.0025470254477113485,
          0.00812781136482954,
          0.002713715424761176,
          0.01816255785524845,
          -0.013681926764547825,
          0.014055312611162663,
          0.008461191318929195,
          -0.011741654016077518,
          0.016802366822957993,
          -0.03139108791947365,
          0.008001127280294895,
          -0.0035538338124752045,
          0.011814997531473637,
          0.012115039862692356,
          0.029577499255537987,
          0.003673850791528821,
          0.018269238993525505,
          -0.0144286984577775,
          0.01733577437698841,
          -0.022403154522180557,
          0.009834717959165573,
          0.015162135474383831,
          0.005960839334875345,
          -0.004463961813598871,
          -0.018215898424386978,
          0.010241442359983921,
          -0.03221787139773369,
          -0.0032254543621093035,
          -0.014868760481476784,
          -0.02552359364926815,
          0.022109780460596085,
          -0.02835065871477127,
          -0.02536357194185257,
          -0.003680518362671137,
          -0.001108489464968443,
          -0.024963514879345894,
          -0.02416340261697769,
          0.02170972339808941,
          0.0020536226220428944,
          0.008201154880225658,
          0.016842372715473175,
          -7.469802221748978e-05,
          -0.020136170089244843,
          0.002567028161138296,
          -0.008741230703890324,
          -0.0031604450196027756,
          0.00635756179690361,
          -0.02727050706744194,
          -0.008581208065152168,
          -0.017882518470287323,
          -0.011448279023170471,
          -0.0006538420566357672,
          0.02216312102973461,
          0.010534817352890968,
          -0.006407569162547588,
          -0.008154481649398804,
          0.00683429604396224,
          0.005110719706863165,
          -0.02209644578397274,
          -0.021896416321396828,
          0.0017252429388463497,
          -0.0078077660873532295,
          -0.009794712997972965,
          -0.0016910715494304895,
          -0.16845038533210754,
          -0.0010668168542906642,
          0.025443583726882935,
          -0.046779923141002655,
          0.011995022185146809,
          0.0024936844129115343,
          0.02479015849530697,
          -0.030110906809568405,
          -0.05054045096039772,
          0.014602056704461575,
          -0.00010371252574259415,
          0.0020119501277804375,
          0.0016485655214637518,
          -0.009794712997972965,
          -0.03248457610607147,
          0.006497581955045462,
          -0.0242567490786314,
          0.01818922907114029,
          0.014588721096515656,
          -0.0027403859421610832,
          0.0377119779586792,
          -0.03800535202026367,
          0.026297036558389664,
          -0.011694980785250664,
          0.03547166287899017,
          -0.0014026975259184837,
          0.013775273226201534,
          0.019189368933439255,
          -0.01246175542473793,
          -0.04491299018263817,
          -0.0012218387564644217,
          -0.006454242393374443,
          0.03488491475582123,
          -0.022043105214834213,
          -0.009341315366327763,
          0.015148799866437912,
          -0.010774850845336914,
          -0.0057208058424293995,
          -0.00950800534337759,
          0.006914306897670031,
          0.029897544533014297,
          0.0073410337790846825,
          0.0043272762559354305,
          -0.008227825164794922,
          -0.02479015849530697,
          0.0020919612143188715,
          0.029124101623892784,
          -0.012181716039776802,
          0.004500634036958218,
          -0.011054890230298042,
          0.008987932465970516,
          -0.0006059186416678131,
          -0.002218645764514804,
          0.011148236691951752,
          0.0030404282733798027,
          0.003563835285604,
          0.028430670499801636,
          0.018469268456101418,
          -0.011301591992378235,
          -0.017189087346196175,
          -0.009121284820139408,
          -0.007814433425664902,
          0.0042939381673932076,
          0.009814715944230556,
          -0.015922242775559425,
          -0.011654974892735481,
          -0.009501338005065918,
          -0.012868478894233704,
          -0.007901112549006939,
          0.021002957597374916,
          -0.031444426625967026,
          -0.011895008385181427,
          -0.01071484200656414,
          -0.0052774096839129925,
          0.005614123772829771,
          0.017855847254395485,
          -0.017802506685256958,
          -0.022043105214834213,
          0.00307876686565578,
          -0.002470347797498107,
          -0.02237648516893387,
          0.021536367014050484,
          -0.0094279944896698,
          0.0033371367026120424,
          0.005594121292233467,
          0.016775695607066154,
          -0.0004917359328828752,
          0.02492351084947586,
          -0.019816124811768532,
          0.0009793045464903116,
          0.039125509560108185,
          -0.012721791863441467,
          0.0025470254477113485,
          -0.0034138141199946404,
          -0.010041413828730583,
          0.011214912869036198,
          0.0067742872051894665,
          0.0020119501277804375,
          0.004130581859499216,
          -0.018269238993525505,
          0.010754847899079323,
          -0.0027687232941389084,
          -0.015242146328091621,
          0.020136170089244843,
          0.03507160767912865,
          0.016175610944628716,
          -0.015562191605567932,
          0.003032093634828925,
          0.04336610808968544,
          0.00842785369604826,
          -0.01685570739209652,
          -0.004907357972115278,
          0.0009042940218932927,
          -0.0053107477724552155,
          -0.004023900255560875,
          0.03003089688718319,
          -0.0006146698724478483,
          -0.017469126731157303,
          -0.0029120768886059523,
          -0.019322721287608147,
          0.048806872218847275,
          -0.028404001146554947,
          -0.03237789496779442,
          0.029257453978061676,
          -0.011568295769393444,
          -0.0314977690577507,
          -0.11649640649557114,
          -0.025030191987752914,
          -0.011788327246904373,
          0.01285514421761036,
          -0.004233929794281721,
          0.009074611589312553,
          0.015922242775559425,
          0.024203408509492874,
          -0.03248457610607147,
          0.04189923405647278,
          -0.00016929468256421387,
          -0.01910935714840889,
          0.01704240031540394,
          -0.015175470151007175,
          -0.0073076956905424595,
          0.001351857092231512,
          0.012901817448437214,
          0.010488144122064114,
          -0.01071484200656414,
          0.028083955869078636,
          0.012068366631865501,
          -0.031071042641997337,
          0.021483026444911957,
          -0.016895713284611702,
          -0.004440625198185444,
          0.0018135887803509831,
          -0.017882518470287323,
          0.008594543673098087,
          -0.0010759849101305008,
          -0.022496500983834267,
          0.016322297975420952,
          -0.036778513342142105,
          0.0071143354289233685,
          -0.012515095993876457,
          0.01876264251768589,
          -0.01033478882163763,
          0.003907216712832451,
          -0.021483026444911957,
          -0.016802366822957993,
          -0.006744283251464367,
          -0.011421608738601208,
          0.005090716760605574,
          0.0011609968496486545,
          0.0032421231735497713,
          -0.01736244559288025,
          -0.0172424279153347,
          -0.022109780460596085,
          0.027030473574995995,
          0.0031721133273094893,
          -0.03235122188925743,
          -0.028270648792386055,
          -0.023643329739570618,
          -0.05024707689881325,
          -0.0038005353417247534,
          0.0024736816994845867,
          -0.014055312611162663,
          -0.001976944971829653,
          0.0036371788010001183,
          -0.01090153492987156,
          -0.005967507138848305,
          0.003607174614444375,
          -0.009701366536319256,
          -0.011088227853178978,
          0.029790861532092094,
          0.01476207934319973,
          -0.005707470700144768,
          -0.010001408867537975,
          -0.03240456432104111,
          -0.006384232547134161,
          -0.026110343635082245,
          -0.0015960581367835402,
          0.00963469035923481,
          0.0008342841756530106,
          0.007801098749041557,
          -0.03565835580229759,
          -0.0012410080526024103,
          -0.018709301948547363,
          -0.01910935714840889,
          -0.0038038690108805895,
          0.010368126444518566,
          -0.026577075943350792,
          -0.003317133756354451,
          0.008401183411478996,
          -0.024803493171930313,
          -0.013721932657063007,
          -0.0030054233502596617,
          -0.006550922524183989,
          -0.005297412630170584,
          -0.002998755546286702,
          -0.02708381414413452,
          0.0029920879751443863,
          0.02933746576309204,
          -0.011468281969428062,
          0.000514655839651823,
          -0.00992139708250761,
          0.021096304059028625,
          -0.0028037282172590494,
          -0.023816687986254692,
          0.0264437235891819,
          0.02308325096964836,
          -0.017949193716049194,
          -0.005944170523434877,
          -0.02301657572388649,
          0.0164423156529665,
          -0.009501338005065918,
          -0.008721228688955307,
          0.002960416954010725,
          -0.011468281969428062,
          0.04200591519474983,
          0.016709020361304283,
          -0.022563178092241287,
          0.008581208065152168,
          -0.009968070313334465,
          0.021136309951543808,
          0.0014035309432074428,
          -0.027217166498303413,
          -0.014922101981937885,
          -0.022403154522180557,
          0.014735408127307892,
          -0.022936563938856125,
          0.022149786353111267,
          0.01940273307263851,
          -0.005264074541628361,
          -0.014868760481476784,
          0.021029628813266754,
          0.0015785556752234697,
          0.007894445210695267,
          -0.002181974006816745,
          -0.00782776903361082,
          0.021056298166513443,
          -0.010708174668252468,
          -0.022176455706357956,
          0.031071042641997337,
          -0.01892266422510147,
          0.005000704433768988,
          0.014255341142416,
          -0.002030286006629467,
          0.015988918021321297,
          0.001396863372065127,
          -0.004817345179617405,
          0.022523172199726105,
          -0.006697610020637512,
          -0.005140724126249552,
          -0.02260318398475647,
          0.029070761054754257,
          -0.01850927248597145,
          0.013388551771640778,
          -0.007467718329280615,
          -0.022456495091319084,
          -0.0038738788571208715,
          0.01860261894762516,
          -0.018709301948547363,
          0.01704240031540394,
          0.0144286984577775,
          0.01222172100096941,
          -0.031444426625967026,
          -0.0016002253396436572,
          -0.004183922428637743,
          -0.016682349145412445,
          0.0001160580141004175,
          0.01542883925139904,
          0.003500493010506034,
          0.02701713889837265,
          0.004593980498611927,
          -0.009021270088851452,
          -0.010654834099113941,
          0.004697328433394432,
          0.013208527117967606,
          0.003597173374146223,
          0.013828613795340061,
          -0.003433816833421588,
          -0.012121707201004028,
          7.297902629943565e-05,
          0.011188242584466934,
          0.0011493285419419408,
          0.0016310630599036813,
          0.0344848558306694,
          0.0188559889793396,
          0.020522890612483025,
          -0.0037671972531825304,
          -0.00636422960087657,
          0.0476067028939724,
          0.014575386419892311,
          -0.019696107134222984,
          -0.016455650329589844,
          0.009374653920531273,
          0.01374860294163227,
          0.00465732254087925,
          0.006860966328531504,
          -0.02559027075767517,
          -0.011908343993127346,
          0.005557449534535408,
          0.007961121387779713,
          0.008087805472314358,
          -0.01366859208792448,
          0.03723191097378731,
          0.0037638633511960506,
          0.01720242202281952,
          0.0061842044815421104,
          0.030137578025460243,
          0.02304324507713318,
          0.006227543577551842,
          0.00256369449198246,
          -0.003430483164265752,
          -0.00278539233841002,
          0.010514814406633377,
          -0.004057237878441811,
          0.027630558237433434,
          -0.011401605792343616,
          -0.01742912083864212,
          0.019376061856746674,
          0.027230501174926758,
          0.0093146450817585,
          -0.008567873388528824,
          -0.01745579205453396,
          -0.007207681890577078,
          -0.017309105023741722,
          0.01660233736038208,
          -0.007327698636800051,
          -0.024870168417692184,
          -0.02463013492524624,
          0.005114053376019001,
          0.011975020170211792,
          -0.015562191605567932,
          0.049126919358968735,
          -0.013468563556671143,
          0.019829459488391876,
          -0.009454664774239063,
          0.025656946003437042,
          -0.020722918212413788,
          0.013681926764547825,
          0.02987087331712246,
          0.01765581965446472,
          -0.006704277358949184,
          0.010174766182899475,
          0.001715241582132876,
          -0.007134337909519672,
          -0.011434943415224552,
          0.02168305404484272,
          0.02603033185005188,
          -0.019949475303292274,
          0.07227684557437897,
          0.024483447894454002,
          0.007394374813884497,
          -0.0018469267524778843,
          -0.02349664270877838,
          0.022109780460596085,
          0.005240737926214933,
          0.015988918021321297,
          -0.023643329739570618,
          0.0017569140763953328,
          0.004317274782806635,
          0.0042572664096951485,
          0.0002967084583360702,
          -0.02361665852367878,
          0.0012376742670312524,
          -0.0051207211799919605,
          0.013175188563764095,
          0.003943888936191797,
          0.003112104954198003,
          0.00040609887219034135,
          0.03669850155711174,
          -0.009928064420819283,
          0.02244316041469574,
          0.0023703337647020817,
          -0.023643329739570618,
          0.004730666056275368,
          0.009594684466719627,
          0.03152443841099739,
          -0.0029920879751443863,
          -0.03699187561869621,
          0.016869042068719864,
          0.013301873579621315,
          -0.029470816254615784,
          -0.0031887823715806007,
          0.002603699918836355,
          -0.011301591992378235,
          -0.012708456255495548,
          -0.011701648123562336,
          0.0018019204726442695,
          0.012915152125060558,
          0.020749589428305626,
          0.03573836758732796,
          -0.021069634705781937,
          -0.023723341524600983,
          -0.0015610532136633992,
          0.002572028897702694,
          -0.01204836368560791,
          -0.02200309932231903,
          -0.006360895931720734
        ],
        "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
        "indexType": "cogsearchvs",
        "question": "What is the advantage of fine-tuning BERT over using feature-based approaches?"
      },
      "output": {
        "jsonAnswer": {
          "data_points": [
            "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
            "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
            "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
          ],
          "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
          "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.']<br>        Question: What is the advantage of fine-tuning BERT over using feature-based approaches?<br>        ",
          "sources": "\nBert.pdf",
          "nextQuestions": "<>\n<>\n<>",
          "error": ""
        },
        "existingAnswer": true
      },
      "metrics": null,
      "error": null,
      "parent_run_id": "4d4879be-de55-475e-85ef-cc3de9de4117_0",
      "start_time": "2023-10-12T14:01:55.798991Z",
      "end_time": "2023-10-12T14:01:57.820562Z",
      "index": 0,
      "api_calls": [
        {
          "name": "checkCacheAnswer",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "embeddedQuestion": [
              -0.016869042068719864,
              -0.0003010840737260878,
              0.022069774568080902,
              -0.01818922907114029,
              -0.0022569845896214247,
              0.013475230894982815,
              -0.0065809269435703754,
              0.004023900255560875,
              -0.032884631305933,
              -0.019216040149331093,
              0.02651040069758892,
              0.030404282733798027,
              -0.024683475494384766,
              0.0007167676230892539,
              -0.018149223178625107,
              -0.015762219205498695,
              0.036565151065588,
              0.028804056346416473,
              0.005427430849522352,
              -0.025256890803575516,
              -0.011601634323596954,
              -0.009081278927624226,
              -0.024976851418614388,
              -0.011588298715651035,
              -0.02108296938240528,
              -0.0024053386878222227,
              0.045446399599313736,
              -0.020642906427383423,
              -0.01844259724020958,
              0.0037738648243248463,
              0.026403717696666718,
              0.02196309342980385,
              -0.016562333330512047,
              -0.02032286301255226,
              -0.011454946361482143,
              0.009701366536319256,
              -0.0029054090846329927,
              -0.01225505955517292,
              0.024843499064445496,
              -0.01254176627844572,
              0.01114156935364008,
              0.032751280814409256,
              0.011975020170211792,
              -0.015215476043522358,
              -0.0032737944275140762,
              0.0264437235891819,
              0.022069774568080902,
              -0.00033192173577845097,
              -0.03133774548768997,
              0.020056158304214478,
              0.002888740273192525,
              0.030564304441213608,
              -0.016975723206996918,
              0.003323801327496767,
              -0.05198065564036369,
              -0.007454383186995983,
              -0.006334225181490183,
              0.005664131138473749,
              -0.0017552472418174148,
              -0.025723623111844063,
              0.002136967610567808,
              0.006624266039580107,
              -0.00904794130474329,
              0.01246175542473793,
              -0.012735126540064812,
              -0.013095177710056305,
              -0.005047377664595842,
              0.022816546261310577,
              0.010688171721994877,
              -0.0024503450840711594,
              0.012088368646800518,
              0.02571028843522072,
              -0.01602892391383648,
              -0.006470911204814911,
              0.027577217668294907,
              -0.014828754588961601,
              -0.0028720712289214134,
              0.021576372906565666,
              0.01041479967534542,
              -0.0031987838447093964,
              -0.0014477039221674204,
              0.006234211381524801,
              -0.01090153492987156,
              0.014268675819039345,
              0.02060290239751339,
              0.014735408127307892,
              0.0026270365342497826,
              0.0010318119311705232,
              0.003170446492731571,
              0.015802225098013878,
              0.005537446588277817,
              0.018749307841062546,
              0.03416481241583824,
              0.02221646159887314,
              0.011074893176555634,
              -0.0029904211405664682,
              -0.005217401310801506,
              0.014562050811946392,
              -0.00028566524269990623,
              0.004353946540504694,
              -0.009027938358485699,
              0.009401324205100536,
              -0.029817532747983932,
              -0.011661642231047153,
              -0.004167253617197275,
              0.009067943319678307,
              0.018629290163517,
              -0.005494107026606798,
              -0.0012393412180244923,
              -0.0036205099895596504,
              -0.03037761151790619,
              0.014482039958238602,
              -0.014588721096515656,
              -0.014175329357385635,
              -0.0054140957072377205,
              0.0022019767202436924,
              0.0005113219958730042,
              -0.010294782929122448,
              -0.01401530671864748,
              0.004560642410069704,
              0.023629995062947273,
              -1.4324934454634786e-05,
              0.018815983086824417,
              -0.017189087346196175,
              0.01698905974626541,
              -0.019322721287608147,
              -0.015028783120214939,
              -0.017975864931941032,
              -0.007247687317430973,
              -0.009167958050966263,
              0.02002948708832264,
              0.021403014659881592,
              0.00963469035923481,
              -0.02092294581234455,
              -0.007207681890577078,
              0.005680799949914217,
              0.01781584322452545,
              -0.016815701499581337,
              -0.014482039958238602,
              -0.019189368933439255,
              0.023830022662878036,
              0.016175610944628716,
              -0.02591031603515148,
              -0.03267126902937889,
              -0.05371423065662384,
              0.02012283354997635,
              -0.000785110576543957,
              0.030564304441213608,
              0.008974596858024597,
              -0.022456495091319084,
              0.012995163910090923,
              -0.029284123331308365,
              -0.012835141271352768,
              0.03224454075098038,
              0.002288655610755086,
              0.0055507817305624485,
              0.0018986007198691368,
              0.006704277358949184,
              -0.014161994680762291,
              -0.0231099221855402,
              0.01802920550107956,
              0.017975864931941032,
              0.023736676201224327,
              -0.006900971755385399,
              0.017149081453680992,
              0.021949758753180504,
              0.0017919189995154738,
              -0.005604122765362263,
              0.017375780269503593,
              -0.001429367926903069,
              0.005760811269283295,
              0.030110906809568405,
              -0.014455368742346764,
              0.007487721275538206,
              -0.0011893342016264796,
              0.01284180860966444,
              0.03205784782767296,
              -0.020056158304214478,
              -0.004427290055900812,
              -0.03920552134513855,
              0.023029910400509834,
              0.010188101790845394,
              0.03371141478419304,
              -0.013248532079160213,
              -0.018642624840140343,
              0.008454523980617523,
              0.013828613795340061,
              -0.017135746777057648,
              0.007401042152196169,
              -0.036805182695388794,
              0.007361036725342274,
              0.004623984452337027,
              -0.0066509367898106575,
              -0.022496500983834267,
              -0.6383832097053528,
              -0.026083674281835556,
              0.011181574314832687,
              -0.022229798138141632,
              0.015642203390598297,
              0.0144286984577775,
              0.010294782929122448,
              0.008834577165544033,
              -0.014375357888638973,
              0.016895713284611702,
              -0.0036705168895423412,
              -0.0242567490786314,
              -0.012355073355138302,
              -0.017922524362802505,
              -0.006840963382273912,
              -0.024243414402008057,
              0.01041479967534542,
              -0.013495233841240406,
              0.005487439688295126,
              0.0028720712289214134,
              -0.015815559774637222,
              0.03184448555111885,
              -0.023856693878769875,
              -0.017122412100434303,
              -0.011888341046869755,
              -0.010554819367825985,
              0.008614546619355679,
              -0.008961262181401253,
              -0.0003694270271807909,
              -0.004160585813224316,
              -0.011228247545659542,
              0.02679044008255005,
              0.026337042450904846,
              -0.008227825164794922,
              0.03952556475996971,
              0.010241442359983921,
              -0.007987791672348976,
              0.011988354846835136,
              -9.897226846078411e-05,
              0.0432327538728714,
              -0.004000563640147448,
              -0.03136441856622696,
              0.01694905385375023,
              -0.006024181842803955,
              0.006390899885445833,
              0.009814715944230556,
              -0.0008313670987263322,
              0.020896276459097862,
              0.00922129862010479,
              -0.009908062405884266,
              -0.0033804760314524174,
              0.011508287861943245,
              -0.0031371084041893482,
              -6.230043800314888e-05,
              0.016562333330512047,
              -0.014135324396193027,
              0.02120298519730568,
              -0.01952274888753891,
              -0.0004071406729053706,
              -0.006010846700519323,
              0.010381462052464485,
              0.003970559220761061,
              -0.004810677375644445,
              -0.03733859211206436,
              -0.005260740872472525,
              0.007574399933218956,
              0.0069809830747544765,
              0.004774005617946386,
              0.007561064790934324,
              -0.04285936802625656,
              0.007201014086604118,
              0.016322297975420952,
              0.0020636238623410463,
              -0.011081560514867306,
              -0.007681081537157297,
              0.026750434190034866,
              0.01434868760406971,
              -0.007314363494515419,
              -0.006454242393374443,
              0.013001831248402596,
              0.010374794714152813,
              -0.01533549278974533,
              0.010788186453282833,
              0.027577217668294907,
              -0.023349955677986145,
              0.009194628335535526,
              -0.004710663575679064,
              -0.011328262276947498,
              -0.00980804767459631,
              0.003322134492918849,
              -0.0023269944358617067,
              0.025150207802653313,
              -0.009768042713403702,
              -0.042939379811286926,
              0.020109498873353004,
              0.004330609925091267,
              0.004727332387119532,
              -0.00904794130474329,
              0.008327839896082878,
              -0.022469831630587578,
              -0.004547307267785072,
              -0.019882800057530403,
              0.027390524744987488,
              0.015042118728160858,
              0.00933464802801609,
              -0.006587594281882048,
              0.022136451676487923,
              0.0007267690380103886,
              -0.00228532194159925,
              0.0009918063879013062,
              -8.381388761335984e-05,
              0.007681081537157297,
              -0.017055734992027283,
              -0.01434868760406971,
              -0.003103770548477769,
              -0.03584504872560501,
              0.012601775117218494,
              0.0008517866372130811,
              -0.008134478703141212,
              -0.009741371497511864,
              0.0019202703842893243,
              0.024656806141138077,
              0.008567873388528824,
              0.004580644890666008,
              0.011261586099863052,
              0.015562191605567932,
              -0.009461332112550735,
              0.012568436563014984,
              -0.00733436644077301,
              0.002718716161325574,
              -0.00025170212029479444,
              0.002108630258589983,
              0.01781584322452545,
              -0.010228106752038002,
              0.02216312102973461,
              0.02409672737121582,
              0.0046206507831811905,
              -0.0006038350402377546,
              -0.020562896504998207,
              0.0012026693439111114,
              -0.011868338100612164,
              -0.012488425709307194,
              0.006294219754636288,
              -0.014415363781154156,
              -0.04717997834086418,
              -0.016722355037927628,
              0.02098962292075157,
              0.015135465189814568,
              0.0012943489709869027,
              -0.01685570739209652,
              0.008741230703890324,
              -0.018389256671071053,
              -0.012808470986783504,
              -0.0009709700825624168,
              -0.001995281083509326,
              -0.010181433521211147,
              -0.004353946540504694,
              -0.05952838435769081,
              -0.0096013518050313,
              -0.04533971846103668,
              0.006727613974362612,
              0.03352472186088562,
              0.0033721416257321835,
              -0.00436394801363349,
              -0.007681081537157297,
              -0.033258017152547836,
              -0.005140724126249552,
              0.03653847798705101,
              -0.015082123689353466,
              -0.027950603514909744,
              0.006354228127747774,
              -0.0450730137526989,
              0.04003230482339859,
              0.01612227037549019,
              -0.01740245148539543,
              -0.0044839647598564625,
              -0.014988777227699757,
              0.004083908628672361,
              -0.031604450196027756,
              -0.015095459297299385,
              0.0010584824485704303,
              0.020109498873353004,
              0.0012585106305778027,
              -0.017189087346196175,
              0.02555026486515999,
              -0.0022236465010792017,
              -0.012055031023919582,
              -0.0006071688258089125,
              -0.029390806332230568,
              0.047473352402448654,
              0.002633704338222742,
              -0.019389396533370018,
              0.009187960997223854,
              0.017509132623672485,
              0.012181716039776802,
              -0.004353946540504694,
              0.02888406813144684,
              -0.0010501479264348745,
              -0.018562614917755127,
              0.0138552850112319,
              -0.0036605154164135456,
              0.019189368933439255,
              -0.003178780898451805,
              -0.03485824167728424,
              -0.013308540917932987,
              -0.008447856642305851,
              0.010388129390776157,
              -0.03920552134513855,
              -0.0034504858776926994,
              0.00650758296251297,
              0.011181574314832687,
              -0.01901601068675518,
              -0.012581772170960903,
              -0.021629713475704193,
              0.01392196025699377,
              0.01990947127342224,
              -0.011021552607417107,
              0.002422007732093334,
              -0.0055174436420202255,
              0.013268535025417805,
              0.012075033970177174,
              -0.014655397273600101,
              0.024270085617899895,
              -0.002778724767267704,
              -0.005184063222259283,
              -0.0003223370586056262,
              0.0032287880312651396,
              0.005104052368551493,
              0.025950321927666664,
              -0.03421815112233162,
              0.0017044066917151213,
              0.03101770207285881,
              0.007821101695299149,
              0.019136028364300728,
              0.0048806872218847275,
              -0.0024486782494932413,
              0.015962248668074608,
              -0.010881532914936543,
              0.02883072756230831,
              -0.011514955200254917,
              0.017122412100434303,
              0.001248509157449007,
              -0.009361318312585354,
              -0.011134901084005833,
              0.02469681203365326,
              0.0333380289375782,
              0.03813870623707771,
              0.03349805250763893,
              -0.017482461407780647,
              -0.006940977647900581,
              -0.012628445401787758,
              -0.013788608834147453,
              -0.004540639463812113,
              0.009768042713403702,
              0.04120580479502678,
              -0.009981405921280384,
              0.015482180751860142,
              0.005974174477159977,
              0.0027420527767390013,
              0.020309526473283768,
              0.007141005713492632,
              0.016735689714550972,
              0.005267408676445484,
              -0.005374090280383825,
              0.032937973737716675,
              0.0012951823882758617,
              0.007094332482665777,
              -0.013895289972424507,
              -0.0007371871615760028,
              0.00447729742154479,
              -0.016749026253819466,
              -0.017282433807849884,
              -0.0014060313114896417,
              -0.025123538449406624,
              0.019056016579270363,
              0.02066957764327526,
              -0.014322017319500446,
              0.03405813127756119,
              0.0005142390727996826,
              0.040112316608428955,
              -0.012008357793092728,
              -0.02021618001163006,
              0.03336469829082489,
              -0.01080818846821785,
              -0.015002112835645676,
              -0.013681926764547825,
              -0.019762782379984856,
              0.014415363781154156,
              -0.005027374718338251,
              -0.0027270507998764515,
              -0.0025253556668758392,
              -0.0008492862689308822,
              -0.023283278569579124,
              0.0034904915373772383,
              0.00037067721132189035,
              0.020816264674067497,
              0.014282011426985264,
              -0.018295910209417343,
              0.004490632563829422,
              0.011054890230298042,
              0.003813870484009385,
              0.012795135378837585,
              -0.029497487470507622,
              -0.016562333330512047,
              0.021763065829873085,
              -0.01116157229989767,
              -0.03952556475996971,
              -0.021883081644773483,
              -0.026110343635082245,
              -0.02713715471327305,
              0.040085647255182266,
              -0.011888341046869755,
              0.022136451676487923,
              -0.005554115399718285,
              0.0242567490786314,
              0.02555026486515999,
              0.009407991543412209,
              -0.0034704888239502907,
              0.027630558237433434,
              0.011928346939384937,
              -0.005354087334126234,
              -0.010068085044622421,
              -0.0007050993153825402,
              0.013168521225452423,
              0.04304606094956398,
              0.03256458789110184,
              -0.00974803976714611,
              0.010461472906172276,
              -0.01704240031540394,
              0.012875146232545376,
              -0.03320467472076416,
              -0.02536357194185257,
              -0.0011718317400664091,
              -0.017895853146910667,
              -0.011321594007313251,
              -0.007901112549006939,
              -0.0026853783056139946,
              -0.005240737926214933,
              -0.016495656222105026,
              -0.007641076110303402,
              -0.018389256671071053,
              -0.0395522378385067,
              0.019856128841638565,
              0.008367844857275486,
              -0.006880969274789095,
              0.021096304059028625,
              0.030617645010352135,
              0.027310512959957123,
              0.014402028173208237,
              0.008981265127658844,
              0.02117631584405899,
              0.02196309342980385,
              -0.005140724126249552,
              -0.019216040149331093,
              0.0032204536255449057,
              -0.016402309760451317,
              0.00904794130474329,
              0.014602056704461575,
              0.00784110464155674,
              0.020549559965729713,
              -0.00772108742967248,
              -0.009901394136250019,
              0.028057284653186798,
              0.008907921612262726,
              0.011394938454031944,
              0.01216171309351921,
              0.007207681890577078,
              -0.014148659072816372,
              0.010581490583717823,
              -0.03504493460059166,
              -0.00036525976611301303,
              0.031791143119335175,
              -0.006360895931720734,
              0.007661079056560993,
              -0.009247968904674053,
              0.01084152702242136,
              -0.027630558237433434,
              -0.006020847707986832,
              0.029630839824676514,
              -0.0015085458289831877,
              -0.018362585455179214,
              -0.0003198367194272578,
              -0.0021619710605591536,
              -0.01740245148539543,
              -0.012695121578872204,
              -0.016375640407204628,
              0.004904023837298155,
              -0.002453678986057639,
              0.002502019051462412,
              -0.0025136873591691256,
              -0.03085767850279808,
              -0.027470534667372704,
              -0.028777386993169785,
              0.019642766565084457,
              -0.001286847866140306,
              -0.024176737293601036,
              -0.049313612282276154,
              -0.014695403166115284,
              0.022069774568080902,
              0.02336329035460949,
              0.017029065638780594,
              -0.017415786162018776,
              -0.02663041651248932,
              -0.00934798363596201,
              0.007987791672348976,
              -0.021736394613981247,
              0.004193923901766539,
              -0.02400338090956211,
              -0.013008498586714268,
              -0.011534958146512508,
              -0.028937408700585365,
              -0.004750669002532959,
              -0.03299131244421005,
              0.013828613795340061,
              -0.009294642135500908,
              0.01660233736038208,
              0.018709301948547363,
              -0.019416067749261856,
              0.017189087346196175,
              0.01101488433778286,
              0.008341174572706223,
              0.011261586099863052,
              0.02142968401312828,
              -0.030937690287828445,
              0.018055876716971397,
              -0.010294782929122448,
              -0.023549983277916908,
              -0.017122412100434303,
              0.006437573116272688,
              0.009481335058808327,
              0.00430060550570488,
              -0.02920411340892315,
              0.011948348954319954,
              -0.014735408127307892,
              0.02057623118162155,
              -0.02467014081776142,
              -7.1364214818459e-05,
              0.005497440695762634,
              0.03685852512717247,
              0.016482321545481682,
              0.016895713284611702,
              0.022803211584687233,
              0.0029120768886059523,
              -0.00465065473690629,
              -0.008261163718998432,
              -0.02492351084947586,
              0.018562614917755127,
              0.0194294024258852,
              -0.010268112644553185,
              0.009821383282542229,
              0.032191202044487,
              -0.005837488919496536,
              -0.035951729863882065,
              0.02536357194185257,
              -0.04083241894841194,
              0.018682630732655525,
              -0.0010476475581526756,
              -0.017642484977841377,
              -0.03285796195268631,
              -0.02396337501704693,
              0.0066842748783528805,
              0.0009509672527201474,
              -0.04283269867300987,
              0.008201154880225658,
              0.0008651218377053738,
              0.0027320515364408493,
              0.0012268394930288196,
              -0.017975864931941032,
              0.023629995062947273,
              -0.02835065871477127,
              -0.018842654302716255,
              0.004237263463437557,
              0.014508710242807865,
              0.015122129581868649,
              -0.01768249087035656,
              0.013508569449186325,
              -0.033258017152547836,
              -0.013895289972424507,
              0.009001268073916435,
              -0.018109217286109924,
              0.004373949486762285,
              0.018055876716971397,
              0.007707752287387848,
              0.01534882839769125,
              0.03581837937235832,
              0.0036538478452712297,
              0.01958942599594593,
              0.021763065829873085,
              -0.004887355025857687,
              0.009214631281793118,
              -0.023856693878769875,
              0.009614687412977219,
              -0.0017619148129597306,
              0.011034887284040451,
              0.00962135475128889,
              -0.01570887863636017,
              -0.01452204491943121,
              -0.0052207354456186295,
              0.019802788272500038,
              0.010608160868287086,
              -0.007567732594907284,
              -0.018042540177702904,
              -0.02867070399224758,
              -0.023403296247124672,
              -0.024776821956038475,
              0.0019369394285604358,
              -0.003133774735033512,
              -0.005497440695762634,
              -0.05158059671521187,
              -0.01812255196273327,
              0.0008013628539629281,
              0.0031587781850248575,
              0.008267831057310104,
              0.0008297002059407532,
              0.020389538258314133,
              0.0036538478452712297,
              -0.007654411252588034,
              0.009061275981366634,
              0.0030070901848375797,
              -0.014468704350292683,
              -0.007314363494515419,
              -0.02752387709915638,
              -0.011081560514867306,
              0.03640512749552727,
              0.021149644628167152,
              -0.00500737177208066,
              0.00585749140009284,
              -0.010301451198756695,
              -0.001103488728404045,
              0.013228530064225197,
              0.0029070761520415545,
              -0.013155185617506504,
              -0.025243554264307022,
              -0.014708737842738628,
              -0.021389678120613098,
              -0.030804337933659554,
              -0.012588439509272575,
              0.02904408983886242,
              0.013721932657063007,
              -0.008114475756883621,
              -0.029124101623892784,
              0.032964643090963364,
              -0.0002389919973211363,
              -0.0033037986140698195,
              0.004023900255560875,
              0.024883504956960678,
              0.032937973737716675,
              -0.001675235922448337,
              0.011381602846086025,
              0.016242288053035736,
              0.0022786541376262903,
              -0.006347560789436102,
              0.0021619710605591536,
              0.004997370298951864,
              -0.03035094030201435,
              -0.000267537689069286,
              -0.009788044728338718,
              -0.0005438266089186072,
              -0.010421467944979668,
              0.016909047961235046,
              0.01042813528329134,
              -0.03117772378027439,
              -0.02053622528910637,
              0.014322017319500446,
              -0.007281025405973196,
              0.023923369124531746,
              -0.002218645764514804,
              -0.019322721287608147,
              -0.001751913339830935,
              0.013868619687855244,
              -0.01653566211462021,
              0.02869737520813942,
              0.002815396524965763,
              0.00013929045235272497,
              0.019096022471785545,
              0.02708381414413452,
              -0.010001408867537975,
              0.022816546261310577,
              -0.01444203406572342,
              0.008687890134751797,
              0.00265037314966321,
              0.006967647932469845,
              -0.020362867042422295,
              -0.011654974892735481,
              0.006797623820602894,
              0.020749589428305626,
              -0.012535098940134048,
              0.017122412100434303,
              0.007974456064403057,
              -0.011614969000220299,
              0.00662760017439723,
              0.009134619496762753,
              -0.0014418697683140635,
              0.03083100914955139,
              -0.02787059172987938,
              -0.005124054849147797,
              -0.0007338533760048449,
              0.004497299902141094,
              0.017375780269503593,
              -0.013788608834147453,
              0.001751913339830935,
              0.0007451049168594182,
              -0.017029065638780594,
              0.01570887863636017,
              0.025563599541783333,
              0.024150067940354347,
              -0.023763347417116165,
              0.0072743576020002365,
              0.005717471707612276,
              -0.0014535380760207772,
              -0.002433676039800048,
              0.006204206962138414,
              0.01993614062666893,
              -0.03555167466402054,
              -0.013788608834147453,
              0.034111469984054565,
              0.014282011426985264,
              0.020149504765868187,
              -0.00895459484308958,
              -0.011481616646051407,
              -0.014455368742346764,
              0.02984420396387577,
              -0.04373949393630028,
              0.007947785779833794,
              0.012395079247653484,
              -0.013681926764547825,
              -0.013208527117967606,
              -0.018269238993525505,
              -0.015575527213513851,
              -0.01293515507131815,
              0.0007967788842506707,
              -0.0015035450924187899,
              -0.022229798138141632,
              -0.0019869464449584484,
              -0.005197398830205202,
              -0.005670798476785421,
              0.007994459010660648,
              -0.022136451676487923,
              -0.020896276459097862,
              0.018936000764369965,
              0.0206962488591671,
              -0.019376061856746674,
              -0.01366859208792448,
              -0.012435084208846092,
              0.0029220781289041042,
              -0.00328212883323431,
              0.01271512359380722,
              -0.027977272868156433,
              -0.01452204491943121,
              0.0016694017685949802,
              0.01758914440870285,
              -0.011928346939384937,
              -0.04101911187171936,
              -0.0011543292785063386,
              -0.006974315736442804,
              0.009461332112550735,
              -0.0018819316755980253,
              -0.015028783120214939,
              0.022083109244704247,
              -0.01878931373357773,
              -0.008787903934717178,
              0.02591031603515148,
              -0.00884124543517828,
              -0.010481475852429867,
              0.003190449206158519,
              -0.00792111549526453,
              0.013341878540813923,
              0.02479015849530697,
              -0.0097147012129426,
              -0.034964922815561295,
              -0.011754988692700863,
              -0.003883880330249667,
              -0.017882518470287323,
              -0.008487861603498459,
              -0.015548856928944588,
              0.051633939146995544,
              0.031444426625967026,
              -0.010228106752038002,
              -0.010674837045371532,
              0.0024153401609510183,
              -0.025830304250121117,
              0.003423815593123436,
              -0.005870827008038759,
              0.0070809973403811455,
              0.0010659834370017052,
              -0.021229656413197517,
              -0.0023453303147107363,
              0.017735831439495087,
              -0.0004996536881662905,
              -0.004964032210409641,
              -0.008261163718998432,
              -0.0018719303188845515,
              -0.021002957597374916,
              -0.01841592602431774,
              0.00025232721236534417,
              -0.012681785970926285,
              -0.03933887183666229,
              0.014708737842738628,
              0.028617363423109055,
              -0.006250880192965269,
              0.0172424279153347,
              0.04387284442782402,
              -0.00871456041932106,
              0.001788585213944316,
              -0.00974803976714611,
              0.016655679792165756,
              0.02186974696815014,
              0.017389114946126938,
              0.011168239638209343,
              -0.026270367205142975,
              0.01154162548482418,
              0.04171254113316536,
              -0.0037538621108978987,
              -0.02050955593585968,
              0.002850401448085904,
              0.0047906748950481415,
              0.00784110464155674,
              -0.012101704254746437,
              -0.021856412291526794,
              -0.0020186176989227533,
              0.0019469409016892314,
              -0.019856128841638565,
              0.005077381618320942,
              0.01781584322452545,
              -0.02888406813144684,
              0.0073076956905424595,
              0.0028604029212146997,
              0.00657092547044158,
              -0.03157778084278107,
              0.017722496762871742,
              -0.010734844952821732,
              -0.0012285063276067376,
              0.01917603425681591,
              -0.0012593440478667617,
              0.0016493989387527108,
              -0.016655679792165756,
              0.005594121292233467,
              0.009928064420819283,
              -0.006174203008413315,
              -0.03301798179745674,
              0.001644398202188313,
              -0.023216603323817253,
              0.008587876334786415,
              0.007734422571957111,
              -0.044966332614421844,
              -0.006710945162922144,
              -0.0020486218854784966,
              -0.019882800057530403,
              -0.017255762591958046,
              -0.004980701487511396,
              0.000838034669868648,
              -0.005024041049182415,
              0.01698905974626541,
              -0.0060508521273732185,
              -0.016709020361304283,
              -0.01949607953429222,
              0.005604122765362263,
              0.02142968401312828,
              0.00415058434009552,
              0.02695046178996563,
              0.228085458278656,
              -0.029284123331308365,
              -0.00036505141179077327,
              0.00515739293769002,
              0.003147109877318144,
              0.019802788272500038,
              -0.0013276869431138039,
              -0.0026303704362362623,
              -0.009774710051715374,
              -0.008867915719747543,
              0.04149917885661125,
              0.006914306897670031,
              -0.01312851533293724,
              -0.0017452457686886191,
              0.018242569640278816,
              -0.02336329035460949,
              -0.014148659072816372,
              -0.03421815112233162,
              -0.012601775117218494,
              0.01242841687053442,
              0.005134056322276592,
              0.01216171309351921,
              0.002530356403440237,
              -0.0008076137164607644,
              0.028937408700585365,
              0.004067239351570606,
              0.0032337887678295374,
              -0.0024486782494932413,
              0.021149644628167152,
              0.004924026783555746,
              -0.006197539623826742,
              -0.005454101599752903,
              0.011701648123562336,
              0.02085627056658268,
              -0.010248109698295593,
              -0.0009134619613178074,
              -0.029977554455399513,
              0.003218786558136344,
              0.018749307841062546,
              -0.017122412100434303,
              0.027657227590680122,
              -0.007541061844676733,
              0.010268112644553185,
              -0.04133915528655052,
              0.008041132241487503,
              -0.0009776377119123936,
              -0.004910691641271114,
              -0.006454242393374443,
              -0.004447293002158403,
              0.02202976867556572,
              -0.008947926573455334,
              0.02545691840350628,
              0.02098962292075157,
              0.005194064695388079,
              0.008361177518963814,
              0.00035546673461794853,
              -0.009854720905423164,
              0.009041273035109043,
              0.027977272868156433,
              0.005440766457468271,
              -0.008961262181401253,
              -0.0001698155829217285,
              -0.0010168098378926516,
              0.02301657572388649,
              -0.01312851533293724,
              0.022109780460596085,
              -0.02612367831170559,
              0.0231099221855402,
              0.0356316864490509,
              0.021509695798158646,
              0.007054326590150595,
              -0.023790016770362854,
              -0.007741090375930071,
              0.014895430766046047,
              -0.04389951750636101,
              -0.03424482420086861,
              0.051313892006874084,
              0.012488425709307194,
              0.039472226053476334,
              0.03003089688718319,
              0.009701366536319256,
              0.001888599363155663,
              -0.018175892531871796,
              -0.015495515428483486,
              -0.0010159764206036925,
              -0.04376616328954697,
              0.029790861532092094,
              -0.023056579753756523,
              0.001105155679397285,
              -0.024736817926168442,
              0.003963891416788101,
              -0.008494529873132706,
              -0.01133492961525917,
              -0.007301028352230787,
              0.008241160772740841,
              0.013481899164617062,
              0.017162416130304337,
              0.012588439509272575,
              -0.010454805567860603,
              -0.009167958050966263,
              -0.02545691840350628,
              0.06902305036783218,
              -0.0025470254477113485,
              0.00812781136482954,
              0.002713715424761176,
              0.01816255785524845,
              -0.013681926764547825,
              0.014055312611162663,
              0.008461191318929195,
              -0.011741654016077518,
              0.016802366822957993,
              -0.03139108791947365,
              0.008001127280294895,
              -0.0035538338124752045,
              0.011814997531473637,
              0.012115039862692356,
              0.029577499255537987,
              0.003673850791528821,
              0.018269238993525505,
              -0.0144286984577775,
              0.01733577437698841,
              -0.022403154522180557,
              0.009834717959165573,
              0.015162135474383831,
              0.005960839334875345,
              -0.004463961813598871,
              -0.018215898424386978,
              0.010241442359983921,
              -0.03221787139773369,
              -0.0032254543621093035,
              -0.014868760481476784,
              -0.02552359364926815,
              0.022109780460596085,
              -0.02835065871477127,
              -0.02536357194185257,
              -0.003680518362671137,
              -0.001108489464968443,
              -0.024963514879345894,
              -0.02416340261697769,
              0.02170972339808941,
              0.0020536226220428944,
              0.008201154880225658,
              0.016842372715473175,
              -7.469802221748978e-05,
              -0.020136170089244843,
              0.002567028161138296,
              -0.008741230703890324,
              -0.0031604450196027756,
              0.00635756179690361,
              -0.02727050706744194,
              -0.008581208065152168,
              -0.017882518470287323,
              -0.011448279023170471,
              -0.0006538420566357672,
              0.02216312102973461,
              0.010534817352890968,
              -0.006407569162547588,
              -0.008154481649398804,
              0.00683429604396224,
              0.005110719706863165,
              -0.02209644578397274,
              -0.021896416321396828,
              0.0017252429388463497,
              -0.0078077660873532295,
              -0.009794712997972965,
              -0.0016910715494304895,
              -0.16845038533210754,
              -0.0010668168542906642,
              0.025443583726882935,
              -0.046779923141002655,
              0.011995022185146809,
              0.0024936844129115343,
              0.02479015849530697,
              -0.030110906809568405,
              -0.05054045096039772,
              0.014602056704461575,
              -0.00010371252574259415,
              0.0020119501277804375,
              0.0016485655214637518,
              -0.009794712997972965,
              -0.03248457610607147,
              0.006497581955045462,
              -0.0242567490786314,
              0.01818922907114029,
              0.014588721096515656,
              -0.0027403859421610832,
              0.0377119779586792,
              -0.03800535202026367,
              0.026297036558389664,
              -0.011694980785250664,
              0.03547166287899017,
              -0.0014026975259184837,
              0.013775273226201534,
              0.019189368933439255,
              -0.01246175542473793,
              -0.04491299018263817,
              -0.0012218387564644217,
              -0.006454242393374443,
              0.03488491475582123,
              -0.022043105214834213,
              -0.009341315366327763,
              0.015148799866437912,
              -0.010774850845336914,
              -0.0057208058424293995,
              -0.00950800534337759,
              0.006914306897670031,
              0.029897544533014297,
              0.0073410337790846825,
              0.0043272762559354305,
              -0.008227825164794922,
              -0.02479015849530697,
              0.0020919612143188715,
              0.029124101623892784,
              -0.012181716039776802,
              0.004500634036958218,
              -0.011054890230298042,
              0.008987932465970516,
              -0.0006059186416678131,
              -0.002218645764514804,
              0.011148236691951752,
              0.0030404282733798027,
              0.003563835285604,
              0.028430670499801636,
              0.018469268456101418,
              -0.011301591992378235,
              -0.017189087346196175,
              -0.009121284820139408,
              -0.007814433425664902,
              0.0042939381673932076,
              0.009814715944230556,
              -0.015922242775559425,
              -0.011654974892735481,
              -0.009501338005065918,
              -0.012868478894233704,
              -0.007901112549006939,
              0.021002957597374916,
              -0.031444426625967026,
              -0.011895008385181427,
              -0.01071484200656414,
              -0.0052774096839129925,
              0.005614123772829771,
              0.017855847254395485,
              -0.017802506685256958,
              -0.022043105214834213,
              0.00307876686565578,
              -0.002470347797498107,
              -0.02237648516893387,
              0.021536367014050484,
              -0.0094279944896698,
              0.0033371367026120424,
              0.005594121292233467,
              0.016775695607066154,
              -0.0004917359328828752,
              0.02492351084947586,
              -0.019816124811768532,
              0.0009793045464903116,
              0.039125509560108185,
              -0.012721791863441467,
              0.0025470254477113485,
              -0.0034138141199946404,
              -0.010041413828730583,
              0.011214912869036198,
              0.0067742872051894665,
              0.0020119501277804375,
              0.004130581859499216,
              -0.018269238993525505,
              0.010754847899079323,
              -0.0027687232941389084,
              -0.015242146328091621,
              0.020136170089244843,
              0.03507160767912865,
              0.016175610944628716,
              -0.015562191605567932,
              0.003032093634828925,
              0.04336610808968544,
              0.00842785369604826,
              -0.01685570739209652,
              -0.004907357972115278,
              0.0009042940218932927,
              -0.0053107477724552155,
              -0.004023900255560875,
              0.03003089688718319,
              -0.0006146698724478483,
              -0.017469126731157303,
              -0.0029120768886059523,
              -0.019322721287608147,
              0.048806872218847275,
              -0.028404001146554947,
              -0.03237789496779442,
              0.029257453978061676,
              -0.011568295769393444,
              -0.0314977690577507,
              -0.11649640649557114,
              -0.025030191987752914,
              -0.011788327246904373,
              0.01285514421761036,
              -0.004233929794281721,
              0.009074611589312553,
              0.015922242775559425,
              0.024203408509492874,
              -0.03248457610607147,
              0.04189923405647278,
              -0.00016929468256421387,
              -0.01910935714840889,
              0.01704240031540394,
              -0.015175470151007175,
              -0.0073076956905424595,
              0.001351857092231512,
              0.012901817448437214,
              0.010488144122064114,
              -0.01071484200656414,
              0.028083955869078636,
              0.012068366631865501,
              -0.031071042641997337,
              0.021483026444911957,
              -0.016895713284611702,
              -0.004440625198185444,
              0.0018135887803509831,
              -0.017882518470287323,
              0.008594543673098087,
              -0.0010759849101305008,
              -0.022496500983834267,
              0.016322297975420952,
              -0.036778513342142105,
              0.0071143354289233685,
              -0.012515095993876457,
              0.01876264251768589,
              -0.01033478882163763,
              0.003907216712832451,
              -0.021483026444911957,
              -0.016802366822957993,
              -0.006744283251464367,
              -0.011421608738601208,
              0.005090716760605574,
              0.0011609968496486545,
              0.0032421231735497713,
              -0.01736244559288025,
              -0.0172424279153347,
              -0.022109780460596085,
              0.027030473574995995,
              0.0031721133273094893,
              -0.03235122188925743,
              -0.028270648792386055,
              -0.023643329739570618,
              -0.05024707689881325,
              -0.0038005353417247534,
              0.0024736816994845867,
              -0.014055312611162663,
              -0.001976944971829653,
              0.0036371788010001183,
              -0.01090153492987156,
              -0.005967507138848305,
              0.003607174614444375,
              -0.009701366536319256,
              -0.011088227853178978,
              0.029790861532092094,
              0.01476207934319973,
              -0.005707470700144768,
              -0.010001408867537975,
              -0.03240456432104111,
              -0.006384232547134161,
              -0.026110343635082245,
              -0.0015960581367835402,
              0.00963469035923481,
              0.0008342841756530106,
              0.007801098749041557,
              -0.03565835580229759,
              -0.0012410080526024103,
              -0.018709301948547363,
              -0.01910935714840889,
              -0.0038038690108805895,
              0.010368126444518566,
              -0.026577075943350792,
              -0.003317133756354451,
              0.008401183411478996,
              -0.024803493171930313,
              -0.013721932657063007,
              -0.0030054233502596617,
              -0.006550922524183989,
              -0.005297412630170584,
              -0.002998755546286702,
              -0.02708381414413452,
              0.0029920879751443863,
              0.02933746576309204,
              -0.011468281969428062,
              0.000514655839651823,
              -0.00992139708250761,
              0.021096304059028625,
              -0.0028037282172590494,
              -0.023816687986254692,
              0.0264437235891819,
              0.02308325096964836,
              -0.017949193716049194,
              -0.005944170523434877,
              -0.02301657572388649,
              0.0164423156529665,
              -0.009501338005065918,
              -0.008721228688955307,
              0.002960416954010725,
              -0.011468281969428062,
              0.04200591519474983,
              0.016709020361304283,
              -0.022563178092241287,
              0.008581208065152168,
              -0.009968070313334465,
              0.021136309951543808,
              0.0014035309432074428,
              -0.027217166498303413,
              -0.014922101981937885,
              -0.022403154522180557,
              0.014735408127307892,
              -0.022936563938856125,
              0.022149786353111267,
              0.01940273307263851,
              -0.005264074541628361,
              -0.014868760481476784,
              0.021029628813266754,
              0.0015785556752234697,
              0.007894445210695267,
              -0.002181974006816745,
              -0.00782776903361082,
              0.021056298166513443,
              -0.010708174668252468,
              -0.022176455706357956,
              0.031071042641997337,
              -0.01892266422510147,
              0.005000704433768988,
              0.014255341142416,
              -0.002030286006629467,
              0.015988918021321297,
              0.001396863372065127,
              -0.004817345179617405,
              0.022523172199726105,
              -0.006697610020637512,
              -0.005140724126249552,
              -0.02260318398475647,
              0.029070761054754257,
              -0.01850927248597145,
              0.013388551771640778,
              -0.007467718329280615,
              -0.022456495091319084,
              -0.0038738788571208715,
              0.01860261894762516,
              -0.018709301948547363,
              0.01704240031540394,
              0.0144286984577775,
              0.01222172100096941,
              -0.031444426625967026,
              -0.0016002253396436572,
              -0.004183922428637743,
              -0.016682349145412445,
              0.0001160580141004175,
              0.01542883925139904,
              0.003500493010506034,
              0.02701713889837265,
              0.004593980498611927,
              -0.009021270088851452,
              -0.010654834099113941,
              0.004697328433394432,
              0.013208527117967606,
              0.003597173374146223,
              0.013828613795340061,
              -0.003433816833421588,
              -0.012121707201004028,
              7.297902629943565e-05,
              0.011188242584466934,
              0.0011493285419419408,
              0.0016310630599036813,
              0.0344848558306694,
              0.0188559889793396,
              0.020522890612483025,
              -0.0037671972531825304,
              -0.00636422960087657,
              0.0476067028939724,
              0.014575386419892311,
              -0.019696107134222984,
              -0.016455650329589844,
              0.009374653920531273,
              0.01374860294163227,
              0.00465732254087925,
              0.006860966328531504,
              -0.02559027075767517,
              -0.011908343993127346,
              0.005557449534535408,
              0.007961121387779713,
              0.008087805472314358,
              -0.01366859208792448,
              0.03723191097378731,
              0.0037638633511960506,
              0.01720242202281952,
              0.0061842044815421104,
              0.030137578025460243,
              0.02304324507713318,
              0.006227543577551842,
              0.00256369449198246,
              -0.003430483164265752,
              -0.00278539233841002,
              0.010514814406633377,
              -0.004057237878441811,
              0.027630558237433434,
              -0.011401605792343616,
              -0.01742912083864212,
              0.019376061856746674,
              0.027230501174926758,
              0.0093146450817585,
              -0.008567873388528824,
              -0.01745579205453396,
              -0.007207681890577078,
              -0.017309105023741722,
              0.01660233736038208,
              -0.007327698636800051,
              -0.024870168417692184,
              -0.02463013492524624,
              0.005114053376019001,
              0.011975020170211792,
              -0.015562191605567932,
              0.049126919358968735,
              -0.013468563556671143,
              0.019829459488391876,
              -0.009454664774239063,
              0.025656946003437042,
              -0.020722918212413788,
              0.013681926764547825,
              0.02987087331712246,
              0.01765581965446472,
              -0.006704277358949184,
              0.010174766182899475,
              0.001715241582132876,
              -0.007134337909519672,
              -0.011434943415224552,
              0.02168305404484272,
              0.02603033185005188,
              -0.019949475303292274,
              0.07227684557437897,
              0.024483447894454002,
              0.007394374813884497,
              -0.0018469267524778843,
              -0.02349664270877838,
              0.022109780460596085,
              0.005240737926214933,
              0.015988918021321297,
              -0.023643329739570618,
              0.0017569140763953328,
              0.004317274782806635,
              0.0042572664096951485,
              0.0002967084583360702,
              -0.02361665852367878,
              0.0012376742670312524,
              -0.0051207211799919605,
              0.013175188563764095,
              0.003943888936191797,
              0.003112104954198003,
              0.00040609887219034135,
              0.03669850155711174,
              -0.009928064420819283,
              0.02244316041469574,
              0.0023703337647020817,
              -0.023643329739570618,
              0.004730666056275368,
              0.009594684466719627,
              0.03152443841099739,
              -0.0029920879751443863,
              -0.03699187561869621,
              0.016869042068719864,
              0.013301873579621315,
              -0.029470816254615784,
              -0.0031887823715806007,
              0.002603699918836355,
              -0.011301591992378235,
              -0.012708456255495548,
              -0.011701648123562336,
              0.0018019204726442695,
              0.012915152125060558,
              0.020749589428305626,
              0.03573836758732796,
              -0.021069634705781937,
              -0.023723341524600983,
              -0.0015610532136633992,
              0.002572028897702694,
              -0.01204836368560791,
              -0.02200309932231903,
              -0.006360895931720734
            ],
            "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
            "indexType": "cogsearchvs",
            "question": "What is the advantage of fine-tuning BERT over using feature-based approaches?"
          },
          "output": {
            "jsonAnswer": {
              "data_points": [
                "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
                "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
              ],
              "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
              "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.']<br>        Question: What is the advantage of fine-tuning BERT over using feature-based approaches?<br>        ",
              "sources": "\nBert.pdf",
              "nextQuestions": "<>\n<>\n<>",
              "error": ""
            },
            "existingAnswer": true
          },
          "start_time": 1697137315.800065,
          "end_time": 1697137317.81253,
          "error": null,
          "children": null,
          "node_name": "check_cache_answer"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "[2023-10-12T14:01:57+0000] Search index aoaikb already exists\n[2023-10-12T14:01:57+0000] KB Search Count: 1\n",
        "stderr": ""
      },
      "system_metrics": {
        "duration": 2.021571
      },
      "result": {
        "jsonAnswer": {
          "data_points": [
            "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
            "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
            "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
          ],
          "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
          "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.']<br>        Question: What is the advantage of fine-tuning BERT over using feature-based approaches?<br>        ",
          "sources": "\nBert.pdf",
          "nextQuestions": "<>\n<>\n<>",
          "error": ""
        },
        "existingAnswer": true
      }
    },
    {
      "node": "followup_questions",
      "flow_run_id": "4d4879be-de55-475e-85ef-cc3de9de4117",
      "run_id": "4d4879be-de55-475e-85ef-cc3de9de4117_followup_questions_0",
      "status": "Completed",
      "inputs": {
        "conn": "entaoai",
        "embeddedQuestion": [
          -0.016869042068719864,
          -0.0003010840737260878,
          0.022069774568080902,
          -0.01818922907114029,
          -0.0022569845896214247,
          0.013475230894982815,
          -0.0065809269435703754,
          0.004023900255560875,
          -0.032884631305933,
          -0.019216040149331093,
          0.02651040069758892,
          0.030404282733798027,
          -0.024683475494384766,
          0.0007167676230892539,
          -0.018149223178625107,
          -0.015762219205498695,
          0.036565151065588,
          0.028804056346416473,
          0.005427430849522352,
          -0.025256890803575516,
          -0.011601634323596954,
          -0.009081278927624226,
          -0.024976851418614388,
          -0.011588298715651035,
          -0.02108296938240528,
          -0.0024053386878222227,
          0.045446399599313736,
          -0.020642906427383423,
          -0.01844259724020958,
          0.0037738648243248463,
          0.026403717696666718,
          0.02196309342980385,
          -0.016562333330512047,
          -0.02032286301255226,
          -0.011454946361482143,
          0.009701366536319256,
          -0.0029054090846329927,
          -0.01225505955517292,
          0.024843499064445496,
          -0.01254176627844572,
          0.01114156935364008,
          0.032751280814409256,
          0.011975020170211792,
          -0.015215476043522358,
          -0.0032737944275140762,
          0.0264437235891819,
          0.022069774568080902,
          -0.00033192173577845097,
          -0.03133774548768997,
          0.020056158304214478,
          0.002888740273192525,
          0.030564304441213608,
          -0.016975723206996918,
          0.003323801327496767,
          -0.05198065564036369,
          -0.007454383186995983,
          -0.006334225181490183,
          0.005664131138473749,
          -0.0017552472418174148,
          -0.025723623111844063,
          0.002136967610567808,
          0.006624266039580107,
          -0.00904794130474329,
          0.01246175542473793,
          -0.012735126540064812,
          -0.013095177710056305,
          -0.005047377664595842,
          0.022816546261310577,
          0.010688171721994877,
          -0.0024503450840711594,
          0.012088368646800518,
          0.02571028843522072,
          -0.01602892391383648,
          -0.006470911204814911,
          0.027577217668294907,
          -0.014828754588961601,
          -0.0028720712289214134,
          0.021576372906565666,
          0.01041479967534542,
          -0.0031987838447093964,
          -0.0014477039221674204,
          0.006234211381524801,
          -0.01090153492987156,
          0.014268675819039345,
          0.02060290239751339,
          0.014735408127307892,
          0.0026270365342497826,
          0.0010318119311705232,
          0.003170446492731571,
          0.015802225098013878,
          0.005537446588277817,
          0.018749307841062546,
          0.03416481241583824,
          0.02221646159887314,
          0.011074893176555634,
          -0.0029904211405664682,
          -0.005217401310801506,
          0.014562050811946392,
          -0.00028566524269990623,
          0.004353946540504694,
          -0.009027938358485699,
          0.009401324205100536,
          -0.029817532747983932,
          -0.011661642231047153,
          -0.004167253617197275,
          0.009067943319678307,
          0.018629290163517,
          -0.005494107026606798,
          -0.0012393412180244923,
          -0.0036205099895596504,
          -0.03037761151790619,
          0.014482039958238602,
          -0.014588721096515656,
          -0.014175329357385635,
          -0.0054140957072377205,
          0.0022019767202436924,
          0.0005113219958730042,
          -0.010294782929122448,
          -0.01401530671864748,
          0.004560642410069704,
          0.023629995062947273,
          -1.4324934454634786e-05,
          0.018815983086824417,
          -0.017189087346196175,
          0.01698905974626541,
          -0.019322721287608147,
          -0.015028783120214939,
          -0.017975864931941032,
          -0.007247687317430973,
          -0.009167958050966263,
          0.02002948708832264,
          0.021403014659881592,
          0.00963469035923481,
          -0.02092294581234455,
          -0.007207681890577078,
          0.005680799949914217,
          0.01781584322452545,
          -0.016815701499581337,
          -0.014482039958238602,
          -0.019189368933439255,
          0.023830022662878036,
          0.016175610944628716,
          -0.02591031603515148,
          -0.03267126902937889,
          -0.05371423065662384,
          0.02012283354997635,
          -0.000785110576543957,
          0.030564304441213608,
          0.008974596858024597,
          -0.022456495091319084,
          0.012995163910090923,
          -0.029284123331308365,
          -0.012835141271352768,
          0.03224454075098038,
          0.002288655610755086,
          0.0055507817305624485,
          0.0018986007198691368,
          0.006704277358949184,
          -0.014161994680762291,
          -0.0231099221855402,
          0.01802920550107956,
          0.017975864931941032,
          0.023736676201224327,
          -0.006900971755385399,
          0.017149081453680992,
          0.021949758753180504,
          0.0017919189995154738,
          -0.005604122765362263,
          0.017375780269503593,
          -0.001429367926903069,
          0.005760811269283295,
          0.030110906809568405,
          -0.014455368742346764,
          0.007487721275538206,
          -0.0011893342016264796,
          0.01284180860966444,
          0.03205784782767296,
          -0.020056158304214478,
          -0.004427290055900812,
          -0.03920552134513855,
          0.023029910400509834,
          0.010188101790845394,
          0.03371141478419304,
          -0.013248532079160213,
          -0.018642624840140343,
          0.008454523980617523,
          0.013828613795340061,
          -0.017135746777057648,
          0.007401042152196169,
          -0.036805182695388794,
          0.007361036725342274,
          0.004623984452337027,
          -0.0066509367898106575,
          -0.022496500983834267,
          -0.6383832097053528,
          -0.026083674281835556,
          0.011181574314832687,
          -0.022229798138141632,
          0.015642203390598297,
          0.0144286984577775,
          0.010294782929122448,
          0.008834577165544033,
          -0.014375357888638973,
          0.016895713284611702,
          -0.0036705168895423412,
          -0.0242567490786314,
          -0.012355073355138302,
          -0.017922524362802505,
          -0.006840963382273912,
          -0.024243414402008057,
          0.01041479967534542,
          -0.013495233841240406,
          0.005487439688295126,
          0.0028720712289214134,
          -0.015815559774637222,
          0.03184448555111885,
          -0.023856693878769875,
          -0.017122412100434303,
          -0.011888341046869755,
          -0.010554819367825985,
          0.008614546619355679,
          -0.008961262181401253,
          -0.0003694270271807909,
          -0.004160585813224316,
          -0.011228247545659542,
          0.02679044008255005,
          0.026337042450904846,
          -0.008227825164794922,
          0.03952556475996971,
          0.010241442359983921,
          -0.007987791672348976,
          0.011988354846835136,
          -9.897226846078411e-05,
          0.0432327538728714,
          -0.004000563640147448,
          -0.03136441856622696,
          0.01694905385375023,
          -0.006024181842803955,
          0.006390899885445833,
          0.009814715944230556,
          -0.0008313670987263322,
          0.020896276459097862,
          0.00922129862010479,
          -0.009908062405884266,
          -0.0033804760314524174,
          0.011508287861943245,
          -0.0031371084041893482,
          -6.230043800314888e-05,
          0.016562333330512047,
          -0.014135324396193027,
          0.02120298519730568,
          -0.01952274888753891,
          -0.0004071406729053706,
          -0.006010846700519323,
          0.010381462052464485,
          0.003970559220761061,
          -0.004810677375644445,
          -0.03733859211206436,
          -0.005260740872472525,
          0.007574399933218956,
          0.0069809830747544765,
          0.004774005617946386,
          0.007561064790934324,
          -0.04285936802625656,
          0.007201014086604118,
          0.016322297975420952,
          0.0020636238623410463,
          -0.011081560514867306,
          -0.007681081537157297,
          0.026750434190034866,
          0.01434868760406971,
          -0.007314363494515419,
          -0.006454242393374443,
          0.013001831248402596,
          0.010374794714152813,
          -0.01533549278974533,
          0.010788186453282833,
          0.027577217668294907,
          -0.023349955677986145,
          0.009194628335535526,
          -0.004710663575679064,
          -0.011328262276947498,
          -0.00980804767459631,
          0.003322134492918849,
          -0.0023269944358617067,
          0.025150207802653313,
          -0.009768042713403702,
          -0.042939379811286926,
          0.020109498873353004,
          0.004330609925091267,
          0.004727332387119532,
          -0.00904794130474329,
          0.008327839896082878,
          -0.022469831630587578,
          -0.004547307267785072,
          -0.019882800057530403,
          0.027390524744987488,
          0.015042118728160858,
          0.00933464802801609,
          -0.006587594281882048,
          0.022136451676487923,
          0.0007267690380103886,
          -0.00228532194159925,
          0.0009918063879013062,
          -8.381388761335984e-05,
          0.007681081537157297,
          -0.017055734992027283,
          -0.01434868760406971,
          -0.003103770548477769,
          -0.03584504872560501,
          0.012601775117218494,
          0.0008517866372130811,
          -0.008134478703141212,
          -0.009741371497511864,
          0.0019202703842893243,
          0.024656806141138077,
          0.008567873388528824,
          0.004580644890666008,
          0.011261586099863052,
          0.015562191605567932,
          -0.009461332112550735,
          0.012568436563014984,
          -0.00733436644077301,
          0.002718716161325574,
          -0.00025170212029479444,
          0.002108630258589983,
          0.01781584322452545,
          -0.010228106752038002,
          0.02216312102973461,
          0.02409672737121582,
          0.0046206507831811905,
          -0.0006038350402377546,
          -0.020562896504998207,
          0.0012026693439111114,
          -0.011868338100612164,
          -0.012488425709307194,
          0.006294219754636288,
          -0.014415363781154156,
          -0.04717997834086418,
          -0.016722355037927628,
          0.02098962292075157,
          0.015135465189814568,
          0.0012943489709869027,
          -0.01685570739209652,
          0.008741230703890324,
          -0.018389256671071053,
          -0.012808470986783504,
          -0.0009709700825624168,
          -0.001995281083509326,
          -0.010181433521211147,
          -0.004353946540504694,
          -0.05952838435769081,
          -0.0096013518050313,
          -0.04533971846103668,
          0.006727613974362612,
          0.03352472186088562,
          0.0033721416257321835,
          -0.00436394801363349,
          -0.007681081537157297,
          -0.033258017152547836,
          -0.005140724126249552,
          0.03653847798705101,
          -0.015082123689353466,
          -0.027950603514909744,
          0.006354228127747774,
          -0.0450730137526989,
          0.04003230482339859,
          0.01612227037549019,
          -0.01740245148539543,
          -0.0044839647598564625,
          -0.014988777227699757,
          0.004083908628672361,
          -0.031604450196027756,
          -0.015095459297299385,
          0.0010584824485704303,
          0.020109498873353004,
          0.0012585106305778027,
          -0.017189087346196175,
          0.02555026486515999,
          -0.0022236465010792017,
          -0.012055031023919582,
          -0.0006071688258089125,
          -0.029390806332230568,
          0.047473352402448654,
          0.002633704338222742,
          -0.019389396533370018,
          0.009187960997223854,
          0.017509132623672485,
          0.012181716039776802,
          -0.004353946540504694,
          0.02888406813144684,
          -0.0010501479264348745,
          -0.018562614917755127,
          0.0138552850112319,
          -0.0036605154164135456,
          0.019189368933439255,
          -0.003178780898451805,
          -0.03485824167728424,
          -0.013308540917932987,
          -0.008447856642305851,
          0.010388129390776157,
          -0.03920552134513855,
          -0.0034504858776926994,
          0.00650758296251297,
          0.011181574314832687,
          -0.01901601068675518,
          -0.012581772170960903,
          -0.021629713475704193,
          0.01392196025699377,
          0.01990947127342224,
          -0.011021552607417107,
          0.002422007732093334,
          -0.0055174436420202255,
          0.013268535025417805,
          0.012075033970177174,
          -0.014655397273600101,
          0.024270085617899895,
          -0.002778724767267704,
          -0.005184063222259283,
          -0.0003223370586056262,
          0.0032287880312651396,
          0.005104052368551493,
          0.025950321927666664,
          -0.03421815112233162,
          0.0017044066917151213,
          0.03101770207285881,
          0.007821101695299149,
          0.019136028364300728,
          0.0048806872218847275,
          -0.0024486782494932413,
          0.015962248668074608,
          -0.010881532914936543,
          0.02883072756230831,
          -0.011514955200254917,
          0.017122412100434303,
          0.001248509157449007,
          -0.009361318312585354,
          -0.011134901084005833,
          0.02469681203365326,
          0.0333380289375782,
          0.03813870623707771,
          0.03349805250763893,
          -0.017482461407780647,
          -0.006940977647900581,
          -0.012628445401787758,
          -0.013788608834147453,
          -0.004540639463812113,
          0.009768042713403702,
          0.04120580479502678,
          -0.009981405921280384,
          0.015482180751860142,
          0.005974174477159977,
          0.0027420527767390013,
          0.020309526473283768,
          0.007141005713492632,
          0.016735689714550972,
          0.005267408676445484,
          -0.005374090280383825,
          0.032937973737716675,
          0.0012951823882758617,
          0.007094332482665777,
          -0.013895289972424507,
          -0.0007371871615760028,
          0.00447729742154479,
          -0.016749026253819466,
          -0.017282433807849884,
          -0.0014060313114896417,
          -0.025123538449406624,
          0.019056016579270363,
          0.02066957764327526,
          -0.014322017319500446,
          0.03405813127756119,
          0.0005142390727996826,
          0.040112316608428955,
          -0.012008357793092728,
          -0.02021618001163006,
          0.03336469829082489,
          -0.01080818846821785,
          -0.015002112835645676,
          -0.013681926764547825,
          -0.019762782379984856,
          0.014415363781154156,
          -0.005027374718338251,
          -0.0027270507998764515,
          -0.0025253556668758392,
          -0.0008492862689308822,
          -0.023283278569579124,
          0.0034904915373772383,
          0.00037067721132189035,
          0.020816264674067497,
          0.014282011426985264,
          -0.018295910209417343,
          0.004490632563829422,
          0.011054890230298042,
          0.003813870484009385,
          0.012795135378837585,
          -0.029497487470507622,
          -0.016562333330512047,
          0.021763065829873085,
          -0.01116157229989767,
          -0.03952556475996971,
          -0.021883081644773483,
          -0.026110343635082245,
          -0.02713715471327305,
          0.040085647255182266,
          -0.011888341046869755,
          0.022136451676487923,
          -0.005554115399718285,
          0.0242567490786314,
          0.02555026486515999,
          0.009407991543412209,
          -0.0034704888239502907,
          0.027630558237433434,
          0.011928346939384937,
          -0.005354087334126234,
          -0.010068085044622421,
          -0.0007050993153825402,
          0.013168521225452423,
          0.04304606094956398,
          0.03256458789110184,
          -0.00974803976714611,
          0.010461472906172276,
          -0.01704240031540394,
          0.012875146232545376,
          -0.03320467472076416,
          -0.02536357194185257,
          -0.0011718317400664091,
          -0.017895853146910667,
          -0.011321594007313251,
          -0.007901112549006939,
          -0.0026853783056139946,
          -0.005240737926214933,
          -0.016495656222105026,
          -0.007641076110303402,
          -0.018389256671071053,
          -0.0395522378385067,
          0.019856128841638565,
          0.008367844857275486,
          -0.006880969274789095,
          0.021096304059028625,
          0.030617645010352135,
          0.027310512959957123,
          0.014402028173208237,
          0.008981265127658844,
          0.02117631584405899,
          0.02196309342980385,
          -0.005140724126249552,
          -0.019216040149331093,
          0.0032204536255449057,
          -0.016402309760451317,
          0.00904794130474329,
          0.014602056704461575,
          0.00784110464155674,
          0.020549559965729713,
          -0.00772108742967248,
          -0.009901394136250019,
          0.028057284653186798,
          0.008907921612262726,
          0.011394938454031944,
          0.01216171309351921,
          0.007207681890577078,
          -0.014148659072816372,
          0.010581490583717823,
          -0.03504493460059166,
          -0.00036525976611301303,
          0.031791143119335175,
          -0.006360895931720734,
          0.007661079056560993,
          -0.009247968904674053,
          0.01084152702242136,
          -0.027630558237433434,
          -0.006020847707986832,
          0.029630839824676514,
          -0.0015085458289831877,
          -0.018362585455179214,
          -0.0003198367194272578,
          -0.0021619710605591536,
          -0.01740245148539543,
          -0.012695121578872204,
          -0.016375640407204628,
          0.004904023837298155,
          -0.002453678986057639,
          0.002502019051462412,
          -0.0025136873591691256,
          -0.03085767850279808,
          -0.027470534667372704,
          -0.028777386993169785,
          0.019642766565084457,
          -0.001286847866140306,
          -0.024176737293601036,
          -0.049313612282276154,
          -0.014695403166115284,
          0.022069774568080902,
          0.02336329035460949,
          0.017029065638780594,
          -0.017415786162018776,
          -0.02663041651248932,
          -0.00934798363596201,
          0.007987791672348976,
          -0.021736394613981247,
          0.004193923901766539,
          -0.02400338090956211,
          -0.013008498586714268,
          -0.011534958146512508,
          -0.028937408700585365,
          -0.004750669002532959,
          -0.03299131244421005,
          0.013828613795340061,
          -0.009294642135500908,
          0.01660233736038208,
          0.018709301948547363,
          -0.019416067749261856,
          0.017189087346196175,
          0.01101488433778286,
          0.008341174572706223,
          0.011261586099863052,
          0.02142968401312828,
          -0.030937690287828445,
          0.018055876716971397,
          -0.010294782929122448,
          -0.023549983277916908,
          -0.017122412100434303,
          0.006437573116272688,
          0.009481335058808327,
          0.00430060550570488,
          -0.02920411340892315,
          0.011948348954319954,
          -0.014735408127307892,
          0.02057623118162155,
          -0.02467014081776142,
          -7.1364214818459e-05,
          0.005497440695762634,
          0.03685852512717247,
          0.016482321545481682,
          0.016895713284611702,
          0.022803211584687233,
          0.0029120768886059523,
          -0.00465065473690629,
          -0.008261163718998432,
          -0.02492351084947586,
          0.018562614917755127,
          0.0194294024258852,
          -0.010268112644553185,
          0.009821383282542229,
          0.032191202044487,
          -0.005837488919496536,
          -0.035951729863882065,
          0.02536357194185257,
          -0.04083241894841194,
          0.018682630732655525,
          -0.0010476475581526756,
          -0.017642484977841377,
          -0.03285796195268631,
          -0.02396337501704693,
          0.0066842748783528805,
          0.0009509672527201474,
          -0.04283269867300987,
          0.008201154880225658,
          0.0008651218377053738,
          0.0027320515364408493,
          0.0012268394930288196,
          -0.017975864931941032,
          0.023629995062947273,
          -0.02835065871477127,
          -0.018842654302716255,
          0.004237263463437557,
          0.014508710242807865,
          0.015122129581868649,
          -0.01768249087035656,
          0.013508569449186325,
          -0.033258017152547836,
          -0.013895289972424507,
          0.009001268073916435,
          -0.018109217286109924,
          0.004373949486762285,
          0.018055876716971397,
          0.007707752287387848,
          0.01534882839769125,
          0.03581837937235832,
          0.0036538478452712297,
          0.01958942599594593,
          0.021763065829873085,
          -0.004887355025857687,
          0.009214631281793118,
          -0.023856693878769875,
          0.009614687412977219,
          -0.0017619148129597306,
          0.011034887284040451,
          0.00962135475128889,
          -0.01570887863636017,
          -0.01452204491943121,
          -0.0052207354456186295,
          0.019802788272500038,
          0.010608160868287086,
          -0.007567732594907284,
          -0.018042540177702904,
          -0.02867070399224758,
          -0.023403296247124672,
          -0.024776821956038475,
          0.0019369394285604358,
          -0.003133774735033512,
          -0.005497440695762634,
          -0.05158059671521187,
          -0.01812255196273327,
          0.0008013628539629281,
          0.0031587781850248575,
          0.008267831057310104,
          0.0008297002059407532,
          0.020389538258314133,
          0.0036538478452712297,
          -0.007654411252588034,
          0.009061275981366634,
          0.0030070901848375797,
          -0.014468704350292683,
          -0.007314363494515419,
          -0.02752387709915638,
          -0.011081560514867306,
          0.03640512749552727,
          0.021149644628167152,
          -0.00500737177208066,
          0.00585749140009284,
          -0.010301451198756695,
          -0.001103488728404045,
          0.013228530064225197,
          0.0029070761520415545,
          -0.013155185617506504,
          -0.025243554264307022,
          -0.014708737842738628,
          -0.021389678120613098,
          -0.030804337933659554,
          -0.012588439509272575,
          0.02904408983886242,
          0.013721932657063007,
          -0.008114475756883621,
          -0.029124101623892784,
          0.032964643090963364,
          -0.0002389919973211363,
          -0.0033037986140698195,
          0.004023900255560875,
          0.024883504956960678,
          0.032937973737716675,
          -0.001675235922448337,
          0.011381602846086025,
          0.016242288053035736,
          0.0022786541376262903,
          -0.006347560789436102,
          0.0021619710605591536,
          0.004997370298951864,
          -0.03035094030201435,
          -0.000267537689069286,
          -0.009788044728338718,
          -0.0005438266089186072,
          -0.010421467944979668,
          0.016909047961235046,
          0.01042813528329134,
          -0.03117772378027439,
          -0.02053622528910637,
          0.014322017319500446,
          -0.007281025405973196,
          0.023923369124531746,
          -0.002218645764514804,
          -0.019322721287608147,
          -0.001751913339830935,
          0.013868619687855244,
          -0.01653566211462021,
          0.02869737520813942,
          0.002815396524965763,
          0.00013929045235272497,
          0.019096022471785545,
          0.02708381414413452,
          -0.010001408867537975,
          0.022816546261310577,
          -0.01444203406572342,
          0.008687890134751797,
          0.00265037314966321,
          0.006967647932469845,
          -0.020362867042422295,
          -0.011654974892735481,
          0.006797623820602894,
          0.020749589428305626,
          -0.012535098940134048,
          0.017122412100434303,
          0.007974456064403057,
          -0.011614969000220299,
          0.00662760017439723,
          0.009134619496762753,
          -0.0014418697683140635,
          0.03083100914955139,
          -0.02787059172987938,
          -0.005124054849147797,
          -0.0007338533760048449,
          0.004497299902141094,
          0.017375780269503593,
          -0.013788608834147453,
          0.001751913339830935,
          0.0007451049168594182,
          -0.017029065638780594,
          0.01570887863636017,
          0.025563599541783333,
          0.024150067940354347,
          -0.023763347417116165,
          0.0072743576020002365,
          0.005717471707612276,
          -0.0014535380760207772,
          -0.002433676039800048,
          0.006204206962138414,
          0.01993614062666893,
          -0.03555167466402054,
          -0.013788608834147453,
          0.034111469984054565,
          0.014282011426985264,
          0.020149504765868187,
          -0.00895459484308958,
          -0.011481616646051407,
          -0.014455368742346764,
          0.02984420396387577,
          -0.04373949393630028,
          0.007947785779833794,
          0.012395079247653484,
          -0.013681926764547825,
          -0.013208527117967606,
          -0.018269238993525505,
          -0.015575527213513851,
          -0.01293515507131815,
          0.0007967788842506707,
          -0.0015035450924187899,
          -0.022229798138141632,
          -0.0019869464449584484,
          -0.005197398830205202,
          -0.005670798476785421,
          0.007994459010660648,
          -0.022136451676487923,
          -0.020896276459097862,
          0.018936000764369965,
          0.0206962488591671,
          -0.019376061856746674,
          -0.01366859208792448,
          -0.012435084208846092,
          0.0029220781289041042,
          -0.00328212883323431,
          0.01271512359380722,
          -0.027977272868156433,
          -0.01452204491943121,
          0.0016694017685949802,
          0.01758914440870285,
          -0.011928346939384937,
          -0.04101911187171936,
          -0.0011543292785063386,
          -0.006974315736442804,
          0.009461332112550735,
          -0.0018819316755980253,
          -0.015028783120214939,
          0.022083109244704247,
          -0.01878931373357773,
          -0.008787903934717178,
          0.02591031603515148,
          -0.00884124543517828,
          -0.010481475852429867,
          0.003190449206158519,
          -0.00792111549526453,
          0.013341878540813923,
          0.02479015849530697,
          -0.0097147012129426,
          -0.034964922815561295,
          -0.011754988692700863,
          -0.003883880330249667,
          -0.017882518470287323,
          -0.008487861603498459,
          -0.015548856928944588,
          0.051633939146995544,
          0.031444426625967026,
          -0.010228106752038002,
          -0.010674837045371532,
          0.0024153401609510183,
          -0.025830304250121117,
          0.003423815593123436,
          -0.005870827008038759,
          0.0070809973403811455,
          0.0010659834370017052,
          -0.021229656413197517,
          -0.0023453303147107363,
          0.017735831439495087,
          -0.0004996536881662905,
          -0.004964032210409641,
          -0.008261163718998432,
          -0.0018719303188845515,
          -0.021002957597374916,
          -0.01841592602431774,
          0.00025232721236534417,
          -0.012681785970926285,
          -0.03933887183666229,
          0.014708737842738628,
          0.028617363423109055,
          -0.006250880192965269,
          0.0172424279153347,
          0.04387284442782402,
          -0.00871456041932106,
          0.001788585213944316,
          -0.00974803976714611,
          0.016655679792165756,
          0.02186974696815014,
          0.017389114946126938,
          0.011168239638209343,
          -0.026270367205142975,
          0.01154162548482418,
          0.04171254113316536,
          -0.0037538621108978987,
          -0.02050955593585968,
          0.002850401448085904,
          0.0047906748950481415,
          0.00784110464155674,
          -0.012101704254746437,
          -0.021856412291526794,
          -0.0020186176989227533,
          0.0019469409016892314,
          -0.019856128841638565,
          0.005077381618320942,
          0.01781584322452545,
          -0.02888406813144684,
          0.0073076956905424595,
          0.0028604029212146997,
          0.00657092547044158,
          -0.03157778084278107,
          0.017722496762871742,
          -0.010734844952821732,
          -0.0012285063276067376,
          0.01917603425681591,
          -0.0012593440478667617,
          0.0016493989387527108,
          -0.016655679792165756,
          0.005594121292233467,
          0.009928064420819283,
          -0.006174203008413315,
          -0.03301798179745674,
          0.001644398202188313,
          -0.023216603323817253,
          0.008587876334786415,
          0.007734422571957111,
          -0.044966332614421844,
          -0.006710945162922144,
          -0.0020486218854784966,
          -0.019882800057530403,
          -0.017255762591958046,
          -0.004980701487511396,
          0.000838034669868648,
          -0.005024041049182415,
          0.01698905974626541,
          -0.0060508521273732185,
          -0.016709020361304283,
          -0.01949607953429222,
          0.005604122765362263,
          0.02142968401312828,
          0.00415058434009552,
          0.02695046178996563,
          0.228085458278656,
          -0.029284123331308365,
          -0.00036505141179077327,
          0.00515739293769002,
          0.003147109877318144,
          0.019802788272500038,
          -0.0013276869431138039,
          -0.0026303704362362623,
          -0.009774710051715374,
          -0.008867915719747543,
          0.04149917885661125,
          0.006914306897670031,
          -0.01312851533293724,
          -0.0017452457686886191,
          0.018242569640278816,
          -0.02336329035460949,
          -0.014148659072816372,
          -0.03421815112233162,
          -0.012601775117218494,
          0.01242841687053442,
          0.005134056322276592,
          0.01216171309351921,
          0.002530356403440237,
          -0.0008076137164607644,
          0.028937408700585365,
          0.004067239351570606,
          0.0032337887678295374,
          -0.0024486782494932413,
          0.021149644628167152,
          0.004924026783555746,
          -0.006197539623826742,
          -0.005454101599752903,
          0.011701648123562336,
          0.02085627056658268,
          -0.010248109698295593,
          -0.0009134619613178074,
          -0.029977554455399513,
          0.003218786558136344,
          0.018749307841062546,
          -0.017122412100434303,
          0.027657227590680122,
          -0.007541061844676733,
          0.010268112644553185,
          -0.04133915528655052,
          0.008041132241487503,
          -0.0009776377119123936,
          -0.004910691641271114,
          -0.006454242393374443,
          -0.004447293002158403,
          0.02202976867556572,
          -0.008947926573455334,
          0.02545691840350628,
          0.02098962292075157,
          0.005194064695388079,
          0.008361177518963814,
          0.00035546673461794853,
          -0.009854720905423164,
          0.009041273035109043,
          0.027977272868156433,
          0.005440766457468271,
          -0.008961262181401253,
          -0.0001698155829217285,
          -0.0010168098378926516,
          0.02301657572388649,
          -0.01312851533293724,
          0.022109780460596085,
          -0.02612367831170559,
          0.0231099221855402,
          0.0356316864490509,
          0.021509695798158646,
          0.007054326590150595,
          -0.023790016770362854,
          -0.007741090375930071,
          0.014895430766046047,
          -0.04389951750636101,
          -0.03424482420086861,
          0.051313892006874084,
          0.012488425709307194,
          0.039472226053476334,
          0.03003089688718319,
          0.009701366536319256,
          0.001888599363155663,
          -0.018175892531871796,
          -0.015495515428483486,
          -0.0010159764206036925,
          -0.04376616328954697,
          0.029790861532092094,
          -0.023056579753756523,
          0.001105155679397285,
          -0.024736817926168442,
          0.003963891416788101,
          -0.008494529873132706,
          -0.01133492961525917,
          -0.007301028352230787,
          0.008241160772740841,
          0.013481899164617062,
          0.017162416130304337,
          0.012588439509272575,
          -0.010454805567860603,
          -0.009167958050966263,
          -0.02545691840350628,
          0.06902305036783218,
          -0.0025470254477113485,
          0.00812781136482954,
          0.002713715424761176,
          0.01816255785524845,
          -0.013681926764547825,
          0.014055312611162663,
          0.008461191318929195,
          -0.011741654016077518,
          0.016802366822957993,
          -0.03139108791947365,
          0.008001127280294895,
          -0.0035538338124752045,
          0.011814997531473637,
          0.012115039862692356,
          0.029577499255537987,
          0.003673850791528821,
          0.018269238993525505,
          -0.0144286984577775,
          0.01733577437698841,
          -0.022403154522180557,
          0.009834717959165573,
          0.015162135474383831,
          0.005960839334875345,
          -0.004463961813598871,
          -0.018215898424386978,
          0.010241442359983921,
          -0.03221787139773369,
          -0.0032254543621093035,
          -0.014868760481476784,
          -0.02552359364926815,
          0.022109780460596085,
          -0.02835065871477127,
          -0.02536357194185257,
          -0.003680518362671137,
          -0.001108489464968443,
          -0.024963514879345894,
          -0.02416340261697769,
          0.02170972339808941,
          0.0020536226220428944,
          0.008201154880225658,
          0.016842372715473175,
          -7.469802221748978e-05,
          -0.020136170089244843,
          0.002567028161138296,
          -0.008741230703890324,
          -0.0031604450196027756,
          0.00635756179690361,
          -0.02727050706744194,
          -0.008581208065152168,
          -0.017882518470287323,
          -0.011448279023170471,
          -0.0006538420566357672,
          0.02216312102973461,
          0.010534817352890968,
          -0.006407569162547588,
          -0.008154481649398804,
          0.00683429604396224,
          0.005110719706863165,
          -0.02209644578397274,
          -0.021896416321396828,
          0.0017252429388463497,
          -0.0078077660873532295,
          -0.009794712997972965,
          -0.0016910715494304895,
          -0.16845038533210754,
          -0.0010668168542906642,
          0.025443583726882935,
          -0.046779923141002655,
          0.011995022185146809,
          0.0024936844129115343,
          0.02479015849530697,
          -0.030110906809568405,
          -0.05054045096039772,
          0.014602056704461575,
          -0.00010371252574259415,
          0.0020119501277804375,
          0.0016485655214637518,
          -0.009794712997972965,
          -0.03248457610607147,
          0.006497581955045462,
          -0.0242567490786314,
          0.01818922907114029,
          0.014588721096515656,
          -0.0027403859421610832,
          0.0377119779586792,
          -0.03800535202026367,
          0.026297036558389664,
          -0.011694980785250664,
          0.03547166287899017,
          -0.0014026975259184837,
          0.013775273226201534,
          0.019189368933439255,
          -0.01246175542473793,
          -0.04491299018263817,
          -0.0012218387564644217,
          -0.006454242393374443,
          0.03488491475582123,
          -0.022043105214834213,
          -0.009341315366327763,
          0.015148799866437912,
          -0.010774850845336914,
          -0.0057208058424293995,
          -0.00950800534337759,
          0.006914306897670031,
          0.029897544533014297,
          0.0073410337790846825,
          0.0043272762559354305,
          -0.008227825164794922,
          -0.02479015849530697,
          0.0020919612143188715,
          0.029124101623892784,
          -0.012181716039776802,
          0.004500634036958218,
          -0.011054890230298042,
          0.008987932465970516,
          -0.0006059186416678131,
          -0.002218645764514804,
          0.011148236691951752,
          0.0030404282733798027,
          0.003563835285604,
          0.028430670499801636,
          0.018469268456101418,
          -0.011301591992378235,
          -0.017189087346196175,
          -0.009121284820139408,
          -0.007814433425664902,
          0.0042939381673932076,
          0.009814715944230556,
          -0.015922242775559425,
          -0.011654974892735481,
          -0.009501338005065918,
          -0.012868478894233704,
          -0.007901112549006939,
          0.021002957597374916,
          -0.031444426625967026,
          -0.011895008385181427,
          -0.01071484200656414,
          -0.0052774096839129925,
          0.005614123772829771,
          0.017855847254395485,
          -0.017802506685256958,
          -0.022043105214834213,
          0.00307876686565578,
          -0.002470347797498107,
          -0.02237648516893387,
          0.021536367014050484,
          -0.0094279944896698,
          0.0033371367026120424,
          0.005594121292233467,
          0.016775695607066154,
          -0.0004917359328828752,
          0.02492351084947586,
          -0.019816124811768532,
          0.0009793045464903116,
          0.039125509560108185,
          -0.012721791863441467,
          0.0025470254477113485,
          -0.0034138141199946404,
          -0.010041413828730583,
          0.011214912869036198,
          0.0067742872051894665,
          0.0020119501277804375,
          0.004130581859499216,
          -0.018269238993525505,
          0.010754847899079323,
          -0.0027687232941389084,
          -0.015242146328091621,
          0.020136170089244843,
          0.03507160767912865,
          0.016175610944628716,
          -0.015562191605567932,
          0.003032093634828925,
          0.04336610808968544,
          0.00842785369604826,
          -0.01685570739209652,
          -0.004907357972115278,
          0.0009042940218932927,
          -0.0053107477724552155,
          -0.004023900255560875,
          0.03003089688718319,
          -0.0006146698724478483,
          -0.017469126731157303,
          -0.0029120768886059523,
          -0.019322721287608147,
          0.048806872218847275,
          -0.028404001146554947,
          -0.03237789496779442,
          0.029257453978061676,
          -0.011568295769393444,
          -0.0314977690577507,
          -0.11649640649557114,
          -0.025030191987752914,
          -0.011788327246904373,
          0.01285514421761036,
          -0.004233929794281721,
          0.009074611589312553,
          0.015922242775559425,
          0.024203408509492874,
          -0.03248457610607147,
          0.04189923405647278,
          -0.00016929468256421387,
          -0.01910935714840889,
          0.01704240031540394,
          -0.015175470151007175,
          -0.0073076956905424595,
          0.001351857092231512,
          0.012901817448437214,
          0.010488144122064114,
          -0.01071484200656414,
          0.028083955869078636,
          0.012068366631865501,
          -0.031071042641997337,
          0.021483026444911957,
          -0.016895713284611702,
          -0.004440625198185444,
          0.0018135887803509831,
          -0.017882518470287323,
          0.008594543673098087,
          -0.0010759849101305008,
          -0.022496500983834267,
          0.016322297975420952,
          -0.036778513342142105,
          0.0071143354289233685,
          -0.012515095993876457,
          0.01876264251768589,
          -0.01033478882163763,
          0.003907216712832451,
          -0.021483026444911957,
          -0.016802366822957993,
          -0.006744283251464367,
          -0.011421608738601208,
          0.005090716760605574,
          0.0011609968496486545,
          0.0032421231735497713,
          -0.01736244559288025,
          -0.0172424279153347,
          -0.022109780460596085,
          0.027030473574995995,
          0.0031721133273094893,
          -0.03235122188925743,
          -0.028270648792386055,
          -0.023643329739570618,
          -0.05024707689881325,
          -0.0038005353417247534,
          0.0024736816994845867,
          -0.014055312611162663,
          -0.001976944971829653,
          0.0036371788010001183,
          -0.01090153492987156,
          -0.005967507138848305,
          0.003607174614444375,
          -0.009701366536319256,
          -0.011088227853178978,
          0.029790861532092094,
          0.01476207934319973,
          -0.005707470700144768,
          -0.010001408867537975,
          -0.03240456432104111,
          -0.006384232547134161,
          -0.026110343635082245,
          -0.0015960581367835402,
          0.00963469035923481,
          0.0008342841756530106,
          0.007801098749041557,
          -0.03565835580229759,
          -0.0012410080526024103,
          -0.018709301948547363,
          -0.01910935714840889,
          -0.0038038690108805895,
          0.010368126444518566,
          -0.026577075943350792,
          -0.003317133756354451,
          0.008401183411478996,
          -0.024803493171930313,
          -0.013721932657063007,
          -0.0030054233502596617,
          -0.006550922524183989,
          -0.005297412630170584,
          -0.002998755546286702,
          -0.02708381414413452,
          0.0029920879751443863,
          0.02933746576309204,
          -0.011468281969428062,
          0.000514655839651823,
          -0.00992139708250761,
          0.021096304059028625,
          -0.0028037282172590494,
          -0.023816687986254692,
          0.0264437235891819,
          0.02308325096964836,
          -0.017949193716049194,
          -0.005944170523434877,
          -0.02301657572388649,
          0.0164423156529665,
          -0.009501338005065918,
          -0.008721228688955307,
          0.002960416954010725,
          -0.011468281969428062,
          0.04200591519474983,
          0.016709020361304283,
          -0.022563178092241287,
          0.008581208065152168,
          -0.009968070313334465,
          0.021136309951543808,
          0.0014035309432074428,
          -0.027217166498303413,
          -0.014922101981937885,
          -0.022403154522180557,
          0.014735408127307892,
          -0.022936563938856125,
          0.022149786353111267,
          0.01940273307263851,
          -0.005264074541628361,
          -0.014868760481476784,
          0.021029628813266754,
          0.0015785556752234697,
          0.007894445210695267,
          -0.002181974006816745,
          -0.00782776903361082,
          0.021056298166513443,
          -0.010708174668252468,
          -0.022176455706357956,
          0.031071042641997337,
          -0.01892266422510147,
          0.005000704433768988,
          0.014255341142416,
          -0.002030286006629467,
          0.015988918021321297,
          0.001396863372065127,
          -0.004817345179617405,
          0.022523172199726105,
          -0.006697610020637512,
          -0.005140724126249552,
          -0.02260318398475647,
          0.029070761054754257,
          -0.01850927248597145,
          0.013388551771640778,
          -0.007467718329280615,
          -0.022456495091319084,
          -0.0038738788571208715,
          0.01860261894762516,
          -0.018709301948547363,
          0.01704240031540394,
          0.0144286984577775,
          0.01222172100096941,
          -0.031444426625967026,
          -0.0016002253396436572,
          -0.004183922428637743,
          -0.016682349145412445,
          0.0001160580141004175,
          0.01542883925139904,
          0.003500493010506034,
          0.02701713889837265,
          0.004593980498611927,
          -0.009021270088851452,
          -0.010654834099113941,
          0.004697328433394432,
          0.013208527117967606,
          0.003597173374146223,
          0.013828613795340061,
          -0.003433816833421588,
          -0.012121707201004028,
          7.297902629943565e-05,
          0.011188242584466934,
          0.0011493285419419408,
          0.0016310630599036813,
          0.0344848558306694,
          0.0188559889793396,
          0.020522890612483025,
          -0.0037671972531825304,
          -0.00636422960087657,
          0.0476067028939724,
          0.014575386419892311,
          -0.019696107134222984,
          -0.016455650329589844,
          0.009374653920531273,
          0.01374860294163227,
          0.00465732254087925,
          0.006860966328531504,
          -0.02559027075767517,
          -0.011908343993127346,
          0.005557449534535408,
          0.007961121387779713,
          0.008087805472314358,
          -0.01366859208792448,
          0.03723191097378731,
          0.0037638633511960506,
          0.01720242202281952,
          0.0061842044815421104,
          0.030137578025460243,
          0.02304324507713318,
          0.006227543577551842,
          0.00256369449198246,
          -0.003430483164265752,
          -0.00278539233841002,
          0.010514814406633377,
          -0.004057237878441811,
          0.027630558237433434,
          -0.011401605792343616,
          -0.01742912083864212,
          0.019376061856746674,
          0.027230501174926758,
          0.0093146450817585,
          -0.008567873388528824,
          -0.01745579205453396,
          -0.007207681890577078,
          -0.017309105023741722,
          0.01660233736038208,
          -0.007327698636800051,
          -0.024870168417692184,
          -0.02463013492524624,
          0.005114053376019001,
          0.011975020170211792,
          -0.015562191605567932,
          0.049126919358968735,
          -0.013468563556671143,
          0.019829459488391876,
          -0.009454664774239063,
          0.025656946003437042,
          -0.020722918212413788,
          0.013681926764547825,
          0.02987087331712246,
          0.01765581965446472,
          -0.006704277358949184,
          0.010174766182899475,
          0.001715241582132876,
          -0.007134337909519672,
          -0.011434943415224552,
          0.02168305404484272,
          0.02603033185005188,
          -0.019949475303292274,
          0.07227684557437897,
          0.024483447894454002,
          0.007394374813884497,
          -0.0018469267524778843,
          -0.02349664270877838,
          0.022109780460596085,
          0.005240737926214933,
          0.015988918021321297,
          -0.023643329739570618,
          0.0017569140763953328,
          0.004317274782806635,
          0.0042572664096951485,
          0.0002967084583360702,
          -0.02361665852367878,
          0.0012376742670312524,
          -0.0051207211799919605,
          0.013175188563764095,
          0.003943888936191797,
          0.003112104954198003,
          0.00040609887219034135,
          0.03669850155711174,
          -0.009928064420819283,
          0.02244316041469574,
          0.0023703337647020817,
          -0.023643329739570618,
          0.004730666056275368,
          0.009594684466719627,
          0.03152443841099739,
          -0.0029920879751443863,
          -0.03699187561869621,
          0.016869042068719864,
          0.013301873579621315,
          -0.029470816254615784,
          -0.0031887823715806007,
          0.002603699918836355,
          -0.011301591992378235,
          -0.012708456255495548,
          -0.011701648123562336,
          0.0018019204726442695,
          0.012915152125060558,
          0.020749589428305626,
          0.03573836758732796,
          -0.021069634705781937,
          -0.023723341524600983,
          -0.0015610532136633992,
          0.002572028897702694,
          -0.01204836368560791,
          -0.02200309932231903,
          -0.006360895931720734
        ],
        "existingAnswer": true,
        "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
        "indexType": "cogsearchvs",
        "jsonAnswer": {
          "data_points": [
            "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
            "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
            "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
          ],
          "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
          "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.']<br>        Question: What is the advantage of fine-tuning BERT over using feature-based approaches?<br>        ",
          "sources": "\nBert.pdf",
          "nextQuestions": "<>\n<>\n<>",
          "error": ""
        },
        "llm": "AzureChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.3, model_kwargs={}, openai_api_key='1361532858cb428c8da412a73105de78', openai_api_base='https://dataaiapim.azure-api.net', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=1000, tiktoken_model_name=None, deployment_name='chat16k', model_version='', openai_api_type='azure', openai_api_version='2023-07-01-preview')",
        "modifiedAnswer": "",
        "overrides": {
          "chainType": "stuff",
          "deploymentType": "gpt3516k",
          "embeddingModelType": "azureopenai",
          "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
          "searchType": "hybridrerank",
          "semantic_captions": false,
          "semantic_ranker": true,
          "temperature": 0,
          "tokenLength": 1000,
          "top": 3
        },
        "promptTemplate": "",
        "question": "What is the advantage of fine-tuning BERT over using feature-based approaches?",
        "retrievedDocs": ""
      },
      "output": {
        "values": [
          {
            "recordId": 0,
            "data": {
              "data_points": [
                "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
                "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
              ],
              "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
              "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.']<br>        Question: What is the advantage of fine-tuning BERT over using feature-based approaches?<br>        ",
              "sources": "\nBert.pdf",
              "nextQuestions": "<>\n<>\n<>",
              "error": ""
            }
          }
        ]
      },
      "metrics": null,
      "error": null,
      "parent_run_id": "4d4879be-de55-475e-85ef-cc3de9de4117_0",
      "start_time": "2023-10-12T14:01:57.824167Z",
      "end_time": "2023-10-12T14:01:57.847382Z",
      "index": 0,
      "api_calls": [
        {
          "name": "generateFollowupQuestions",
          "type": "Tool",
          "inputs": {
            "conn": "entaoai",
            "embeddedQuestion": [
              -0.016869042068719864,
              -0.0003010840737260878,
              0.022069774568080902,
              -0.01818922907114029,
              -0.0022569845896214247,
              0.013475230894982815,
              -0.0065809269435703754,
              0.004023900255560875,
              -0.032884631305933,
              -0.019216040149331093,
              0.02651040069758892,
              0.030404282733798027,
              -0.024683475494384766,
              0.0007167676230892539,
              -0.018149223178625107,
              -0.015762219205498695,
              0.036565151065588,
              0.028804056346416473,
              0.005427430849522352,
              -0.025256890803575516,
              -0.011601634323596954,
              -0.009081278927624226,
              -0.024976851418614388,
              -0.011588298715651035,
              -0.02108296938240528,
              -0.0024053386878222227,
              0.045446399599313736,
              -0.020642906427383423,
              -0.01844259724020958,
              0.0037738648243248463,
              0.026403717696666718,
              0.02196309342980385,
              -0.016562333330512047,
              -0.02032286301255226,
              -0.011454946361482143,
              0.009701366536319256,
              -0.0029054090846329927,
              -0.01225505955517292,
              0.024843499064445496,
              -0.01254176627844572,
              0.01114156935364008,
              0.032751280814409256,
              0.011975020170211792,
              -0.015215476043522358,
              -0.0032737944275140762,
              0.0264437235891819,
              0.022069774568080902,
              -0.00033192173577845097,
              -0.03133774548768997,
              0.020056158304214478,
              0.002888740273192525,
              0.030564304441213608,
              -0.016975723206996918,
              0.003323801327496767,
              -0.05198065564036369,
              -0.007454383186995983,
              -0.006334225181490183,
              0.005664131138473749,
              -0.0017552472418174148,
              -0.025723623111844063,
              0.002136967610567808,
              0.006624266039580107,
              -0.00904794130474329,
              0.01246175542473793,
              -0.012735126540064812,
              -0.013095177710056305,
              -0.005047377664595842,
              0.022816546261310577,
              0.010688171721994877,
              -0.0024503450840711594,
              0.012088368646800518,
              0.02571028843522072,
              -0.01602892391383648,
              -0.006470911204814911,
              0.027577217668294907,
              -0.014828754588961601,
              -0.0028720712289214134,
              0.021576372906565666,
              0.01041479967534542,
              -0.0031987838447093964,
              -0.0014477039221674204,
              0.006234211381524801,
              -0.01090153492987156,
              0.014268675819039345,
              0.02060290239751339,
              0.014735408127307892,
              0.0026270365342497826,
              0.0010318119311705232,
              0.003170446492731571,
              0.015802225098013878,
              0.005537446588277817,
              0.018749307841062546,
              0.03416481241583824,
              0.02221646159887314,
              0.011074893176555634,
              -0.0029904211405664682,
              -0.005217401310801506,
              0.014562050811946392,
              -0.00028566524269990623,
              0.004353946540504694,
              -0.009027938358485699,
              0.009401324205100536,
              -0.029817532747983932,
              -0.011661642231047153,
              -0.004167253617197275,
              0.009067943319678307,
              0.018629290163517,
              -0.005494107026606798,
              -0.0012393412180244923,
              -0.0036205099895596504,
              -0.03037761151790619,
              0.014482039958238602,
              -0.014588721096515656,
              -0.014175329357385635,
              -0.0054140957072377205,
              0.0022019767202436924,
              0.0005113219958730042,
              -0.010294782929122448,
              -0.01401530671864748,
              0.004560642410069704,
              0.023629995062947273,
              -1.4324934454634786e-05,
              0.018815983086824417,
              -0.017189087346196175,
              0.01698905974626541,
              -0.019322721287608147,
              -0.015028783120214939,
              -0.017975864931941032,
              -0.007247687317430973,
              -0.009167958050966263,
              0.02002948708832264,
              0.021403014659881592,
              0.00963469035923481,
              -0.02092294581234455,
              -0.007207681890577078,
              0.005680799949914217,
              0.01781584322452545,
              -0.016815701499581337,
              -0.014482039958238602,
              -0.019189368933439255,
              0.023830022662878036,
              0.016175610944628716,
              -0.02591031603515148,
              -0.03267126902937889,
              -0.05371423065662384,
              0.02012283354997635,
              -0.000785110576543957,
              0.030564304441213608,
              0.008974596858024597,
              -0.022456495091319084,
              0.012995163910090923,
              -0.029284123331308365,
              -0.012835141271352768,
              0.03224454075098038,
              0.002288655610755086,
              0.0055507817305624485,
              0.0018986007198691368,
              0.006704277358949184,
              -0.014161994680762291,
              -0.0231099221855402,
              0.01802920550107956,
              0.017975864931941032,
              0.023736676201224327,
              -0.006900971755385399,
              0.017149081453680992,
              0.021949758753180504,
              0.0017919189995154738,
              -0.005604122765362263,
              0.017375780269503593,
              -0.001429367926903069,
              0.005760811269283295,
              0.030110906809568405,
              -0.014455368742346764,
              0.007487721275538206,
              -0.0011893342016264796,
              0.01284180860966444,
              0.03205784782767296,
              -0.020056158304214478,
              -0.004427290055900812,
              -0.03920552134513855,
              0.023029910400509834,
              0.010188101790845394,
              0.03371141478419304,
              -0.013248532079160213,
              -0.018642624840140343,
              0.008454523980617523,
              0.013828613795340061,
              -0.017135746777057648,
              0.007401042152196169,
              -0.036805182695388794,
              0.007361036725342274,
              0.004623984452337027,
              -0.0066509367898106575,
              -0.022496500983834267,
              -0.6383832097053528,
              -0.026083674281835556,
              0.011181574314832687,
              -0.022229798138141632,
              0.015642203390598297,
              0.0144286984577775,
              0.010294782929122448,
              0.008834577165544033,
              -0.014375357888638973,
              0.016895713284611702,
              -0.0036705168895423412,
              -0.0242567490786314,
              -0.012355073355138302,
              -0.017922524362802505,
              -0.006840963382273912,
              -0.024243414402008057,
              0.01041479967534542,
              -0.013495233841240406,
              0.005487439688295126,
              0.0028720712289214134,
              -0.015815559774637222,
              0.03184448555111885,
              -0.023856693878769875,
              -0.017122412100434303,
              -0.011888341046869755,
              -0.010554819367825985,
              0.008614546619355679,
              -0.008961262181401253,
              -0.0003694270271807909,
              -0.004160585813224316,
              -0.011228247545659542,
              0.02679044008255005,
              0.026337042450904846,
              -0.008227825164794922,
              0.03952556475996971,
              0.010241442359983921,
              -0.007987791672348976,
              0.011988354846835136,
              -9.897226846078411e-05,
              0.0432327538728714,
              -0.004000563640147448,
              -0.03136441856622696,
              0.01694905385375023,
              -0.006024181842803955,
              0.006390899885445833,
              0.009814715944230556,
              -0.0008313670987263322,
              0.020896276459097862,
              0.00922129862010479,
              -0.009908062405884266,
              -0.0033804760314524174,
              0.011508287861943245,
              -0.0031371084041893482,
              -6.230043800314888e-05,
              0.016562333330512047,
              -0.014135324396193027,
              0.02120298519730568,
              -0.01952274888753891,
              -0.0004071406729053706,
              -0.006010846700519323,
              0.010381462052464485,
              0.003970559220761061,
              -0.004810677375644445,
              -0.03733859211206436,
              -0.005260740872472525,
              0.007574399933218956,
              0.0069809830747544765,
              0.004774005617946386,
              0.007561064790934324,
              -0.04285936802625656,
              0.007201014086604118,
              0.016322297975420952,
              0.0020636238623410463,
              -0.011081560514867306,
              -0.007681081537157297,
              0.026750434190034866,
              0.01434868760406971,
              -0.007314363494515419,
              -0.006454242393374443,
              0.013001831248402596,
              0.010374794714152813,
              -0.01533549278974533,
              0.010788186453282833,
              0.027577217668294907,
              -0.023349955677986145,
              0.009194628335535526,
              -0.004710663575679064,
              -0.011328262276947498,
              -0.00980804767459631,
              0.003322134492918849,
              -0.0023269944358617067,
              0.025150207802653313,
              -0.009768042713403702,
              -0.042939379811286926,
              0.020109498873353004,
              0.004330609925091267,
              0.004727332387119532,
              -0.00904794130474329,
              0.008327839896082878,
              -0.022469831630587578,
              -0.004547307267785072,
              -0.019882800057530403,
              0.027390524744987488,
              0.015042118728160858,
              0.00933464802801609,
              -0.006587594281882048,
              0.022136451676487923,
              0.0007267690380103886,
              -0.00228532194159925,
              0.0009918063879013062,
              -8.381388761335984e-05,
              0.007681081537157297,
              -0.017055734992027283,
              -0.01434868760406971,
              -0.003103770548477769,
              -0.03584504872560501,
              0.012601775117218494,
              0.0008517866372130811,
              -0.008134478703141212,
              -0.009741371497511864,
              0.0019202703842893243,
              0.024656806141138077,
              0.008567873388528824,
              0.004580644890666008,
              0.011261586099863052,
              0.015562191605567932,
              -0.009461332112550735,
              0.012568436563014984,
              -0.00733436644077301,
              0.002718716161325574,
              -0.00025170212029479444,
              0.002108630258589983,
              0.01781584322452545,
              -0.010228106752038002,
              0.02216312102973461,
              0.02409672737121582,
              0.0046206507831811905,
              -0.0006038350402377546,
              -0.020562896504998207,
              0.0012026693439111114,
              -0.011868338100612164,
              -0.012488425709307194,
              0.006294219754636288,
              -0.014415363781154156,
              -0.04717997834086418,
              -0.016722355037927628,
              0.02098962292075157,
              0.015135465189814568,
              0.0012943489709869027,
              -0.01685570739209652,
              0.008741230703890324,
              -0.018389256671071053,
              -0.012808470986783504,
              -0.0009709700825624168,
              -0.001995281083509326,
              -0.010181433521211147,
              -0.004353946540504694,
              -0.05952838435769081,
              -0.0096013518050313,
              -0.04533971846103668,
              0.006727613974362612,
              0.03352472186088562,
              0.0033721416257321835,
              -0.00436394801363349,
              -0.007681081537157297,
              -0.033258017152547836,
              -0.005140724126249552,
              0.03653847798705101,
              -0.015082123689353466,
              -0.027950603514909744,
              0.006354228127747774,
              -0.0450730137526989,
              0.04003230482339859,
              0.01612227037549019,
              -0.01740245148539543,
              -0.0044839647598564625,
              -0.014988777227699757,
              0.004083908628672361,
              -0.031604450196027756,
              -0.015095459297299385,
              0.0010584824485704303,
              0.020109498873353004,
              0.0012585106305778027,
              -0.017189087346196175,
              0.02555026486515999,
              -0.0022236465010792017,
              -0.012055031023919582,
              -0.0006071688258089125,
              -0.029390806332230568,
              0.047473352402448654,
              0.002633704338222742,
              -0.019389396533370018,
              0.009187960997223854,
              0.017509132623672485,
              0.012181716039776802,
              -0.004353946540504694,
              0.02888406813144684,
              -0.0010501479264348745,
              -0.018562614917755127,
              0.0138552850112319,
              -0.0036605154164135456,
              0.019189368933439255,
              -0.003178780898451805,
              -0.03485824167728424,
              -0.013308540917932987,
              -0.008447856642305851,
              0.010388129390776157,
              -0.03920552134513855,
              -0.0034504858776926994,
              0.00650758296251297,
              0.011181574314832687,
              -0.01901601068675518,
              -0.012581772170960903,
              -0.021629713475704193,
              0.01392196025699377,
              0.01990947127342224,
              -0.011021552607417107,
              0.002422007732093334,
              -0.0055174436420202255,
              0.013268535025417805,
              0.012075033970177174,
              -0.014655397273600101,
              0.024270085617899895,
              -0.002778724767267704,
              -0.005184063222259283,
              -0.0003223370586056262,
              0.0032287880312651396,
              0.005104052368551493,
              0.025950321927666664,
              -0.03421815112233162,
              0.0017044066917151213,
              0.03101770207285881,
              0.007821101695299149,
              0.019136028364300728,
              0.0048806872218847275,
              -0.0024486782494932413,
              0.015962248668074608,
              -0.010881532914936543,
              0.02883072756230831,
              -0.011514955200254917,
              0.017122412100434303,
              0.001248509157449007,
              -0.009361318312585354,
              -0.011134901084005833,
              0.02469681203365326,
              0.0333380289375782,
              0.03813870623707771,
              0.03349805250763893,
              -0.017482461407780647,
              -0.006940977647900581,
              -0.012628445401787758,
              -0.013788608834147453,
              -0.004540639463812113,
              0.009768042713403702,
              0.04120580479502678,
              -0.009981405921280384,
              0.015482180751860142,
              0.005974174477159977,
              0.0027420527767390013,
              0.020309526473283768,
              0.007141005713492632,
              0.016735689714550972,
              0.005267408676445484,
              -0.005374090280383825,
              0.032937973737716675,
              0.0012951823882758617,
              0.007094332482665777,
              -0.013895289972424507,
              -0.0007371871615760028,
              0.00447729742154479,
              -0.016749026253819466,
              -0.017282433807849884,
              -0.0014060313114896417,
              -0.025123538449406624,
              0.019056016579270363,
              0.02066957764327526,
              -0.014322017319500446,
              0.03405813127756119,
              0.0005142390727996826,
              0.040112316608428955,
              -0.012008357793092728,
              -0.02021618001163006,
              0.03336469829082489,
              -0.01080818846821785,
              -0.015002112835645676,
              -0.013681926764547825,
              -0.019762782379984856,
              0.014415363781154156,
              -0.005027374718338251,
              -0.0027270507998764515,
              -0.0025253556668758392,
              -0.0008492862689308822,
              -0.023283278569579124,
              0.0034904915373772383,
              0.00037067721132189035,
              0.020816264674067497,
              0.014282011426985264,
              -0.018295910209417343,
              0.004490632563829422,
              0.011054890230298042,
              0.003813870484009385,
              0.012795135378837585,
              -0.029497487470507622,
              -0.016562333330512047,
              0.021763065829873085,
              -0.01116157229989767,
              -0.03952556475996971,
              -0.021883081644773483,
              -0.026110343635082245,
              -0.02713715471327305,
              0.040085647255182266,
              -0.011888341046869755,
              0.022136451676487923,
              -0.005554115399718285,
              0.0242567490786314,
              0.02555026486515999,
              0.009407991543412209,
              -0.0034704888239502907,
              0.027630558237433434,
              0.011928346939384937,
              -0.005354087334126234,
              -0.010068085044622421,
              -0.0007050993153825402,
              0.013168521225452423,
              0.04304606094956398,
              0.03256458789110184,
              -0.00974803976714611,
              0.010461472906172276,
              -0.01704240031540394,
              0.012875146232545376,
              -0.03320467472076416,
              -0.02536357194185257,
              -0.0011718317400664091,
              -0.017895853146910667,
              -0.011321594007313251,
              -0.007901112549006939,
              -0.0026853783056139946,
              -0.005240737926214933,
              -0.016495656222105026,
              -0.007641076110303402,
              -0.018389256671071053,
              -0.0395522378385067,
              0.019856128841638565,
              0.008367844857275486,
              -0.006880969274789095,
              0.021096304059028625,
              0.030617645010352135,
              0.027310512959957123,
              0.014402028173208237,
              0.008981265127658844,
              0.02117631584405899,
              0.02196309342980385,
              -0.005140724126249552,
              -0.019216040149331093,
              0.0032204536255449057,
              -0.016402309760451317,
              0.00904794130474329,
              0.014602056704461575,
              0.00784110464155674,
              0.020549559965729713,
              -0.00772108742967248,
              -0.009901394136250019,
              0.028057284653186798,
              0.008907921612262726,
              0.011394938454031944,
              0.01216171309351921,
              0.007207681890577078,
              -0.014148659072816372,
              0.010581490583717823,
              -0.03504493460059166,
              -0.00036525976611301303,
              0.031791143119335175,
              -0.006360895931720734,
              0.007661079056560993,
              -0.009247968904674053,
              0.01084152702242136,
              -0.027630558237433434,
              -0.006020847707986832,
              0.029630839824676514,
              -0.0015085458289831877,
              -0.018362585455179214,
              -0.0003198367194272578,
              -0.0021619710605591536,
              -0.01740245148539543,
              -0.012695121578872204,
              -0.016375640407204628,
              0.004904023837298155,
              -0.002453678986057639,
              0.002502019051462412,
              -0.0025136873591691256,
              -0.03085767850279808,
              -0.027470534667372704,
              -0.028777386993169785,
              0.019642766565084457,
              -0.001286847866140306,
              -0.024176737293601036,
              -0.049313612282276154,
              -0.014695403166115284,
              0.022069774568080902,
              0.02336329035460949,
              0.017029065638780594,
              -0.017415786162018776,
              -0.02663041651248932,
              -0.00934798363596201,
              0.007987791672348976,
              -0.021736394613981247,
              0.004193923901766539,
              -0.02400338090956211,
              -0.013008498586714268,
              -0.011534958146512508,
              -0.028937408700585365,
              -0.004750669002532959,
              -0.03299131244421005,
              0.013828613795340061,
              -0.009294642135500908,
              0.01660233736038208,
              0.018709301948547363,
              -0.019416067749261856,
              0.017189087346196175,
              0.01101488433778286,
              0.008341174572706223,
              0.011261586099863052,
              0.02142968401312828,
              -0.030937690287828445,
              0.018055876716971397,
              -0.010294782929122448,
              -0.023549983277916908,
              -0.017122412100434303,
              0.006437573116272688,
              0.009481335058808327,
              0.00430060550570488,
              -0.02920411340892315,
              0.011948348954319954,
              -0.014735408127307892,
              0.02057623118162155,
              -0.02467014081776142,
              -7.1364214818459e-05,
              0.005497440695762634,
              0.03685852512717247,
              0.016482321545481682,
              0.016895713284611702,
              0.022803211584687233,
              0.0029120768886059523,
              -0.00465065473690629,
              -0.008261163718998432,
              -0.02492351084947586,
              0.018562614917755127,
              0.0194294024258852,
              -0.010268112644553185,
              0.009821383282542229,
              0.032191202044487,
              -0.005837488919496536,
              -0.035951729863882065,
              0.02536357194185257,
              -0.04083241894841194,
              0.018682630732655525,
              -0.0010476475581526756,
              -0.017642484977841377,
              -0.03285796195268631,
              -0.02396337501704693,
              0.0066842748783528805,
              0.0009509672527201474,
              -0.04283269867300987,
              0.008201154880225658,
              0.0008651218377053738,
              0.0027320515364408493,
              0.0012268394930288196,
              -0.017975864931941032,
              0.023629995062947273,
              -0.02835065871477127,
              -0.018842654302716255,
              0.004237263463437557,
              0.014508710242807865,
              0.015122129581868649,
              -0.01768249087035656,
              0.013508569449186325,
              -0.033258017152547836,
              -0.013895289972424507,
              0.009001268073916435,
              -0.018109217286109924,
              0.004373949486762285,
              0.018055876716971397,
              0.007707752287387848,
              0.01534882839769125,
              0.03581837937235832,
              0.0036538478452712297,
              0.01958942599594593,
              0.021763065829873085,
              -0.004887355025857687,
              0.009214631281793118,
              -0.023856693878769875,
              0.009614687412977219,
              -0.0017619148129597306,
              0.011034887284040451,
              0.00962135475128889,
              -0.01570887863636017,
              -0.01452204491943121,
              -0.0052207354456186295,
              0.019802788272500038,
              0.010608160868287086,
              -0.007567732594907284,
              -0.018042540177702904,
              -0.02867070399224758,
              -0.023403296247124672,
              -0.024776821956038475,
              0.0019369394285604358,
              -0.003133774735033512,
              -0.005497440695762634,
              -0.05158059671521187,
              -0.01812255196273327,
              0.0008013628539629281,
              0.0031587781850248575,
              0.008267831057310104,
              0.0008297002059407532,
              0.020389538258314133,
              0.0036538478452712297,
              -0.007654411252588034,
              0.009061275981366634,
              0.0030070901848375797,
              -0.014468704350292683,
              -0.007314363494515419,
              -0.02752387709915638,
              -0.011081560514867306,
              0.03640512749552727,
              0.021149644628167152,
              -0.00500737177208066,
              0.00585749140009284,
              -0.010301451198756695,
              -0.001103488728404045,
              0.013228530064225197,
              0.0029070761520415545,
              -0.013155185617506504,
              -0.025243554264307022,
              -0.014708737842738628,
              -0.021389678120613098,
              -0.030804337933659554,
              -0.012588439509272575,
              0.02904408983886242,
              0.013721932657063007,
              -0.008114475756883621,
              -0.029124101623892784,
              0.032964643090963364,
              -0.0002389919973211363,
              -0.0033037986140698195,
              0.004023900255560875,
              0.024883504956960678,
              0.032937973737716675,
              -0.001675235922448337,
              0.011381602846086025,
              0.016242288053035736,
              0.0022786541376262903,
              -0.006347560789436102,
              0.0021619710605591536,
              0.004997370298951864,
              -0.03035094030201435,
              -0.000267537689069286,
              -0.009788044728338718,
              -0.0005438266089186072,
              -0.010421467944979668,
              0.016909047961235046,
              0.01042813528329134,
              -0.03117772378027439,
              -0.02053622528910637,
              0.014322017319500446,
              -0.007281025405973196,
              0.023923369124531746,
              -0.002218645764514804,
              -0.019322721287608147,
              -0.001751913339830935,
              0.013868619687855244,
              -0.01653566211462021,
              0.02869737520813942,
              0.002815396524965763,
              0.00013929045235272497,
              0.019096022471785545,
              0.02708381414413452,
              -0.010001408867537975,
              0.022816546261310577,
              -0.01444203406572342,
              0.008687890134751797,
              0.00265037314966321,
              0.006967647932469845,
              -0.020362867042422295,
              -0.011654974892735481,
              0.006797623820602894,
              0.020749589428305626,
              -0.012535098940134048,
              0.017122412100434303,
              0.007974456064403057,
              -0.011614969000220299,
              0.00662760017439723,
              0.009134619496762753,
              -0.0014418697683140635,
              0.03083100914955139,
              -0.02787059172987938,
              -0.005124054849147797,
              -0.0007338533760048449,
              0.004497299902141094,
              0.017375780269503593,
              -0.013788608834147453,
              0.001751913339830935,
              0.0007451049168594182,
              -0.017029065638780594,
              0.01570887863636017,
              0.025563599541783333,
              0.024150067940354347,
              -0.023763347417116165,
              0.0072743576020002365,
              0.005717471707612276,
              -0.0014535380760207772,
              -0.002433676039800048,
              0.006204206962138414,
              0.01993614062666893,
              -0.03555167466402054,
              -0.013788608834147453,
              0.034111469984054565,
              0.014282011426985264,
              0.020149504765868187,
              -0.00895459484308958,
              -0.011481616646051407,
              -0.014455368742346764,
              0.02984420396387577,
              -0.04373949393630028,
              0.007947785779833794,
              0.012395079247653484,
              -0.013681926764547825,
              -0.013208527117967606,
              -0.018269238993525505,
              -0.015575527213513851,
              -0.01293515507131815,
              0.0007967788842506707,
              -0.0015035450924187899,
              -0.022229798138141632,
              -0.0019869464449584484,
              -0.005197398830205202,
              -0.005670798476785421,
              0.007994459010660648,
              -0.022136451676487923,
              -0.020896276459097862,
              0.018936000764369965,
              0.0206962488591671,
              -0.019376061856746674,
              -0.01366859208792448,
              -0.012435084208846092,
              0.0029220781289041042,
              -0.00328212883323431,
              0.01271512359380722,
              -0.027977272868156433,
              -0.01452204491943121,
              0.0016694017685949802,
              0.01758914440870285,
              -0.011928346939384937,
              -0.04101911187171936,
              -0.0011543292785063386,
              -0.006974315736442804,
              0.009461332112550735,
              -0.0018819316755980253,
              -0.015028783120214939,
              0.022083109244704247,
              -0.01878931373357773,
              -0.008787903934717178,
              0.02591031603515148,
              -0.00884124543517828,
              -0.010481475852429867,
              0.003190449206158519,
              -0.00792111549526453,
              0.013341878540813923,
              0.02479015849530697,
              -0.0097147012129426,
              -0.034964922815561295,
              -0.011754988692700863,
              -0.003883880330249667,
              -0.017882518470287323,
              -0.008487861603498459,
              -0.015548856928944588,
              0.051633939146995544,
              0.031444426625967026,
              -0.010228106752038002,
              -0.010674837045371532,
              0.0024153401609510183,
              -0.025830304250121117,
              0.003423815593123436,
              -0.005870827008038759,
              0.0070809973403811455,
              0.0010659834370017052,
              -0.021229656413197517,
              -0.0023453303147107363,
              0.017735831439495087,
              -0.0004996536881662905,
              -0.004964032210409641,
              -0.008261163718998432,
              -0.0018719303188845515,
              -0.021002957597374916,
              -0.01841592602431774,
              0.00025232721236534417,
              -0.012681785970926285,
              -0.03933887183666229,
              0.014708737842738628,
              0.028617363423109055,
              -0.006250880192965269,
              0.0172424279153347,
              0.04387284442782402,
              -0.00871456041932106,
              0.001788585213944316,
              -0.00974803976714611,
              0.016655679792165756,
              0.02186974696815014,
              0.017389114946126938,
              0.011168239638209343,
              -0.026270367205142975,
              0.01154162548482418,
              0.04171254113316536,
              -0.0037538621108978987,
              -0.02050955593585968,
              0.002850401448085904,
              0.0047906748950481415,
              0.00784110464155674,
              -0.012101704254746437,
              -0.021856412291526794,
              -0.0020186176989227533,
              0.0019469409016892314,
              -0.019856128841638565,
              0.005077381618320942,
              0.01781584322452545,
              -0.02888406813144684,
              0.0073076956905424595,
              0.0028604029212146997,
              0.00657092547044158,
              -0.03157778084278107,
              0.017722496762871742,
              -0.010734844952821732,
              -0.0012285063276067376,
              0.01917603425681591,
              -0.0012593440478667617,
              0.0016493989387527108,
              -0.016655679792165756,
              0.005594121292233467,
              0.009928064420819283,
              -0.006174203008413315,
              -0.03301798179745674,
              0.001644398202188313,
              -0.023216603323817253,
              0.008587876334786415,
              0.007734422571957111,
              -0.044966332614421844,
              -0.006710945162922144,
              -0.0020486218854784966,
              -0.019882800057530403,
              -0.017255762591958046,
              -0.004980701487511396,
              0.000838034669868648,
              -0.005024041049182415,
              0.01698905974626541,
              -0.0060508521273732185,
              -0.016709020361304283,
              -0.01949607953429222,
              0.005604122765362263,
              0.02142968401312828,
              0.00415058434009552,
              0.02695046178996563,
              0.228085458278656,
              -0.029284123331308365,
              -0.00036505141179077327,
              0.00515739293769002,
              0.003147109877318144,
              0.019802788272500038,
              -0.0013276869431138039,
              -0.0026303704362362623,
              -0.009774710051715374,
              -0.008867915719747543,
              0.04149917885661125,
              0.006914306897670031,
              -0.01312851533293724,
              -0.0017452457686886191,
              0.018242569640278816,
              -0.02336329035460949,
              -0.014148659072816372,
              -0.03421815112233162,
              -0.012601775117218494,
              0.01242841687053442,
              0.005134056322276592,
              0.01216171309351921,
              0.002530356403440237,
              -0.0008076137164607644,
              0.028937408700585365,
              0.004067239351570606,
              0.0032337887678295374,
              -0.0024486782494932413,
              0.021149644628167152,
              0.004924026783555746,
              -0.006197539623826742,
              -0.005454101599752903,
              0.011701648123562336,
              0.02085627056658268,
              -0.010248109698295593,
              -0.0009134619613178074,
              -0.029977554455399513,
              0.003218786558136344,
              0.018749307841062546,
              -0.017122412100434303,
              0.027657227590680122,
              -0.007541061844676733,
              0.010268112644553185,
              -0.04133915528655052,
              0.008041132241487503,
              -0.0009776377119123936,
              -0.004910691641271114,
              -0.006454242393374443,
              -0.004447293002158403,
              0.02202976867556572,
              -0.008947926573455334,
              0.02545691840350628,
              0.02098962292075157,
              0.005194064695388079,
              0.008361177518963814,
              0.00035546673461794853,
              -0.009854720905423164,
              0.009041273035109043,
              0.027977272868156433,
              0.005440766457468271,
              -0.008961262181401253,
              -0.0001698155829217285,
              -0.0010168098378926516,
              0.02301657572388649,
              -0.01312851533293724,
              0.022109780460596085,
              -0.02612367831170559,
              0.0231099221855402,
              0.0356316864490509,
              0.021509695798158646,
              0.007054326590150595,
              -0.023790016770362854,
              -0.007741090375930071,
              0.014895430766046047,
              -0.04389951750636101,
              -0.03424482420086861,
              0.051313892006874084,
              0.012488425709307194,
              0.039472226053476334,
              0.03003089688718319,
              0.009701366536319256,
              0.001888599363155663,
              -0.018175892531871796,
              -0.015495515428483486,
              -0.0010159764206036925,
              -0.04376616328954697,
              0.029790861532092094,
              -0.023056579753756523,
              0.001105155679397285,
              -0.024736817926168442,
              0.003963891416788101,
              -0.008494529873132706,
              -0.01133492961525917,
              -0.007301028352230787,
              0.008241160772740841,
              0.013481899164617062,
              0.017162416130304337,
              0.012588439509272575,
              -0.010454805567860603,
              -0.009167958050966263,
              -0.02545691840350628,
              0.06902305036783218,
              -0.0025470254477113485,
              0.00812781136482954,
              0.002713715424761176,
              0.01816255785524845,
              -0.013681926764547825,
              0.014055312611162663,
              0.008461191318929195,
              -0.011741654016077518,
              0.016802366822957993,
              -0.03139108791947365,
              0.008001127280294895,
              -0.0035538338124752045,
              0.011814997531473637,
              0.012115039862692356,
              0.029577499255537987,
              0.003673850791528821,
              0.018269238993525505,
              -0.0144286984577775,
              0.01733577437698841,
              -0.022403154522180557,
              0.009834717959165573,
              0.015162135474383831,
              0.005960839334875345,
              -0.004463961813598871,
              -0.018215898424386978,
              0.010241442359983921,
              -0.03221787139773369,
              -0.0032254543621093035,
              -0.014868760481476784,
              -0.02552359364926815,
              0.022109780460596085,
              -0.02835065871477127,
              -0.02536357194185257,
              -0.003680518362671137,
              -0.001108489464968443,
              -0.024963514879345894,
              -0.02416340261697769,
              0.02170972339808941,
              0.0020536226220428944,
              0.008201154880225658,
              0.016842372715473175,
              -7.469802221748978e-05,
              -0.020136170089244843,
              0.002567028161138296,
              -0.008741230703890324,
              -0.0031604450196027756,
              0.00635756179690361,
              -0.02727050706744194,
              -0.008581208065152168,
              -0.017882518470287323,
              -0.011448279023170471,
              -0.0006538420566357672,
              0.02216312102973461,
              0.010534817352890968,
              -0.006407569162547588,
              -0.008154481649398804,
              0.00683429604396224,
              0.005110719706863165,
              -0.02209644578397274,
              -0.021896416321396828,
              0.0017252429388463497,
              -0.0078077660873532295,
              -0.009794712997972965,
              -0.0016910715494304895,
              -0.16845038533210754,
              -0.0010668168542906642,
              0.025443583726882935,
              -0.046779923141002655,
              0.011995022185146809,
              0.0024936844129115343,
              0.02479015849530697,
              -0.030110906809568405,
              -0.05054045096039772,
              0.014602056704461575,
              -0.00010371252574259415,
              0.0020119501277804375,
              0.0016485655214637518,
              -0.009794712997972965,
              -0.03248457610607147,
              0.006497581955045462,
              -0.0242567490786314,
              0.01818922907114029,
              0.014588721096515656,
              -0.0027403859421610832,
              0.0377119779586792,
              -0.03800535202026367,
              0.026297036558389664,
              -0.011694980785250664,
              0.03547166287899017,
              -0.0014026975259184837,
              0.013775273226201534,
              0.019189368933439255,
              -0.01246175542473793,
              -0.04491299018263817,
              -0.0012218387564644217,
              -0.006454242393374443,
              0.03488491475582123,
              -0.022043105214834213,
              -0.009341315366327763,
              0.015148799866437912,
              -0.010774850845336914,
              -0.0057208058424293995,
              -0.00950800534337759,
              0.006914306897670031,
              0.029897544533014297,
              0.0073410337790846825,
              0.0043272762559354305,
              -0.008227825164794922,
              -0.02479015849530697,
              0.0020919612143188715,
              0.029124101623892784,
              -0.012181716039776802,
              0.004500634036958218,
              -0.011054890230298042,
              0.008987932465970516,
              -0.0006059186416678131,
              -0.002218645764514804,
              0.011148236691951752,
              0.0030404282733798027,
              0.003563835285604,
              0.028430670499801636,
              0.018469268456101418,
              -0.011301591992378235,
              -0.017189087346196175,
              -0.009121284820139408,
              -0.007814433425664902,
              0.0042939381673932076,
              0.009814715944230556,
              -0.015922242775559425,
              -0.011654974892735481,
              -0.009501338005065918,
              -0.012868478894233704,
              -0.007901112549006939,
              0.021002957597374916,
              -0.031444426625967026,
              -0.011895008385181427,
              -0.01071484200656414,
              -0.0052774096839129925,
              0.005614123772829771,
              0.017855847254395485,
              -0.017802506685256958,
              -0.022043105214834213,
              0.00307876686565578,
              -0.002470347797498107,
              -0.02237648516893387,
              0.021536367014050484,
              -0.0094279944896698,
              0.0033371367026120424,
              0.005594121292233467,
              0.016775695607066154,
              -0.0004917359328828752,
              0.02492351084947586,
              -0.019816124811768532,
              0.0009793045464903116,
              0.039125509560108185,
              -0.012721791863441467,
              0.0025470254477113485,
              -0.0034138141199946404,
              -0.010041413828730583,
              0.011214912869036198,
              0.0067742872051894665,
              0.0020119501277804375,
              0.004130581859499216,
              -0.018269238993525505,
              0.010754847899079323,
              -0.0027687232941389084,
              -0.015242146328091621,
              0.020136170089244843,
              0.03507160767912865,
              0.016175610944628716,
              -0.015562191605567932,
              0.003032093634828925,
              0.04336610808968544,
              0.00842785369604826,
              -0.01685570739209652,
              -0.004907357972115278,
              0.0009042940218932927,
              -0.0053107477724552155,
              -0.004023900255560875,
              0.03003089688718319,
              -0.0006146698724478483,
              -0.017469126731157303,
              -0.0029120768886059523,
              -0.019322721287608147,
              0.048806872218847275,
              -0.028404001146554947,
              -0.03237789496779442,
              0.029257453978061676,
              -0.011568295769393444,
              -0.0314977690577507,
              -0.11649640649557114,
              -0.025030191987752914,
              -0.011788327246904373,
              0.01285514421761036,
              -0.004233929794281721,
              0.009074611589312553,
              0.015922242775559425,
              0.024203408509492874,
              -0.03248457610607147,
              0.04189923405647278,
              -0.00016929468256421387,
              -0.01910935714840889,
              0.01704240031540394,
              -0.015175470151007175,
              -0.0073076956905424595,
              0.001351857092231512,
              0.012901817448437214,
              0.010488144122064114,
              -0.01071484200656414,
              0.028083955869078636,
              0.012068366631865501,
              -0.031071042641997337,
              0.021483026444911957,
              -0.016895713284611702,
              -0.004440625198185444,
              0.0018135887803509831,
              -0.017882518470287323,
              0.008594543673098087,
              -0.0010759849101305008,
              -0.022496500983834267,
              0.016322297975420952,
              -0.036778513342142105,
              0.0071143354289233685,
              -0.012515095993876457,
              0.01876264251768589,
              -0.01033478882163763,
              0.003907216712832451,
              -0.021483026444911957,
              -0.016802366822957993,
              -0.006744283251464367,
              -0.011421608738601208,
              0.005090716760605574,
              0.0011609968496486545,
              0.0032421231735497713,
              -0.01736244559288025,
              -0.0172424279153347,
              -0.022109780460596085,
              0.027030473574995995,
              0.0031721133273094893,
              -0.03235122188925743,
              -0.028270648792386055,
              -0.023643329739570618,
              -0.05024707689881325,
              -0.0038005353417247534,
              0.0024736816994845867,
              -0.014055312611162663,
              -0.001976944971829653,
              0.0036371788010001183,
              -0.01090153492987156,
              -0.005967507138848305,
              0.003607174614444375,
              -0.009701366536319256,
              -0.011088227853178978,
              0.029790861532092094,
              0.01476207934319973,
              -0.005707470700144768,
              -0.010001408867537975,
              -0.03240456432104111,
              -0.006384232547134161,
              -0.026110343635082245,
              -0.0015960581367835402,
              0.00963469035923481,
              0.0008342841756530106,
              0.007801098749041557,
              -0.03565835580229759,
              -0.0012410080526024103,
              -0.018709301948547363,
              -0.01910935714840889,
              -0.0038038690108805895,
              0.010368126444518566,
              -0.026577075943350792,
              -0.003317133756354451,
              0.008401183411478996,
              -0.024803493171930313,
              -0.013721932657063007,
              -0.0030054233502596617,
              -0.006550922524183989,
              -0.005297412630170584,
              -0.002998755546286702,
              -0.02708381414413452,
              0.0029920879751443863,
              0.02933746576309204,
              -0.011468281969428062,
              0.000514655839651823,
              -0.00992139708250761,
              0.021096304059028625,
              -0.0028037282172590494,
              -0.023816687986254692,
              0.0264437235891819,
              0.02308325096964836,
              -0.017949193716049194,
              -0.005944170523434877,
              -0.02301657572388649,
              0.0164423156529665,
              -0.009501338005065918,
              -0.008721228688955307,
              0.002960416954010725,
              -0.011468281969428062,
              0.04200591519474983,
              0.016709020361304283,
              -0.022563178092241287,
              0.008581208065152168,
              -0.009968070313334465,
              0.021136309951543808,
              0.0014035309432074428,
              -0.027217166498303413,
              -0.014922101981937885,
              -0.022403154522180557,
              0.014735408127307892,
              -0.022936563938856125,
              0.022149786353111267,
              0.01940273307263851,
              -0.005264074541628361,
              -0.014868760481476784,
              0.021029628813266754,
              0.0015785556752234697,
              0.007894445210695267,
              -0.002181974006816745,
              -0.00782776903361082,
              0.021056298166513443,
              -0.010708174668252468,
              -0.022176455706357956,
              0.031071042641997337,
              -0.01892266422510147,
              0.005000704433768988,
              0.014255341142416,
              -0.002030286006629467,
              0.015988918021321297,
              0.001396863372065127,
              -0.004817345179617405,
              0.022523172199726105,
              -0.006697610020637512,
              -0.005140724126249552,
              -0.02260318398475647,
              0.029070761054754257,
              -0.01850927248597145,
              0.013388551771640778,
              -0.007467718329280615,
              -0.022456495091319084,
              -0.0038738788571208715,
              0.01860261894762516,
              -0.018709301948547363,
              0.01704240031540394,
              0.0144286984577775,
              0.01222172100096941,
              -0.031444426625967026,
              -0.0016002253396436572,
              -0.004183922428637743,
              -0.016682349145412445,
              0.0001160580141004175,
              0.01542883925139904,
              0.003500493010506034,
              0.02701713889837265,
              0.004593980498611927,
              -0.009021270088851452,
              -0.010654834099113941,
              0.004697328433394432,
              0.013208527117967606,
              0.003597173374146223,
              0.013828613795340061,
              -0.003433816833421588,
              -0.012121707201004028,
              7.297902629943565e-05,
              0.011188242584466934,
              0.0011493285419419408,
              0.0016310630599036813,
              0.0344848558306694,
              0.0188559889793396,
              0.020522890612483025,
              -0.0037671972531825304,
              -0.00636422960087657,
              0.0476067028939724,
              0.014575386419892311,
              -0.019696107134222984,
              -0.016455650329589844,
              0.009374653920531273,
              0.01374860294163227,
              0.00465732254087925,
              0.006860966328531504,
              -0.02559027075767517,
              -0.011908343993127346,
              0.005557449534535408,
              0.007961121387779713,
              0.008087805472314358,
              -0.01366859208792448,
              0.03723191097378731,
              0.0037638633511960506,
              0.01720242202281952,
              0.0061842044815421104,
              0.030137578025460243,
              0.02304324507713318,
              0.006227543577551842,
              0.00256369449198246,
              -0.003430483164265752,
              -0.00278539233841002,
              0.010514814406633377,
              -0.004057237878441811,
              0.027630558237433434,
              -0.011401605792343616,
              -0.01742912083864212,
              0.019376061856746674,
              0.027230501174926758,
              0.0093146450817585,
              -0.008567873388528824,
              -0.01745579205453396,
              -0.007207681890577078,
              -0.017309105023741722,
              0.01660233736038208,
              -0.007327698636800051,
              -0.024870168417692184,
              -0.02463013492524624,
              0.005114053376019001,
              0.011975020170211792,
              -0.015562191605567932,
              0.049126919358968735,
              -0.013468563556671143,
              0.019829459488391876,
              -0.009454664774239063,
              0.025656946003437042,
              -0.020722918212413788,
              0.013681926764547825,
              0.02987087331712246,
              0.01765581965446472,
              -0.006704277358949184,
              0.010174766182899475,
              0.001715241582132876,
              -0.007134337909519672,
              -0.011434943415224552,
              0.02168305404484272,
              0.02603033185005188,
              -0.019949475303292274,
              0.07227684557437897,
              0.024483447894454002,
              0.007394374813884497,
              -0.0018469267524778843,
              -0.02349664270877838,
              0.022109780460596085,
              0.005240737926214933,
              0.015988918021321297,
              -0.023643329739570618,
              0.0017569140763953328,
              0.004317274782806635,
              0.0042572664096951485,
              0.0002967084583360702,
              -0.02361665852367878,
              0.0012376742670312524,
              -0.0051207211799919605,
              0.013175188563764095,
              0.003943888936191797,
              0.003112104954198003,
              0.00040609887219034135,
              0.03669850155711174,
              -0.009928064420819283,
              0.02244316041469574,
              0.0023703337647020817,
              -0.023643329739570618,
              0.004730666056275368,
              0.009594684466719627,
              0.03152443841099739,
              -0.0029920879751443863,
              -0.03699187561869621,
              0.016869042068719864,
              0.013301873579621315,
              -0.029470816254615784,
              -0.0031887823715806007,
              0.002603699918836355,
              -0.011301591992378235,
              -0.012708456255495548,
              -0.011701648123562336,
              0.0018019204726442695,
              0.012915152125060558,
              0.020749589428305626,
              0.03573836758732796,
              -0.021069634705781937,
              -0.023723341524600983,
              -0.0015610532136633992,
              0.002572028897702694,
              -0.01204836368560791,
              -0.02200309932231903,
              -0.006360895931720734
            ],
            "existingAnswer": true,
            "indexNs": "8fe8ee44933240fa8bc1d72858e4d1eb",
            "indexType": "cogsearchvs",
            "jsonAnswer": {
              "data_points": [
                "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
                "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
              ],
              "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
              "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.']<br>        Question: What is the advantage of fine-tuning BERT over using feature-based approaches?<br>        ",
              "sources": "\nBert.pdf",
              "nextQuestions": "<>\n<>\n<>",
              "error": ""
            },
            "llm": {
              "model": "gpt-3.5-turbo",
              "request_timeout": null,
              "max_tokens": 1000,
              "stream": false,
              "n": 1,
              "temperature": 0.3,
              "engine": "chat16k",
              "_type": "azure-openai-chat"
            },
            "modifiedAnswer": "",
            "overrides": {
              "chainType": "stuff",
              "deploymentType": "gpt3516k",
              "embeddingModelType": "azureopenai",
              "promptTemplate": "Given the following extracted parts of a long document and a question, create a final answer. \n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n        If the answer is not contained within the text below, say \"I don't know\".\n\n        {summaries}\n        Question: {question}\n        ",
              "searchType": "hybridrerank",
              "semantic_captions": false,
              "semantic_ranker": true,
              "temperature": 0,
              "tokenLength": 1000,
              "top": 3
            },
            "promptTemplate": "",
            "question": "What is the advantage of fine-tuning BERT over using feature-based approaches?",
            "retrievedDocs": ""
          },
          "output": {
            "values": [
              {
                "recordId": 0,
                "data": {
                  "data_points": [
                    "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
                    "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                    "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
                  ],
                  "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
                  "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.']<br>        Question: What is the advantage of fine-tuning BERT over using feature-based approaches?<br>        ",
                  "sources": "\nBert.pdf",
                  "nextQuestions": "<>\n<>\n<>",
                  "error": ""
                }
              }
            ]
          },
          "start_time": 1697137317.824167,
          "end_time": 1697137317.831733,
          "error": null,
          "children": null,
          "node_name": "followup_questions"
        }
      ],
      "variant_id": "",
      "cached_run_id": null,
      "cached_flow_run_id": null,
      "logs": {
        "stdout": "",
        "stderr": ""
      },
      "system_metrics": {
        "duration": 0.023215
      },
      "result": {
        "values": [
          {
            "recordId": 0,
            "data": {
              "data_points": [
                "Fine-tuning approach\n\nBERTLARGE\nBERTBASE\n\nFeature-based approach (BERTBASE)\n\nEmbeddings\nSecond-to-Last Hidden\nLast Hidden\nWeighted Sum Last Four Hidden\nConcat Last Four Hidden\nWeighted Sum All 12 Layers\n\n95.7\n-\n-\n\n96.6\n96.4\n\n91.0\n95.6\n94.9\n95.9\n96.1\n95.5\n\n92.2\n92.6\n93.1\n\n92.8\n92.4\n\n-\n-\n-\n-\n-\n-\n\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\n\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\n\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n\n6 Conclusion\n\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\n\n\fReferences\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nIn Proceedings of the 27th International\nlabeling.\nConference on Computational Linguistics, pages\n1638\u20131649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\n\nLuisa Bentivogli, Bernardo Magnini,\n\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\n\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural\nlanguage.\nComputational linguistics, 18(4):467\u2013479.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\nIn Proceedings\ncrosslingual focused evaluation.\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\n\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\n\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\n\nQuora question pairs.\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nSemi-supervised se-\nning, and Quoc Le. 2018.\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\n\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nIn Pro-\nneural networks with multitask learning.\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\n\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\n\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nImageNet: A Large-Scale Hierarchical\n\nFei. 2009.\nImage Database. In CVPR09.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nIn Proceedings of the 2016\nfrom unlabelled data.\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\n\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nReinforced\nFuru Wei, and Ming Zhou. 2018.\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\n\n\fMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\n\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors.\nIn\nAdvances in neural information processing systems,\npages 3294\u20133302.\n\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\n\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge.\nIn\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\n\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\nIn International Conference on Learning\ntations.\nRepresentations.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.",
                "2.2 Unsupervised Fine-tuning Approaches\n\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\n(Col-\nbedding parameters from unlabeled text\nlobert and Weston, 2008).\n\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\nlanguage model-\nLeft-to-right\net al., 2018a).\n\n\fFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n\n2.3 Transfer Learning from Supervised Data\n\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n\n3 BERT\n\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\n\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\n\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\n\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\n\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\n\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\n\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n\n4We note that in the literature the bidirectional Trans-\n\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\fInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\n\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208 RH ,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208 RH .\n\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n\n3.1 Pre-training BERT\n\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\n\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.",
                "A.5\n\nIllustrations of Fine-tuning on Different\nTasks\n\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks.\nIn\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\n\nB Detailed Experimental Setup\n\nB.1 Detailed Descriptions for the GLUE\n\nBenchmark Experiments.\n\nOur GLUE results\nin Table1 are obtained\nhttps://gluebenchmark.com/\nfrom\nleaderboard\nhttps://blog.\nand\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\n\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\n\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\n\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\n\n\fFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\n\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\n\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\n\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\n\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\n\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\n\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n\n15https://gluebenchmark.com/faq\n\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\fNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\n\nMasking Rates\n\nDev Set Results\n\nMASK SAME RND MNLI\n\nNER\n\nFine-tune Fine-tune Feature-based\n\n80% 10% 10%\n0%\n0%\n100%\n0% 20%\n80%\n80% 20%\n0%\n0% 20% 80%\n0% 100%\n0%\n\n84.2\n84.3\n84.1\n84.4\n83.7\n83.6\n\n95.4\n94.9\n95.2\n95.2\n94.8\n94.9\n\n94.9\n94.0\n94.6\n94.7\n94.6\n94.6\n\nTable 8: Ablation over different masking strategies.\n\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\n\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\n\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n\njority class.\n\nC Additional Ablation Studies\n\nC.1 Effect of Number of Training Steps\n\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\n\nC.2 Ablation for Different Masking\n\nProcedures\n\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n\ny\nc\na\nr\nu\nc\nc\nA\nv\ne\nD\n\nI\nL\nN\nM\n\n84\n\n82\n\n80\n\n78\n\n76\n\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nPre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k."
              ],
              "answer": "The advantage of fine-tuning BERT over using feature-based approaches is that fine-tuning allows the model to adjust its representations specifically for the downstream task, resulting in better performance.",
              "thoughts": "<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don't know the answer, just say that you don't know. Don't try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don't know\".<br><br>        ['Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe \ufb01rst sub-token as the input to the token-level\\nclassi\ufb01er over the NER label set.\\n\\nTo ablate the \ufb01ne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without \ufb01ne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classi\ufb01cation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind \ufb01ne-tuning the entire model. This\\ndemonstrates that BERT is effective for both \ufb01ne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to bene\ufb01t from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these \ufb01ndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638\u20131649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817\u20131853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe \ufb01fth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120\u2013128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467\u2013479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914\u2013\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160\u2013167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670\u2013680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079\u20133087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via \ufb01lling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294\u20133302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188\u20131196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nef\ufb01cient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111\u20133119. Curran Associates,\\nInc.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the \ufb01rst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\n\ufb01ne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to \ufb01ne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and \ufb01ne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For \ufb01ne-\\ntuning, the BERT model is \ufb01rst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are \ufb01ne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate \ufb01ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uni\ufb01ed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the \ufb01nal downstream architecture.\\n\\nModel Architecture BERT\u2019s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as \u201cThe Annotated Transformer.\u201d2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensor\ufb02ow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a \u201csentence\u201d can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A \u201csequence\u201d refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The \ufb01rst\\ntoken of every sequence is always a special clas-\\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classi\ufb01cation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the \ufb01nal hidden\\nvector of the special [CLS] token as C \u2208 RH ,\\nand the \ufb01nal hidden vector for the ith input token\\nas Ti \u2208 RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly \u201csee itself\u201d, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a \u201cmasked\\nLM\u201d (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the \ufb01nal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.', 'A.5\\n\\nIllustrations of Fine-tuning on Different\\nTasks\\n\\nThe illustration of \ufb01ne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speci\ufb01c\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks.\\nIn\\nthe \ufb01gure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classi\ufb01cation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\n\\nB Detailed Experimental Setup\\n\\nB.1 Detailed Descriptions for the GLUE\\n\\nBenchmark Experiments.\\n\\nOur GLUE results\\nin Table1 are obtained\\nhttps://gluebenchmark.com/\\nfrom\\nleaderboard\\nhttps://blog.\\nand\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\n\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classi\ufb01-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the \ufb01rst one.\\n\\nQQP Quora Question Pairs is a binary classi\ufb01-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\n\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classi\ufb01cation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\n\\n\\x0cFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\n\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classi\ufb01cation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\n\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classi\ufb01cation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically \u201cacceptable\u201d or not (Warstadt\\net al., 2018).\\n\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\n\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\n\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\n\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\n\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that\u2019s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n\\n14Note that we only report single-task \ufb01ne-tuning results\\nin this paper. A multitask \ufb01ne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n\\n15https://gluebenchmark.com/faq\\n\\nBERTE[CLS]E1 E[SEP]...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphBERTE[CLS]E1 E2 ENCT1 T2 TNSingle Sentence ......BERTTok 1 Tok 2 Tok N...[CLS]E[CLS]E1 E2 ENCT1 T2 TNSingle Sentence B-PEROO......E[CLS]E1 E[SEP]Class Label...ENE1\u2019...EM\u2019CT1T[SEP]...TNT1\u2019...TM\u2019Start/End SpanClass LabelBERTTok 1 Tok 2 Tok N...[CLS]Tok 1[CLS][CLS]Tok 1 [SEP]...Tok NTok 1...TokMSentence 1...Sentence 2\\x0cNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\\npears during the \ufb01ne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both \ufb01ne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npli\ufb01ed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\nMasking Rates\\n\\nDev Set Results\\n\\nMASK SAME RND MNLI\\n\\nNER\\n\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10%\\n0%\\n0%\\n100%\\n0% 20%\\n80%\\n80% 20%\\n0%\\n0% 20% 80%\\n0% 100%\\n0%\\n\\n84.2\\n84.3\\n84.1\\n84.4\\n83.7\\n83.6\\n\\n95.4\\n94.9\\n95.2\\n95.2\\n94.8\\n94.9\\n\\n94.9\\n94.0\\n94.6\\n94.7\\n94.6\\n94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speci\ufb01c strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that \ufb01ne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\njority class.\\n\\nC Additional Ablation Studies\\n\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh \ufb01ne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\n\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nv\\ne\\nD\\n\\nI\\nL\\nN\\nM\\n\\n84\\n\\n82\\n\\n80\\n\\n78\\n\\n76\\n\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\n\\n200\\n\\n400\\n\\n600\\n\\n800\\n\\n1,000\\n\\nPre-training Steps (Thousands)\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after \ufb01ne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.']<br>        Question: What is the advantage of fine-tuning BERT over using feature-based approaches?<br>        ",
              "sources": "\nBert.pdf",
              "nextQuestions": "<>\n<>\n<>",
              "error": ""
            }
          }
        ]
      }
    }
  ]
}